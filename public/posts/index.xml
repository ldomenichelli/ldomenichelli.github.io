<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Lucia</title>
    <link>https://ldomenichelli.github.io/posts/</link>
    <description>Recent content on Lucia</description>
    <image>
      <title>Lucia</title>
      <url>https://ldomenichelli.github.io/logo.png</url>
      <link>https://ldomenichelli.github.io/logo.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://ldomenichelli.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Optimal Trasport &amp; Wasserstein distance</title>
      <link>https://ldomenichelli.github.io/posts/post2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post2/</guid>
      <description>&lt;p&gt;To qualify as a &lt;strong&gt;distance&lt;/strong&gt;, a measure must satisfy the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Symmetry&lt;/strong&gt;: $ d(P, Q) = d(Q, P) )$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Triangle inequality&lt;/strong&gt;: $ d(P, Q) + d(Q, R) \geq d(P, R) $&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, in practice, we often deal with &lt;strong&gt;weaker notions of distances&lt;/strong&gt;, commonly referred to as &lt;strong&gt;divergences&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example-kl-divergence&#34;&gt;Example: KL Divergence&lt;/h3&gt;
&lt;p&gt;The Kullback-Leibler (KL) divergence is defined as:
$
[
D_{\text{KL}}(P || Q) = \int p(x) \log \frac{p(x)}{q(x)} dx
]
$&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simplicity Bias</title>
      <link>https://ldomenichelli.github.io/posts/post3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post3/</guid>
      <description>&lt;h1 id=&#34;simplicity-bias-in-neural-networks&#34;&gt;Simplicity Bias in Neural Networks&lt;/h1&gt;
&lt;p&gt;Neural networks (NNs) exhibit a fascinating property often referred to as &lt;strong&gt;simplicity bias&lt;/strong&gt;. As described by Chizat and Bach in their work:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;NNs are fundamentally Bayesian: an NN is biased, at initialization, towards simple functions.&amp;rdquo;&lt;br&gt;
&lt;em&gt;(Chizat &amp;amp; Bach)&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;this-idea-sheds-light-on-why-nns-tend-to-generalize-well-despite-their-enormous-capacity-to-memorize-complex-datasets&#34;&gt;This idea sheds light on why NNs tend to generalize well despite their enormous capacity to memorize complex datasets.&lt;/h2&gt;
&lt;h2 id=&#34;understanding-simplicity-bias&#34;&gt;Understanding Simplicity Bias&lt;/h2&gt;
&lt;h3 id=&#34;the-setup&#34;&gt;The Setup&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fix a Neural Network:&lt;/strong&gt;&lt;br&gt;
Let $ \Theta$ denote the space of its parameters, where each point $ \theta \in \Theta$ represents a specific configuration of weights and biases of the network.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Space uniformity</title>
      <link>https://ldomenichelli.github.io/posts/post1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post1/</guid>
      <description>&lt;p&gt;Understanding the &lt;strong&gt;curse of dimensionality&lt;/strong&gt; requires more than just examining the representational aspect of data. A key insight lies in the concept of &lt;strong&gt;intrinsic dimensionality (ID)&lt;/strong&gt;â€”the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
