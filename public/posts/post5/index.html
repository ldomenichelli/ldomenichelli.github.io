<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>HPLT & NLPL Winter School | Lucia</title>
<meta name=keywords content="pretraining,factuality"><meta name=description content="On the HPLT & NLPL Winter School"><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post5/><link crossorigin=anonymous href=/assets/css/stylesheet.545ff313fe3387bb6faa83d75fcde7d20949cbe2fb53935708ada17d12aff612.css integrity="sha256-VF/zE/4zh7tvqoPXX83n0glJy+L7U5NXCK2hfRKv9hI=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="HPLT & NLPL Winter School"><meta property="og:description" content="On the HPLT & NLPL Winter School"><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post5/"><meta property="og:image" content="https://ldomenichelli.github.io/img/school.jpeg"><meta property="article:section" content="posts"><meta property="og:site_name" content="Lucia"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/img/school.jpeg"><meta name=twitter:title content="HPLT & NLPL Winter School"><meta name=twitter:description content="On the HPLT & NLPL Winter School"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"HPLT \u0026 NLPL Winter School","item":"https://ldomenichelli.github.io/posts/post5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"HPLT \u0026 NLPL Winter School","name":"HPLT \u0026 NLPL Winter School","description":"On the HPLT \u0026 NLPL Winter School","keywords":["pretraining","factuality"],"articleBody":"Day 1 Here are my notes from the HPLT \u0026 NLPL Winter School taken in February 2025 in Skeikampen, NOR.\nCommon Crawl is a comprehensive open-access repository of web data, updated monthly and curated by a non-profit organization. Its principal aim is to facilitate large-scale data analysis by providing a massive corpus of HTML pages, text content, and associated metadata captured from the open internet. Researchers across various domains—from computational linguistics to social network analysis—leverage this resource to study the evolution of online discourse, monitor changes in hyperlink structures, and train advanced machine learning models.\nFrom a scientific perspective, Common Crawl serves as a practical embodiment of the vast, interconnected nature of the World Wide Web. By offering a parsed version of each webpage’s HTML source, it provides structural information essential for tasks like hyperlink analysis and website categorization. At the same time, the raw text captures shifts in language usage over time, enabling longitudinal studies on semantic change, sentiment, and topic modeling. Because the dataset is regularly updated, it can be used to detect emerging trends or abrupt changes—such as sudden spikes in online references to a new technology or event.\nResearchers and institutions often turn to Common Crawl to eliminate the need for building and maintaining their own large-scale web crawlers—a process that would be both costly and difficult to manage. The ability to directly download large data segments means that specialized methods in natural language processing can be developed and tested on a real-world corpus that reflects the genuine complexities and variety of internet text. This helps advance algorithmic research in areas like named entity recognition, topic classification, or question-answering systems. Moreover, the data is openly licensed, allowing teams of any size to collaborate, reproduce findings, and ensure transparency in methods. By pooling efforts around a shared resource, researchers can more rapidly refine existing techniques, propose new computational models, and conduct experiments that push the limits of current technology.\nCommon Crawl Factuality and Hallucinations in LLMs Second talk was about Factuality and Hallucinations and was presented by Anna Rogers, University of Cophenhagen. Here are the slides she shared:\nIt discusses the nature of “bullshit” in LLMs, comparing them with human hallucination abilities, and examines proposed solutions such as RAG (Retrieval-Augmented Generation) and CoT (Chain-of-Thought), highlighting their limitations. Finally, it explores the negative impact of LLMs on the information ecosystem, with the proliferation of spam and synthetic content, and addresses the challenges in identifying and countering these issues. RAG (Retrieval-Augmented Generation) and CoT (Chain-of-Thought) are two approaches proposed to improve the factual accuracy of LLMs, but neither fully solves the problem.\nRAG is a technique that involves providing LLMs with a database or external context from which to draw information during text generation. The idea is that by offering a reference context, the LLM will be able to produce more factual and accurate responses. The RAG process involves:\nRetrieval: Extracting relevant information from an external database. Generation: Using the retrieved information as context to generate a response. However, RAG is not a perfect solution and presents several issues:\nIt can worsen the situation if the knowledge graph is divided in such a way that test questions have no supporting evidence, causing the model to still give correct answers (5-8%) even when instructed to respond “False.” The evaluation of a RAG system’s quality is complex and depends on various criteria such as retrieval accuracy, the relevance of the generated content to the query, fidelity to sources, and correctness against underlying truth. Evaluation metrics are often calculated by another LLM (e.g., GPT-3.5-turbo), which could introduce potential biases. RAG does not guarantee that the LLM will convey information from the database faithfully. There are citation issues, as a RAG model might misquote a source, which doesn’t necessarily help the model produce better results. CoT is a prompting technique that involves providing the LLM with step-by-step reasoning examples, aiming to encourage it to follow a similar thought process during text generation. The idea is that by providing intermediate reasoning, the LLM will generate more accurate and understandable responses. The advantages of CoT include:\nIn many cases, CoT works better than standard prompting. It can improve model accuracy on certain tasks. It may reduce sensitivity to biases, but this effect varies significantly depending on the type of bias and model. However, CoT also has limitations:\nIn biased scenarios, zero-shot CoT prompting can worsen results. The “explanations” produced by CoT are not always faithful to the actual thought process of the model. CoT can be used to “jailbreak” the LLM and bypass restrictions, for example, to generate harmful content. Day 2 Firt talk of the day was by Guilherme Penedo (Hugging Face) on FineWeb2, a recent multilingual web based dataset for LLM pretraining. The talk is centered around adaptig the processing pipelines used in English to different languages. The second talk was presented by Jenia Jitsev \u0026 Marianna Nezhurina, interesting talk about power law and the problmes in quantifying generalization. ","wordCount":"826","inLanguage":"en","image":"https://ldomenichelli.github.io/img/school.jpeg","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post5/"},"publisher":{"@type":"Organization","name":"Lucia","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="Lucia's Notes (Alt + H)"><img src=https://ldomenichelli.github.io/logo.svg alt="Site icon in header" aria-label=logo height=35>Lucia's Notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=Notes><span>Notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=Random><span>Random</span></a></li><li><a href=https://ldomenichelli.github.io/games/ title=Games><span>Games</span></a></li><li><a href=https://ldomenichelli.github.io/archive/ title=Paper><span>Paper</span></a></li><li><a href=https://perfectmotherfuckingwebsite.com/ title="Suggested pages"><span>Suggested pages</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>HPLT & NLPL Winter School</h1><div class=post-description>On the HPLT & NLPL Winter School</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#common-crawl><em>Common Crawl</em></a></li></ul></nav></div></details></div><div class=post-content><h1 id=day-1><em>Day 1</em><a hidden class=anchor aria-hidden=true href=#day-1>#</a></h1><p>Here are my notes from the HPLT & NLPL Winter School taken in February 2025 in Skeikampen, NOR.</p><p>Common Crawl is a comprehensive open-access repository of web data, updated monthly and curated by a non-profit organization. Its principal aim is to facilitate large-scale data analysis by providing a massive corpus of HTML pages, text content, and associated metadata captured from the open internet. Researchers across various domains—from computational linguistics to social network analysis—leverage this resource to study the evolution of online discourse, monitor changes in hyperlink structures, and train advanced machine learning models.</p><p>From a scientific perspective, Common Crawl serves as a practical embodiment of the vast, interconnected nature of the World Wide Web. By offering a parsed version of each webpage’s HTML source, it provides structural information essential for tasks like hyperlink analysis and website categorization. At the same time, the raw text captures shifts in language usage over time, enabling longitudinal studies on semantic change, sentiment, and topic modeling. Because the dataset is regularly updated, it can be used to detect emerging trends or abrupt changes—such as sudden spikes in online references to a new technology or event.</p><p>Researchers and institutions often turn to Common Crawl to eliminate the need for building and maintaining their own large-scale web crawlers—a process that would be both costly and difficult to manage. The ability to directly download large data segments means that specialized methods in natural language processing can be developed and tested on a real-world corpus that reflects the genuine complexities and variety of internet text. This helps advance algorithmic research in areas like named entity recognition, topic classification, or question-answering systems. Moreover, the data is openly licensed, allowing teams of any size to collaborate, reproduce findings, and ensure transparency in methods. By pooling efforts around a shared resource, researchers can more rapidly refine existing techniques, propose new computational models, and conduct experiments that push the limits of current technology.</p><h2 id=common-crawl><em>Common Crawl</em><a hidden class=anchor aria-hidden=true href=#common-crawl>#</a></h2><embed src=/CommonCrawl.pdf width=100% height=800px type=application/pdf><h1 id=factuality-and-hallucinations-in-llms><em>Factuality and Hallucinations in LLMs</em><a hidden class=anchor aria-hidden=true href=#factuality-and-hallucinations-in-llms>#</a></h1><p>Second talk was about Factuality and Hallucinations and was presented by Anna Rogers, University of Cophenhagen. Here are the slides she shared:</p><p>It discusses the nature of &ldquo;bullshit&rdquo; in LLMs, comparing them with human hallucination abilities, and examines proposed solutions such as RAG (Retrieval-Augmented Generation) and CoT (Chain-of-Thought), highlighting their limitations. Finally, it explores the negative impact of LLMs on the information ecosystem, with the proliferation of spam and synthetic content, and addresses the challenges in identifying and countering these issues.
<embed src=/fact.pdf width=100% height=800px type=application/pdf></p><p><strong>RAG (Retrieval-Augmented Generation) and CoT (Chain-of-Thought)</strong> are two approaches proposed to improve the factual accuracy of LLMs, but neither fully solves the problem.</p><ul><li><p><strong>RAG</strong> is a technique that involves providing LLMs with a database or external context from which to draw information during text generation. The idea is that by offering a reference context, the LLM will be able to produce more factual and accurate responses. The RAG process involves:</p><ul><li><strong>Retrieval:</strong> Extracting relevant information from an external database.</li><li><strong>Generation:</strong> Using the retrieved information as context to generate a response.</li></ul><p>However, RAG is not a perfect solution and presents several issues:</p><ul><li>It can worsen the situation if the knowledge graph is divided in such a way that test questions have no supporting evidence, causing the model to still give correct answers (5-8%) even when instructed to respond &ldquo;False.&rdquo;</li><li>The evaluation of a RAG system&rsquo;s quality is complex and depends on various criteria such as retrieval accuracy, the relevance of the generated content to the query, fidelity to sources, and correctness against underlying truth.</li><li>Evaluation metrics are often calculated by another LLM (e.g., GPT-3.5-turbo), which could introduce potential biases.</li><li>RAG does not guarantee that the LLM will convey information from the database faithfully.</li><li>There are citation issues, as a RAG model might misquote a source, which doesn’t necessarily help the model produce better results.</li></ul></li><li><p><strong>CoT</strong> is a prompting technique that involves providing the LLM with step-by-step reasoning examples, aiming to encourage it to follow a similar thought process during text generation. The idea is that by providing intermediate reasoning, the LLM will generate more accurate and understandable responses. The advantages of CoT include:</p><ul><li>In many cases, CoT works better than standard prompting.</li><li>It can improve model accuracy on certain tasks.</li><li>It may reduce sensitivity to biases, but this effect varies significantly depending on the type of bias and model.</li></ul><p>However, CoT also has limitations:</p><ul><li>In biased scenarios, zero-shot CoT prompting can worsen results.</li><li>The &ldquo;explanations&rdquo; produced by CoT are not always faithful to the actual thought process of the model.</li><li>CoT can be used to &ldquo;jailbreak&rdquo; the LLM and bypass restrictions, for example, to generate harmful content.</li></ul></li></ul><h1 id=day-2><em>Day 2</em><a hidden class=anchor aria-hidden=true href=#day-2>#</a></h1><p>Firt talk of the day was by Guilherme Penedo (Hugging Face) on FineWeb2, a recent multilingual web based dataset for LLM pretraining. The talk is centered around adaptig the processing pipelines used in English to different languages.
<embed src=/fine.pdf width=100% height=800px type=application/pdf>The second talk was presented by Jenia Jitsev & Marianna Nezhurina, interesting talk about <em>power law</em> and the problmes in quantifying generalization.
<embed src=/jenia.pdf width=100% height=800px type=application/pdf></p><embed src=/maria.pdf width=100% height=800px type=application/pdf></div><footer class=post-footer><ul class=post-tags><li><a href=https://ldomenichelli.github.io/tags/pretraining/>Pretraining</a></li><li><a href=https://ldomenichelli.github.io/tags/factuality/>Factuality</a></li></ul><nav class=paginav><a class=next href=https://ldomenichelli.github.io/posts/post6/><span class=title>Next »</span><br><span>Notes on Mechanistic Interpretability</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>Lucia</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>