<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>â„ï¸ HPLT Ã— NLPL Winter School | lucia's notes</title>
<meta name=keywords content="winterâ€‘school,pretraining,factuality,LLMs,datasets"><meta name=description content="Short notes and slide links from the Winter School in Skeikampen, Norway (Febâ€¯10â€‘14,â€¯2025)."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post5/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="â„ï¸ HPLT Ã— NLPL Winter School"><meta property="og:description" content="Short notes and slide links from the Winter School in Skeikampen, Norway (Febâ€¯10â€‘14,â€¯2025)."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post5/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="â„ï¸ HPLT Ã— NLPL Winter School"><meta name=twitter:description content="Short notes and slide links from the Winter School in Skeikampen, Norway (Febâ€¯10â€‘14,â€¯2025)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"â„ï¸ HPLT Ã— NLPL Winter School","item":"https://ldomenichelli.github.io/posts/post5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"â„ï¸ HPLT Ã— NLPL Winter School","name":"â„ï¸ HPLT Ã— NLPL Winter School","description":"Short notes and slide links from the Winter School in Skeikampen, Norway (Febâ€¯10â€‘14,â€¯2025).","keywords":["winterâ€‘school","pretraining","factuality","LLMs","datasets"],"articleBody":"ğŸ—“ DayÂ 1 â€” Monday, 10â€¯Febâ€¯2025 ğŸŒ What Is Commonâ€¯Crawl? Commonâ€¯Crawl is a huge, free snapshot of the public web.\nA nonâ€‘profit updates it every month, storing:\nBillions of HTML pages Their cleanedâ€‘up text content Extra metadata (links, timestamps, MIME types, â€¦) Why It Matters Track language change â€“ see how words, memes, and topics shift over time. Map the webâ€™s link network â€“ study which sites connect and why. Train big ML models â€“ use realâ€‘world data instead of tiny toy datasets. Because each release includes both the raw HTML and a parsed text layer, you can analyze:\nLayer What you can study Raw HTML structure Link graphs, page layout, site categories Clean text content Sentiment, topic trends, new buzzwords Perks for Researchers No crawler needed â€“ skip the cost and hassle of scraping the web yourself. Open licence â€“ anyone can share code, replicate results, and build on your work. Regular updates â€“ monthly snapshots reveal sudden spikes (e.g., when a new tech goes viral). All of this makes Commonâ€¯Crawl a goâ€‘to resource for tasks like:\nNamedâ€‘entity recognition Topic classification Question answering By pooling efforts around one massive, open dataset, researchers push the limits of NLP faster than they could alone.\nğŸ“ Common Crawl ğŸ“ FactualityÂ \u0026Â HallucinationsÂ inÂ LLMs Speaker: Annaâ€¯Rogers (Universityâ€¯ofâ€¯Copenhagen)\nâ€œLarge language models are fluentâ€¯bullshit generatorsâ€”they sound right even when theyâ€™re wrong.â€\nâ€”â€¯A.â€¯Rogers\nLLMs can drift from the truth, a problem known as hallucination. Rogers reviews two popular fixes and where they fall short:\nApproach How it works Main weakness RAG\n(Retrievalâ€‘Augmented Generation) Looks up facts in a search index or database, then feeds the snippets to the model as it writes. Bad retrieval = bad answer; citations can be incorrect or missing. CoT\n(Chainâ€‘ofâ€‘Thought prompting) Prompts the model to show stepâ€‘byâ€‘step reasoning before the final answer. â€œReasoningâ€ may be invented; method can be abused to jailbreak the model. Impact on the Web Surge in AIâ€‘generated spam and clickâ€‘bait Harder to tell real news from synthetic text New headaches for search engines and factâ€‘checkers Takeaway: RAG and CoT help, but they donâ€™t eliminate hallucinations. Better evaluation metrics and stronger guardrails are still needed.\nRAG (Retrievalâ€‘Augmented Generation) and CoT (Chainâ€‘ofâ€‘Thought) both try to make LLM answers more trustworthy, but neither is a silver bullet.\nğŸ” RAG â€” Look it up, then write How it works\nRetrieveâ€‡Find facts in a search index or database. Generateâ€‡Feed those facts to the model so it can weave them into its answer. Where it breaks\nIf the search misses the right passage, the answer is still wrong. Measuring â€œqualityâ€ is tricky: you need to score retrieval hitâ€‘rate, answer truthfulness, and source fidelityâ€”all at once. Evaluations often rely on yet another LLM, which can add bias. Even with good sources, the model may paraphrase or misquote them. ğŸ“ CoT â€” Show your thinking How it works\nGive the model examples that spell out stepâ€‘byâ€‘step reasoning. Ask it to copy that style: â€œFirst, think. Then, answer.â€ Where it breaks\nWorks great on some tasks, worse on othersâ€”especially biased ones. The â€œreasoningâ€ it prints may be made up, not its true internal logic. Attackers can use CoT to slip past safety rules (â€œjailbreakingâ€). ğŸ—“ DayÂ 2 â€” Tuesday, 11â€¯Febâ€¯2025 ğŸ“ FineWebâ€¯2 â€” Multilingual Web Data at Scale Speaker: Guilhermeâ€¯Penedo (Huggingâ€¯Face)\nThe opening talk introduced FineWebâ€¯2, a brandâ€‘new, multilingual web corpus for preâ€‘training large language models.\nPenedo explained how the team is porting and tuning the Englishâ€‘centric cleaning pipelineâ€”deduplication, language ID, toxicity filters, and moreâ€”so it works reliably across dozens of other languages.\nğŸ“ Powerâ€¯Laws \u0026 Generalization Speakers: Jeniaâ€¯Jitsevâ€¯\u0026â€¯Mariannaâ€¯Nezhurina\nThis session explored how powerâ€‘law scaling shows up in deepâ€‘learning curvesâ€”and why turning those neat mathematical fits into realâ€‘world â€œgeneralization scoresâ€ is harder than it looks. The speakers highlighted pitfalls such as noisy data, shifting task definitions, and compute limits that break the powerâ€‘law trend once models get big enough.\n## ğŸ“ *Generalization* ","wordCount":"671","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post5/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>â„ï¸ HPLT Ã— NLPL Winter School</h1><div class=post-description>Short notes and slide links from the Winter School in Skeikampen, Norway (Febâ€¯10â€‘14,â€¯2025).</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#-what-is-commoncrawl>ğŸŒ What Is <strong>Commonâ€¯Crawl</strong>?</a><ul><li><a href=#why-it-matters>Why It Matters</a></li><li><a href=#perks-for-researchers>Perks for Researchers</a></li></ul></li></ul></li><li><a href=#-common-crawl>ğŸ“ <em>Common Crawl</em></a></li><li><a href=#-factualityhallucinationsinllms>ğŸ“ FactualityÂ &Â HallucinationsÂ inÂ LLMs</a><ul><li><ul><li><a href=#impact-on-the-web>Impact on the Web</a></li></ul></li><li><a href=#-rag--look-it-up-then-write>ğŸ” RAG â€” Look it up, then write</a></li><li><a href=#-cot--show-your-thinking>ğŸ“ CoT â€” Show your thinking</a></li></ul></li></ul><ul><li><a href=#-fineweb2--multilingual-web-data-at-scale>ğŸ“ FineWebâ€¯2 â€” Multilingual Web Data at Scale</a></li><li><a href=#-powerlaws--generalization>ğŸ“ Powerâ€¯Laws & Generalization</a></li></ul></nav></div></details></div><div class=post-content><h1 id=-day1--monday-10feb2025>ğŸ—“ DayÂ 1 â€” Monday, 10â€¯Febâ€¯2025<a hidden class=anchor aria-hidden=true href=#-day1--monday-10feb2025>#</a></h1><h3 id=-what-is-commoncrawl>ğŸŒ What Is <strong>Commonâ€¯Crawl</strong>?<a hidden class=anchor aria-hidden=true href=#-what-is-commoncrawl>#</a></h3><p>Commonâ€¯Crawl is a <strong>huge, free snapshot of the public web</strong>.<br>A nonâ€‘profit updates it every month, storing:</p><ul><li><strong>Billions of HTML pages</strong></li><li>Their cleanedâ€‘up <strong>text content</strong></li><li>Extra <strong>metadata</strong> (links, timestamps, MIME types, â€¦)</li></ul><h4 id=why-it-matters>Why It Matters<a hidden class=anchor aria-hidden=true href=#why-it-matters>#</a></h4><ul><li><strong>Track language change</strong> â€“ see how words, memes, and topics shift over time.</li><li><strong>Map the webâ€™s link network</strong> â€“ study which sites connect and why.</li><li><strong>Train big ML models</strong> â€“ use realâ€‘world data instead of tiny toy datasets.</li></ul><p>Because each release includes both the <strong>raw HTML</strong> and a parsed text layer, you can analyze:</p><table><thead><tr><th>Layer</th><th>What you can study</th></tr></thead><tbody><tr><td>Raw HTML structure</td><td>Link graphs, page layout, site categories</td></tr><tr><td>Clean text content</td><td>Sentiment, topic trends, new buzzwords</td></tr></tbody></table><hr><h4 id=perks-for-researchers>Perks for Researchers<a hidden class=anchor aria-hidden=true href=#perks-for-researchers>#</a></h4><ol><li><strong>No crawler needed</strong> â€“ skip the cost and hassle of scraping the web yourself.</li><li><strong>Open licence</strong> â€“ anyone can share code, replicate results, and build on your work.</li><li><strong>Regular updates</strong> â€“ monthly snapshots reveal sudden spikes (e.g., when a new tech goes viral).</li></ol><p>All of this makes Commonâ€¯Crawl a goâ€‘to resource for tasks like:</p><ul><li>Namedâ€‘entity recognition</li><li>Topic classification</li><li>Question answering</li></ul><p>By pooling efforts around one massive, open dataset, researchers push the limits of NLP faster than they could alone.</p><h2 id=-common-crawl>ğŸ“ <em>Common Crawl</em><a hidden class=anchor aria-hidden=true href=#-common-crawl>#</a></h2><embed src=/CommonCrawl.pdf width=100% height=800px type=application/pdf><h2 id=-factualityhallucinationsinllms>ğŸ“ FactualityÂ &Â HallucinationsÂ inÂ LLMs<a hidden class=anchor aria-hidden=true href=#-factualityhallucinationsinllms>#</a></h2><p><strong>Speaker:</strong> Annaâ€¯Rogers (Universityâ€¯ofâ€¯Copenhagen)</p><blockquote><p>â€œLarge language models are <strong>fluentâ€¯bullshit generators</strong>â€”they sound right even when theyâ€™re wrong.â€<br><small>â€”â€¯A.â€¯Rogers</small></p></blockquote><p>LLMs can drift from the truth, a problem known as <strong>hallucination</strong>. Rogers reviews two popular fixes and where they fall short:</p><table><thead><tr><th>Approach</th><th>How it works</th><th>Main weakness</th></tr></thead><tbody><tr><td><strong>RAG</strong><br>(Retrievalâ€‘Augmented Generation)</td><td>Looks up facts in a search index or database, then feeds the snippets to the model as it writes.</td><td>Bad retrieval = bad answer; citations can be incorrect or missing.</td></tr><tr><td><strong>CoT</strong><br>(Chainâ€‘ofâ€‘Thought prompting)</td><td>Prompts the model to show stepâ€‘byâ€‘step reasoning before the final answer.</td><td>â€œReasoningâ€ may be invented; method can be abused to jailbreak the model.</td></tr></tbody></table><h4 id=impact-on-the-web>Impact on the Web<a hidden class=anchor aria-hidden=true href=#impact-on-the-web>#</a></h4><ul><li>Surge in AIâ€‘generated spam and clickâ€‘bait</li><li>Harder to tell real news from synthetic text</li><li>New headaches for search engines and factâ€‘checkers</li></ul><p><strong>Takeaway:</strong> RAG and CoT help, but they donâ€™t eliminate hallucinations. Better evaluation metrics and stronger guardrails are still needed.</p><embed src=/fact.pdf width=100% height=800px type=application/pdf><p><strong>RAG (Retrievalâ€‘Augmented Generation)</strong> and <strong>CoT (Chainâ€‘ofâ€‘Thought)</strong> both try to make LLM answers more trustworthy, but neither is a silver bullet.</p><hr><h3 id=-rag--look-it-up-then-write>ğŸ” RAG â€” Look it up, then write<a hidden class=anchor aria-hidden=true href=#-rag--look-it-up-then-write>#</a></h3><p><strong>How it works</strong></p><ol><li><strong>Retrieve</strong>â€‡Find facts in a search index or database.</li><li><strong>Generate</strong>â€‡Feed those facts to the model so it can weave them into its answer.</li></ol><p><strong>Where it breaks</strong></p><ul><li>If the search misses the right passage, the answer is still wrong.</li><li>Measuring â€œqualityâ€ is tricky: you need to score retrieval hitâ€‘rate, answer truthfulness, and source fidelityâ€”all at once.</li><li>Evaluations often rely on yet another LLM, which can add bias.</li><li>Even with good sources, the model may paraphrase or misquote them.</li></ul><hr><h3 id=-cot--show-your-thinking>ğŸ“ CoT â€” Show your thinking<a hidden class=anchor aria-hidden=true href=#-cot--show-your-thinking>#</a></h3><p><strong>How it works</strong></p><ol><li>Give the model examples that spell out stepâ€‘byâ€‘step reasoning.</li><li>Ask it to copy that style: <em>â€œFirst, think. Then, answer.â€</em></li></ol><p><strong>Where it breaks</strong></p><ul><li>Works great on some tasks, worse on othersâ€”especially biased ones.</li><li>The â€œreasoningâ€ it prints may be made up, not its true internal logic.</li><li>Attackers can use CoT to slip past safety rules (â€œjailbreakingâ€).</li></ul><h1 id=-day2--tuesday-11feb2025>ğŸ—“ DayÂ 2 â€” Tuesday, 11â€¯Febâ€¯2025<a hidden class=anchor aria-hidden=true href=#-day2--tuesday-11feb2025>#</a></h1><h2 id=-fineweb2--multilingual-web-data-at-scale>ğŸ“ FineWebâ€¯2 â€” Multilingual Web Data at Scale<a hidden class=anchor aria-hidden=true href=#-fineweb2--multilingual-web-data-at-scale>#</a></h2><p><strong>Speaker:</strong> Guilhermeâ€¯Penedo (Huggingâ€¯Face)</p><p>The opening talk introduced <strong>FineWebâ€¯2</strong>, a brandâ€‘new, multilingual web corpus for preâ€‘training large language models.<br>Penedo explained how the team is <strong>porting and tuning the Englishâ€‘centric cleaning pipeline</strong>â€”deduplication, language ID, toxicity filters, and moreâ€”so it works reliably across dozens of other languages.</p><embed src=/fine.pdf width=100% height=800px type=application/pdf><h2 id=-powerlaws--generalization>ğŸ“ Powerâ€¯Laws & Generalization<a hidden class=anchor aria-hidden=true href=#-powerlaws--generalization>#</a></h2><p><strong>Speakers:</strong> Jeniaâ€¯Jitsevâ€¯&â€¯Mariannaâ€¯Nezhurina</p><p>This session explored how <strong>powerâ€‘law scaling</strong> shows up in deepâ€‘learning curvesâ€”and why turning those neat mathematical fits into realâ€‘world â€œgeneralization scoresâ€ is harder than it looks. The speakers highlighted pitfalls such as noisy data, shifting task definitions, and compute limits that break the powerâ€‘law trend once models get big enough.</p><embed src=/jenia.pdf width=100% height=800px type=application/pdf>## ğŸ“ *Generalization*
<embed src=/maria.pdf width=100% height=800px type=application/pdf></div><footer class=post-footer><ul class=post-tags><li><a href=https://ldomenichelli.github.io/tags/winterschool/>Winterâ€‘school</a></li><li><a href=https://ldomenichelli.github.io/tags/pretraining/>Pretraining</a></li><li><a href=https://ldomenichelli.github.io/tags/factuality/>Factuality</a></li><li><a href=https://ldomenichelli.github.io/tags/llms/>LLMs</a></li><li><a href=https://ldomenichelli.github.io/tags/datasets/>Datasets</a></li></ul><nav class=paginav><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next Â»</span><br><span>Embeddings space ğ–¦¹×‚ â‚ŠËšâŠ¹â‹†</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>â€¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>â€¢
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>