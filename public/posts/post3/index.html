<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ACL Vienna 2025 ğŸ‡¦ğŸ‡¹ | lucia's notes</title>
<meta name=keywords content><meta name=description content="Here are the notes from some talks I attented at ACL 2025 in Vienna!

    Eye-tracking 

Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (PubMed)
Data is maturing: Multilingual, multiâ€‘lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made highâ€‘quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv)
Models & evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned. 
Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology)


Eyeâ€‘tracking 101 ğŸ‘ï¸
Some basic concepts:"><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post3/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="ACL Vienna 2025 ğŸ‡¦ğŸ‡¹"><meta property="og:description" content="Here are the notes from some talks I attented at ACL 2025 in Vienna!

    Eye-tracking 

Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (PubMed)
Data is maturing: Multilingual, multiâ€‘lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made highâ€‘quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv)
Models & evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned. 
Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology)


Eyeâ€‘tracking 101 ğŸ‘ï¸
Some basic concepts:"><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post3/"><meta property="og:image" content="https://ldomenichelli.github.io/posts/post3/img/acl1.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/posts/post3/img/acl1.png"><meta name=twitter:title content="ACL Vienna 2025 ğŸ‡¦ğŸ‡¹"><meta name=twitter:description content="Here are the notes from some talks I attented at ACL 2025 in Vienna!

    Eye-tracking 

Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (PubMed)
Data is maturing: Multilingual, multiâ€‘lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made highâ€‘quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv)
Models & evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned. 
Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology)


Eyeâ€‘tracking 101 ğŸ‘ï¸
Some basic concepts:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ACL Vienna 2025 ğŸ‡¦ğŸ‡¹","item":"https://ldomenichelli.github.io/posts/post3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ACL Vienna 2025 ğŸ‡¦ğŸ‡¹","name":"ACL Vienna 2025 ğŸ‡¦ğŸ‡¹","description":"Here are the notes from some talks I attented at ACL 2025 in Vienna!\nEye-tracking Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (PubMed) Data is maturing: Multilingual, multiâ€‘lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made highâ€‘quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv) Models \u0026amp; evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned. Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology) Eyeâ€‘tracking 101 ğŸ‘ï¸ Some basic concepts:\n","keywords":[],"articleBody":"Here are the notes from some talks I attented at ACL 2025 in Vienna!\nEye-tracking Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (PubMed) Data is maturing: Multilingual, multiâ€‘lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made highâ€‘quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv) Models \u0026 evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned. Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology) Eyeâ€‘tracking 101 ğŸ‘ï¸ Some basic concepts:\nFixations \u0026 saccades. Reading is a hopâ€‘andâ€‘pause routine: brief saccades (tens of ms) between ~200â€“250â€¯ms fixations; perception occurs mostly during fixations, not saccades. The classic eyeâ€‘mind assumption: minimal lag between whatâ€™s fixated and whatâ€™s processed. (andrewd.ces.clemson.edu, PubMed)\nPerceptual span. Highâ€‘acuity foveal vision is coneâ€‘rich, while parafoveal vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (NCBI, PubMed Central, Frontiers)\nReading measures youâ€™ll see in papers: skip rate, firstâ€‘fixation duration, gaze duration, regression rate, goâ€‘past duration, total fixation time. These map fixations to Areasâ€‘ofâ€‘Interest (AoIs) at token/region level. Hardware \u0026 sampling. For reading studies, stationary trackers with head stabilization and â‰¥200â€¯Hz sampling are typical to get characterâ€‘level precision and reliable on/offsets. Pipelines \u0026 data structure. Raw samples â†’ fixation detection â†’ map to AoIs â†’ compute measures per readerÃ—word. Remember: data is not i.i.d. (nested readers/texts), which affects stats and ML splits. Lowâ€‘tech alternatives. When eyeâ€‘tracking isnâ€™t feasible: Selfâ€‘Paced Reading (SPR), Maze, mouseâ€‘tracking can capture useful online signalsâ€”with different tradeâ€‘offs. (PubMed Central, SpringerLink, SpringerLink)\nDatasets \u0026 tools ğŸ“š MECO (Multilingual Eyeâ€‘movement Corpus): large, coordinated, crossâ€‘linguistic reading data; Waveâ€¯2 keeps expanding. (meco-read.com, PubMed Central) MultiplEYE (COST Action): enabling multilingual eyeâ€‘trackingâ€‘whileâ€‘reading at scale; infrastructure, protocols, and community. (multipleye.eu, Radboud Universiteit) OneStop Eye Movements: 360 native readers, 2.6M tokens; great for comprehensionâ€‘linked analyses. (lacclab.github.io) Provo, ZuCo, Dundee, CELERâ€¦ Useful complements for different tasks and populations. (PubMed) pymovements: openâ€‘source package to download datasets and preprocess gaze (event detection, angles/velocities, etc.). (arXiv, pymovements.readthedocs.io) ![[Pasted image 20250729095400.png]] Using gaze in NLP models ğŸ”§ Wordâ€‘level alignment \u0026 embeddings. Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gazeâ€‘motivated positional encodings and human attention signals. Synthetic scanpaths help scale. Since human gaze is scarce, Eyettentionâ€‘style scanpath generators and followâ€‘ups inject synthetic gaze to fineâ€‘tune LMs, improving GLUE tasks (especially lowâ€‘resource). (arXiv, ACL Anthology, ACL Anthology)\nTaskâ€‘specific multitask learning. Training to predict reading measures jointly with downstream tasks (e.g., QA with question preview vs. ordinary reading) can induce more humanâ€‘like attention. (ACL Anthology) What to expect. Reported gains are real but often modest without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial. Examples \u0026 pointers: NER with gaze, compression/paraphrase, readability, parsingâ€”plus general â€œgazeâ€‘augmented PLMs.â€ (ACL Anthology, ACL Anthology, arXiv)\nUsing gaze to evaluate NLP ğŸ“ Behavioral evaluation uses online human signalsâ€”complementing labels or preferences. We saw applications to MT (reading effort), summarization (human vs. model saliency), and readability (readingâ€‘ease metrics). (SpringerLink, ACL Anthology)\nCase study â€” Automatic Readability Assessment (ARA). A new eyeâ€‘trackingâ€‘based benchmark correlates model scores with reading speed, skip rate, regressions, and total fixation time, revealing weak spots of classic readability formulas. Promising direction for cognitive evaluation. (hundred.org)\nPsycholinguistics NLP Surprisal \u0026 RTs. Foundational results show a strong relation between LM surprisal and reading times; this holds across languages and for many modern LMsâ€”with nuances. (lexplore.com, lexplore.com) *Classics to know: Surprisal theory, Dependency Locality Theory, Uniform Information Density, Cueâ€‘based retrieval/ACTâ€‘Râ€”usually operationalized via parsers/LMs. (eyetechds.com)\nControlled tests. Agreement phenomena with GPTâ€‘2 surprisal; embeddings as cognitive features for memory retrieval. (Appsource â€“ Business Apps, eyetechds.com) Are LLMs aligned with human reading? ğŸ¤–ğŸ§â€â™€ï¸ Itâ€™s complicated (and active in 2023â€“2025):\nBigger isnâ€™t always better: Larger Transformers can fit worse to human RTs than smaller ones (surprisalâ€‘RT link weakens with size). (ACL Anthology) â€¦but layer matters: Intermediate layers may reverse that trend. (dfki.de) Individual differences: Surprisal better predicts firstâ€‘pass RTs for lower verbal IQ readers; entropy better fits those with higher working memory. (PubMed) Text \u0026 decoding matter: PP varies across generation strategies and reading measures; evaluating produced texts against human reading is informative. (ACL Anthology, ACL Anthology) Add cognitive bias: Injecting recency biases (e.g., ALiBi) improves LM fit to reading times. (ACL Anthology, ACL Anthology) Modeling eye movements themselves ğŸ› ï¸ Cognitive models (fewer, interpretable parameters): Eâ€‘Z Reader, SWIFT, SEAM, OB1â€‘Reader. ML/NLP models (dataâ€‘hungry, highâ€‘capacity): Eyettention, ScanDLâ€¯2.0, SPâ€‘EyeGAN. The recent trend is to combine strengths (e.g., selfâ€‘supervised frameworks grounded in cognitive constraints). (PubMed, ScienceDirect, arXiv, ACM Digital Library, Zora, ACM Digital Library)\nHumanâ€‘centered applications ğŸŒ Language assessment (L2): Eye movements carry proficiency signals (e.g., EyeScoreâ€‘style similarity to L1 prototypes). Reading impairment screening/monitoring: Commercial tools (e.g., Lexplore) and research platforms point to scalable screening and longitudinal tracking. (eyetechusa.com) Reading comprehension modeling: Predicting comprehension from gaze during QA is an emerging task on OneStop. (arXiv) How to get started Pick a dataset that matches your question (MECO/OneStop/Provo/etc.). (meco-read.com, lacclab.github.io, PubMed) Mind the structure (reader/text effects) and choose proper splits/stats. Use a pipeline (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (arXiv) Decide your integration: (a) features/embeddings, (b) auxiliary losses (multitask), or (c) synthetic gaze + LM fineâ€‘tuning. (ACL Anthology, ACL Anthology) Evaluate cognitively: add behavioral metrics (e.g., ARA with eyeâ€‘tracking) alongside standard accuracy. (hundred.org) References \u0026 links ğŸ”— Tutorial slides: Eye Tracking and NLP (ACL 2025) â€” many figures and examples here are adapted from the tutorial. Foundations: Raynerâ€™s classic review on eye movements \u0026 cognition; eyeâ€‘mind assumption background. (andrewd.ces.clemson.edu, PubMed) Perceptual span \u0026 physiology: asymmetries and fovea/cone density. (Frontiers, NCBI) Datasets/initiatives: MECO, MultiplEYE, OneStop, Provo; toolkit pymovements. (meco-read.com, multipleye.eu, lacclab.github.io, PubMed, arXiv) Gaze for modeling: NER with gaze; synthetic scanpaths + GLUE; multitask QA. (ACL Anthology, ACL Anthology, ACL Anthology, ACL Anthology) Behavioral eval: MT (eyeâ€‘tracking), summarization with eyeâ€‘gaze, readability via eyeâ€‘tracking. (SpringerLink, ACL Anthology, hundred.org) Psycholinguistic links: Smith \u0026â€¯Levy; Demberg \u0026â€¯Keller; Shain etâ€¯al.; Wilcox etâ€¯al.; Ryu \u0026â€¯Lewis; Smith \u0026â€¯Vasishth. (lexplore.com, eyetechds.com, lexplore.com, Appsource â€“ Business Apps, eyetechds.com) Alignment \u0026 recency bias: Oh \u0026â€¯Schuler (TACL 2023); Kuribayashi etâ€¯al. (2025); Haller etâ€¯al. (2024); Bolliger etâ€¯al. (2024); deâ€¯Varda \u0026â€¯Marelli (2024); Clark etâ€¯al. (COLING 2025). (ACL Anthology, dfki.de, PubMed, ACL Anthology, ACL Anthology, ACL Anthology) Scanpath modeling: Eyettention; ScanDLâ€¯2.0; SPâ€‘EyeGAN; SEAM; OB1â€‘Reader. (arXiv, Zora, ACM Digital Library, arXiv, PubMed) Synthetic data for NLP At a glance We canâ€™t label everything: synthetic data fills gaps when scraping/manual labels/privacy limits hit. Good synthetic data is task-tailored, sized right, and clean â€” but beware distribution shift. Evaluate two ways: extrinsic (downstream task) vs intrinsic (what the data itself looks like). Diversity sometimes beats raw correctness; noisy but varied sets can still improve models. Best results come from Human â†”ï¸ AI collaboration + strong filtering before use. Where do we get data? Scraping the web (scale, but licensing/noise). Manual labeling (accurate, expensive). Product/system data (useful but privacy-sensitive). Creative curation (high quality, limited volume).\nSynthetic data tries to extend/augment all of the above. What makes â€œgoodâ€ synthetic data? Flexible \u0026 task-specific: format, difficulty, and style match your target task. Appropriate size: enough to move the needle, not so much it drowns real data. Clean: minimal contradictions/formatting errors. Aligned distributions: cover the same kinds of inputs/labels you expect in production. Why the warning? Because the real joint distribution often differs from the synthetic one:\n( P_{\\text{true}}(x,y) \\neq P_{\\text{synth}}(x,y) ).\nMismatch shows up as off-manifold inputs, wrong labels, or flawed reasoning traces.\nHow do we evaluate synthetic data? Extrinsic: train/evaluate models on tasks with/without the synthetic set.\nâœ… Directly answers â€œdoes it help?â€. âŒ Costly, indirect diagnostic signal. Intrinsic: inspect the data/generation process itself.\nCorrectness: e.g., spot-checking/self-instruct style manual audits. Diversity/coverage: does it span plausible inputs? (e.g., DataTuneâ€™s bigram diversity as a quick signal). Privacy, fairness, distributional similarity: toolkits like SynthTextEval help stress-test. Model choice as a proxy: pick the generator by how well its synthetic data matches human-written (e.g., AgoraBench). How is synthetic data created? Knowledge distillation (Hinton â€™15 â†’ sentence-level): train a student to mimic a teacher; cheap SFT, keeps style on-target. In-context generation: prompt LMs to produce new labeled examples for arbitrary tasks (Schick \u0026 SchÃ¼tze â€™21). Self-training/bootstrapping: fine-tune on the modelâ€™s own outputs (Wang et al. â€™22). Observed effects: generated text is often more diverse (e.g., BERTScore) but only ~Â½ examples fully correctâ€”and models can still improve thanks to coverage. Common patterns\nSampling-based generation: temperature/top-p, curriculum over difficulty. Instruction back-translation: given an answer (y), generate an instruction (x) that (y) would satisfy. Transform existing data: retrieve/convert to the target format (QA from StackExchange; ground to KGs; rephrase documents for pretraining, Maini â€™24). Humanâ€“AI collaboration: LLM drafts, humans edit/verify (creativity â†”ï¸ correctness). Symbolic/programmable data: pretrain on formal languages/grammars to improve generalization (Hu â€™24). #Filter before you use it\nDiversity filters: keep sets that are far apart (ROUGE-L, embedding cosine, semantic tags). Gradient diversity: prefer examples that produce different loss gradients â†’ more robust models. Quality filters: pick highest-reward responses (e.g., RAFT-style ranking). Correctness filters: keep chains of thought that reach the right answer; drop inconsistent traces. Where synthetic data fits in the pipeline Pretraining\nWhen real data plateaus: rephrase corpora, verbalize knowledge bases, or mix in formal languages for structure. Domain adaptation to reduce hallucinations on niche topics. Supervised fine-tuning (SFT)\nDistillation is cheap but can be â€œstyle-lockedâ€. Self-Instruct / Evol-Instruct to grow instruction variance. MAmmoTH-style transform/extract tasks (e.g., convert docs to QA). RL \u0026 feedback\nMinimal supervision, negative examples, adaptation to own token distribution. Synthetic rewards (e.g., Prometheus evaluators; checklist-style rubrics). Choosing a â€œjudgeâ€: agreement with human prefs (RewardBench), agreement with benchmarks (re-ranking), effectiveness inside RL loops. â€œTeacherâ€ qualities: good accuracy and low variance; even non-RL textual feedback can helpâ€”mainly when the base model is already strong. Reasoning\nScale up inference to get longer CoT/PoT traces; pipelines like OpenThoughts curate reasoning data. Even noisy reasoning data can help; more (curated) data â†’ better reasoning. Code\nCodeAlpaca, WaveCoder, WizardCoder, Magicoder, etc. Train with execution feedback (tests, runtimes). Useful data types: single-turn, simulated multi-turn, â€œfix-the-bugâ€, and near-duplicate (LeetCode-style) variants. Tools \u0026 agents\nGorilla (API-calling), ToolLLM, ToRa (tool-integrated math), AgentTuning, CodeActInstruct, AgentE, â€œGPT-4-toolsâ€ style setups. Multimodal / Multilingual\nLLaVA-style visual instruction tuning; multilingual instruction pipelines. Limitations \u0026 open questions Synthetic sets still trail real data in size, diversity, and distribution; production usage (e.g., Anthropicâ€™s Clio) shows gaps. In controlled tests, synthetic often underperforms; artifacts creep in. Synthetic eval data can overestimate model performance. Model collapse risk under recursive self-training (Shumailov â€™23). We should measure instance-level quality, not just dataset averages. Governance: â€œdistillation-friendlyâ€ models, usage restrictions, provenance tracking. Practical checklist (what Iâ€™d do) Define the target: task format, difficulty mix, and metrics. Generate broad, then filter hard: diversity â†’ quality â†’ correctness. Mix with real data: keep an anchor set for sanity checks. Evaluate both ways: quick intrinsic dashboards + periodic extrinsic runs. Close the loop: human spot-edits, error mining, and focused regeneration. Watch drift: compare (P_{\\text{true}}(x,y)) vs (P_{\\text{synth}}(x,y)) over time. ","wordCount":"1876","inLanguage":"en","image":"https://ldomenichelli.github.io/posts/post3/img/acl1.png","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post3/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script><script src="/assets/js/analytics.js" async></script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>ACL Vienna 2025 ğŸ‡¦ğŸ‡¹</h1><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#eyetracking-101->Eyeâ€‘tracking 101 ğŸ‘ï¸</a></li></ul></nav></div></details></div><div class=post-content><p>Here are the notes from some talks I attented at ACL 2025 in Vienna!</p><details id=talk-eye><summary>Eye-tracking</summary><ul><li><strong>Why gaze?</strong> Eye movements reflect <em>online</em> processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (<a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Data is maturing:</strong> Multilingual, multiâ€‘lab efforts (e.g., <strong>MECO</strong>, <strong>MultiplEYE</strong>) + tooling (e.g., <strong>pymovements</strong>) have made highâ€‘quality datasets and pipelines more accessible. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE â€“ Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Models & evals:</strong> Gaze can <strong>improve</strong> certain NLP tasks and also <strong>evaluate</strong> systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned.</li><li><strong>Open debates:</strong> How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding <strong>recency biases</strong> can help fit human behavior. (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h2 id=eyetracking-101->Eyeâ€‘tracking 101 ğŸ‘ï¸<a hidden class=anchor aria-hidden=true href=#eyetracking-101->#</a></h2><p>Some basic concepts:</p><p><strong>Fixations & saccades.</strong> Reading is a hopâ€‘andâ€‘pause routine: brief saccades (tens of ms) between ~200â€“250â€¯ms fixations; perception occurs mostly during fixations, not saccades. The classic <strong>eyeâ€‘mind assumption</strong>: minimal lag between whatâ€™s fixated and whatâ€™s processed. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</p><p><input type=checkbox id=zoomCheck-ee0e7 hidden>
<label for=zoomCheck-ee0e7><img class=zoomCheck loading=lazy decoding=async src=./img/fix.png alt=1>
</label><strong>Perceptual span.</strong> Highâ€‘acuity <strong>foveal</strong> vision is coneâ€‘rich, while <strong>parafoveal</strong> vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (<a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3075059/?utm_source=chatgpt.com" title="Eye movements, the perceptual span, and reading speed - PMC">PubMed Central</a>, <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>)</p><p><input type=checkbox id=zoomCheck-d495c hidden>
<label for=zoomCheck-d495c><img class=zoomCheck loading=lazy decoding=async src=./img/fovea.png alt></label></p><p><strong>Reading measures youâ€™ll see in papers:</strong>
<em>skip rate, firstâ€‘fixation duration, gaze duration, regression rate, goâ€‘past duration, total fixation time</em>. These map fixations to Areasâ€‘ofâ€‘Interest (AoIs) at token/region level.</p><p><input type=checkbox id=zoomCheck-9015d hidden>
<label for=zoomCheck-9015d><img class=zoomCheck loading=lazy decoding=async src=./img/gaze1.png alt></label></p><p><strong>Hardware & sampling.</strong> For reading studies, stationary trackers with head stabilization and <strong>â‰¥200â€¯Hz</strong> sampling are typical to get characterâ€‘level precision and reliable on/offsets.</p><p><strong>Pipelines & data structure.</strong> Raw samples â†’ fixation detection â†’ map to AoIs â†’ compute measures per readerÃ—word. Remember: <strong>data is not i.i.d.</strong> (nested readers/texts), which affects stats and ML splits.</p><p><strong>Lowâ€‘tech alternatives.</strong> When eyeâ€‘tracking isnâ€™t feasible: <strong>Selfâ€‘Paced Reading (SPR)</strong>, <strong>Maze</strong>, <strong>mouseâ€‘tracking</strong> can capture useful online signalsâ€”with different tradeâ€‘offs. (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12058810/?utm_source=chatgpt.com" title="A multiverse analysis of cleaning and analyzing procedures of eye ...">PubMed Central</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02536-8?utm_source=chatgpt.com" title="PoTeC: A German naturalistic eye-tracking-while-reading corpus">SpringerLink</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02517-x?utm_source=chatgpt.com" title="A Persian sentence reading corpus of eye movements">SpringerLink</a>)</p><p><input type=checkbox id=zoomCheck-3f0e6 hidden>
<label for=zoomCheck-3f0e6><img class=zoomCheck loading=lazy decoding=async src=./img/alter.png alt></label></p><hr><h1 id=datasets--tools-->Datasets & tools ğŸ“š<a hidden class=anchor aria-hidden=true href=#datasets--tools-->#</a></h1><ul><li><strong>MECO</strong> (Multilingual Eyeâ€‘movement Corpus): large, coordinated, crossâ€‘linguistic reading data; Waveâ€¯2 keeps expanding. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12246088/?utm_source=chatgpt.com" title="Wave 2 of the Multilingual Eye-Movement Corpus (MECO): New text ...">PubMed Central</a>)</li><li><strong>MultiplEYE (COST Action)</strong>: enabling multilingual eyeâ€‘trackingâ€‘whileâ€‘reading at scale; infrastructure, protocols, and community. (<a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE â€“ Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://www.ru.nl/en/research/research-projects/multipleye?utm_source=chatgpt.com" title="MultiplEYE | Radboud University">Radboud Universiteit</a>)</li><li><strong>OneStop Eye Movements</strong>: 360 native readers, 2.6M tokens; great for comprehensionâ€‘linked analyses. (<a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>)</li><li><strong>Provo, ZuCo, Dundee, CELERâ€¦</strong> Useful complements for different tasks and populations. (<a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>pymovements</strong>: openâ€‘source package to <strong>download</strong> datasets and <strong>preprocess</strong> gaze (event detection, angles/velocities, etc.). (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>, <a href="https://pymovements.readthedocs.io/en/v0.17.0/tutorials/preprocessing-raw-data.html?utm_source=chatgpt.com" title="Preprocessing Raw Gaze Data - the pymovements documentation!">pymovements.readthedocs.io</a>) ![[Pasted image 20250729095400.png]]</li></ul><hr><h1 id=using-gaze-in-nlp-models->Using gaze <strong>in</strong> NLP models ğŸ”§<a hidden class=anchor aria-hidden=true href=#using-gaze-in-nlp-models->#</a></h1><p><strong>Wordâ€‘level alignment & embeddings.</strong> Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gazeâ€‘motivated positional encodings and human attention signals.</p><p><input type=checkbox id=zoomCheck-2e7a3 hidden>
<label for=zoomCheck-2e7a3><img class=zoomCheck loading=lazy decoding=async src=./img/1.png alt>
</label><input type=checkbox id=zoomCheck-579e1 hidden>
<label for=zoomCheck-579e1><img class=zoomCheck loading=lazy decoding=async src=./img/2.png alt>
</label><input type=checkbox id=zoomCheck-d6034 hidden>
<label for=zoomCheck-d6034><img class=zoomCheck loading=lazy decoding=async src=./img/3.png alt></label></p><p><strong>Synthetic scanpaths help scale.</strong> Since human gaze is scarce, <strong>Eyettention</strong>â€‘style scanpath generators and followâ€‘ups inject <em>synthetic</em> gaze to fineâ€‘tune LMs, improving <strong>GLUE</strong> tasks (especially lowâ€‘resource). (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>)</p><p><strong>Taskâ€‘specific multitask learning.</strong> Training to predict reading measures jointly with downstream tasks (e.g., <strong>QA</strong> with question preview vs. ordinary reading) can induce more humanâ€‘like attention. (<a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)
<input type=checkbox id=zoomCheck-1c412 hidden>
<label for=zoomCheck-1c412><img class=zoomCheck loading=lazy decoding=async src=./img/4.png alt>
</label><strong>What to expect.</strong> Reported gains are real but often <strong>modest</strong> without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial.</p><p><strong>Examples & pointers:</strong> NER with gaze, compression/paraphrase, readability, parsingâ€”plus general â€œgazeâ€‘augmented PLMs.â€ (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.conll-1.8.pdf?utm_source=chatgpt.com" title="[PDF] A Comparative Study on Textual Saliency of Styles from Eye ...">ACL Anthology</a>, <a href="https://arxiv.org/pdf/2310.14676?utm_source=chatgpt.com" title="[PDF] arXiv:2310.14676v1 [cs.CL] 23 Oct 2023">arXiv</a>)</p><hr><h1 id=using-gaze-to-evaluate-nlp->Using gaze to <strong>evaluate</strong> NLP ğŸ“<a hidden class=anchor aria-hidden=true href=#using-gaze-to-evaluate-nlp->#</a></h1><p><strong>Behavioral evaluation</strong> uses <em>online</em> human signalsâ€”complementing labels or preferences. We saw applications to <strong>MT</strong> (reading effort), <strong>summarization</strong> (human vs. model saliency), and <strong>readability</strong> (readingâ€‘ease metrics). (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>)</p><p><strong>Case study â€” Automatic Readability Assessment (ARA).</strong> A new eyeâ€‘trackingâ€‘based benchmark correlates model scores with <strong>reading speed</strong>, <strong>skip rate</strong>, <strong>regressions</strong>, and <strong>total fixation time</strong>, revealing weak spots of classic readability formulas. Promising direction for <strong>cognitive evaluation</strong>. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</p><hr><h1 id=psycholinguistics-nlp>Psycholinguistics NLP<a hidden class=anchor aria-hidden=true href=#psycholinguistics-nlp>#</a></h1><ul><li><strong>Surprisal & RTs.</strong> Foundational results show a strong relation between <strong>LM surprisal</strong> and <strong>reading times</strong>; this holds across languages and for many modern LMsâ€”with nuances. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>)</li></ul><p><input type=checkbox id=zoomCheck-b130c hidden>
<label for=zoomCheck-b130c><img class=zoomCheck loading=lazy decoding=async src=./img/5.png alt>
</label>*<strong>Classics to know:</strong> <em>Surprisal theory</em>, <em>Dependency Locality Theory</em>, <em>Uniform Information Density</em>, <em>Cueâ€‘based retrieval/ACTâ€‘R</em>â€”usually operationalized via parsers/LMs. (<a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>)</p><ul><li><strong>Controlled tests.</strong> Agreement phenomena with GPTâ€‘2 surprisal; embeddings as cognitive features for memory retrieval. (<a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource â€“ Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li></ul><hr><h1 id=are-llms-aligned-with-human-reading->Are LLMs aligned with human reading? ğŸ¤–ğŸ§â€â™€ï¸<a hidden class=anchor aria-hidden=true href=#are-llms-aligned-with-human-reading->#</a></h1><p>Itâ€™s <strong>complicated</strong> (and active in 2023â€“2025):</p><p><input type=checkbox id=zoomCheck-b47b4 hidden>
<label for=zoomCheck-b47b4><img class=zoomCheck loading=lazy decoding=async src=./img/6.png alt></label></p><ul><li><strong>Bigger isnâ€™t always better:</strong> Larger Transformers can fit <em>worse</em> to human RTs than smaller ones (surprisalâ€‘RT link weakens with size). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>)</li><li><strong>â€¦but layer matters:</strong> <strong>Intermediate layers</strong> may reverse that trend. (<a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>)</li><li><strong>Individual differences:</strong> <strong>Surprisal</strong> better predicts firstâ€‘pass RTs for <strong>lower verbal IQ</strong> readers; <strong>entropy</strong> better fits those with <strong>higher working memory</strong>. (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li><li><strong>Text & decoding matter:</strong> PP varies across <strong>generation strategies</strong> and <strong>reading measures</strong>; evaluating <em>produced</em> texts against human reading is informative. (<a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14/?utm_source=chatgpt.com" title="On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>)</li><li><strong>Add cognitive bias:</strong> Injecting <strong>recency biases</strong> (e.g., <strong>ALiBi</strong>) improves LM fit to reading times. (<a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517.pdf?utm_source=chatgpt.com" title="[PDF] Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h1 id=modeling-eye-movements-themselves->Modeling eye movements themselves ğŸ› ï¸<a hidden class=anchor aria-hidden=true href=#modeling-eye-movements-themselves->#</a></h1><p><strong>Cognitive models</strong> (fewer, interpretable parameters): <strong>Eâ€‘Z Reader</strong>, <strong>SWIFT</strong>, <strong>SEAM</strong>, <strong>OB1â€‘Reader</strong>. <strong>ML/NLP models</strong> (dataâ€‘hungry, highâ€‘capacity): <strong>Eyettention</strong>, <strong>ScanDLâ€¯2.0</strong>, <strong>SPâ€‘EyeGAN</strong>. The recent trend is to combine strengths (e.g., selfâ€‘supervised frameworks grounded in cognitive constraints). (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0749596X23000955?utm_source=chatgpt.com" title="SEAM: An integrated activation-coupled model of sentence ...">ScienceDirect</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://dl.acm.org/doi/10.1145/3591131?utm_source=chatgpt.com" title="An Attention-based Dual-Sequence Model for Predicting Human ...">ACM Digital Library</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>)</p><hr><h1 id=humancentered-applications->Humanâ€‘centered applications ğŸŒ<a hidden class=anchor aria-hidden=true href=#humancentered-applications->#</a></h1><ul><li><strong>Language assessment (L2):</strong> Eye movements carry proficiency signals (e.g., <strong>EyeScore</strong>â€‘style similarity to L1 prototypes).</li><li><strong>Reading impairment screening/monitoring:</strong> Commercial tools (e.g., <strong>Lexplore</strong>) and research platforms point to scalable screening and longitudinal tracking. (<a href="https://eyetechusa.com/?utm_source=chatgpt.com" title="eye model, episcleral venomanometer, scleral depressors">eyetechusa.com</a>)</li><li><strong>Reading comprehension modeling:</strong> Predicting comprehension from gaze during QA is an emerging task on <strong>OneStop</strong>. (<a href="https://arxiv.org/html/2410.04484v1?utm_source=chatgpt.com" title="Fine-Grained Prediction of Reading Comprehension from Eye ...">arXiv</a>)</li></ul><hr><h1 id=how-to-get-started>How to get started<a hidden class=anchor aria-hidden=true href=#how-to-get-started>#</a></h1><ol><li><strong>Pick a dataset</strong> that matches your question (MECO/OneStop/Provo/etc.). (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>Mind the structure</strong> (reader/text effects) and choose proper splits/stats.</li><li><strong>Use a pipeline</strong> (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Decide your integration</strong>: (a) <strong>features/embeddings</strong>, (b) <strong>auxiliary losses</strong> (multitask), or (c) <strong>synthetic gaze</strong> + LM fineâ€‘tuning. (<a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>)</li><li><strong>Evaluate cognitively</strong>: add behavioral metrics (e.g., ARA with eyeâ€‘tracking) alongside standard accuracy. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li></ol><hr><h1 id=references--links->References & links ğŸ”—<a hidden class=anchor aria-hidden=true href=#references--links->#</a></h1><ul><li><strong>Tutorial slides:</strong> <em>Eye Tracking and NLP</em> (ACL 2025) â€” many figures and examples here are adapted from the tutorial.</li><li><strong>Foundations:</strong> Raynerâ€™s classic review on eye movements & cognition; eyeâ€‘mind assumption background. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Perceptual span & physiology:</strong> asymmetries and fovea/cone density. (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>, <a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>)</li><li><strong>Datasets/initiatives:</strong> MECO, MultiplEYE, OneStop, Provo; toolkit <strong>pymovements</strong>. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE â€“ Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Gaze for modeling:</strong> NER with gaze; synthetic scanpaths + GLUE; multitask QA. (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)</li><li><strong>Behavioral eval:</strong> MT (eyeâ€‘tracking), summarization with eyeâ€‘gaze, readability via eyeâ€‘tracking. (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>, <a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li><li><strong>Psycholinguistic links:</strong> Smith &â€¯Levy; Demberg &â€¯Keller; Shain etâ€¯al.; Wilcox etâ€¯al.; Ryu &â€¯Lewis; Smith &â€¯Vasishth. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>, <a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource â€“ Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li><li><strong>Alignment & recency bias:</strong> Oh &â€¯Schuler (TACL 2023); Kuribayashi etâ€¯al. (2025); Haller etâ€¯al. (2024); Bolliger etâ€¯al. (2024); deâ€¯Varda &â€¯Marelli (2024); Clark etâ€¯al. (COLING 2025). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li><li><strong>Scanpath modeling:</strong> Eyettention; ScanDLâ€¯2.0; SPâ€‘EyeGAN; SEAM; OB1â€‘Reader. (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li></ul><hr></details><details id=talk-synthetic-data><summary>Synthetic data for NLP</summary><h1 id=at-a-glance>At a glance<a hidden class=anchor aria-hidden=true href=#at-a-glance>#</a></h1><ul><li>We canâ€™t label everything: synthetic data fills gaps when scraping/manual labels/privacy limits hit.</li><li>Good synthetic data is <strong>task-tailored</strong>, sized right, and <strong>clean</strong> â€” but beware distribution shift.</li><li>Evaluate two ways: <strong>extrinsic</strong> (downstream task) vs <strong>intrinsic</strong> (what the data itself looks like).</li><li>Diversity sometimes beats raw correctness; noisy but varied sets can still improve models.</li><li>Best results come from <strong>Human â†”ï¸ AI</strong> collaboration + strong filtering before use.</li></ul><hr><h1 id=where-do-we-get-data>Where do we get data?<a hidden class=anchor aria-hidden=true href=#where-do-we-get-data>#</a></h1><ul><li><strong>Scraping</strong> the web (scale, but licensing/noise).</li><li><strong>Manual labeling</strong> (accurate, expensive).</li><li><strong>Product/system data</strong> (useful but <strong>privacy</strong>-sensitive).</li><li><strong>Creative curation</strong> (high quality, limited volume).<br>Synthetic data tries to extend/augment all of the above.</li></ul><hr><h1 id=what-makes-good-synthetic-data>What makes â€œgoodâ€ synthetic data?<a hidden class=anchor aria-hidden=true href=#what-makes-good-synthetic-data>#</a></h1><ul><li><strong>Flexible & task-specific:</strong> format, difficulty, and style match your target task.</li><li><strong>Appropriate size:</strong> enough to move the needle, not so much it drowns real data.</li><li><strong>Clean:</strong> minimal contradictions/formatting errors.</li><li><strong>Aligned distributions:</strong> cover the same <em>kinds</em> of inputs/labels you expect in production.</li></ul><blockquote><p>Why the warning? Because the real joint distribution often differs from the synthetic one:<br>( P_{\text{true}}(x,y) \neq P_{\text{synth}}(x,y) ).<br>Mismatch shows up as off-manifold inputs, <strong>wrong labels</strong>, or flawed reasoning traces.</p></blockquote><hr><h1 id=how-do-we-evaluate-synthetic-data>How do we evaluate synthetic data?<a hidden class=anchor aria-hidden=true href=#how-do-we-evaluate-synthetic-data>#</a></h1><p><strong>Extrinsic:</strong> train/evaluate models on tasks with/without the synthetic set.</p><ul><li>âœ… Directly answers â€œdoes it help?â€.</li><li>âŒ Costly, indirect diagnostic signal.</li></ul><p><strong>Intrinsic:</strong> inspect the data/generation process itself.</p><ul><li><strong>Correctness:</strong> e.g., spot-checking/self-instruct style manual audits.</li><li><strong>Diversity/coverage:</strong> does it span plausible inputs? <em>(e.g., DataTuneâ€™s bigram diversity as a quick signal).</em></li><li><strong>Privacy, fairness, distributional similarity:</strong> toolkits like <em>SynthTextEval</em> help stress-test.</li><li><strong>Model choice as a proxy:</strong> pick the generator by how well its synthetic data matches human-written (e.g., <em>AgoraBench</em>).</li></ul><hr><h1 id=how-is-synthetic-data-created>How is synthetic data created?<a hidden class=anchor aria-hidden=true href=#how-is-synthetic-data-created>#</a></h1><ul><li><strong>Knowledge distillation (Hinton â€™15 â†’ sentence-level):</strong> train a student to mimic a teacher; cheap SFT, keeps style on-target.</li><li><strong>In-context generation:</strong> prompt LMs to produce new labeled examples for arbitrary tasks (Schick & SchÃ¼tze â€™21).</li><li><strong>Self-training/bootstrapping:</strong> fine-tune on the modelâ€™s own outputs (Wang et al. â€™22).</li><li><strong>Observed effects:</strong> generated text is often <strong>more diverse</strong> (e.g., BERTScore) but only ~Â½ examples fully correctâ€”<strong>and models can still improve</strong> thanks to coverage.</li></ul><p><strong>Common patterns</strong></p><ul><li><strong>Sampling-based generation:</strong> temperature/top-p, curriculum over difficulty.</li><li><strong>Instruction back-translation:</strong> given an answer (y), generate an instruction (x) that (y) would satisfy.</li><li><strong>Transform existing data:</strong> retrieve/convert to the target format (QA from StackExchange; ground to KGs; rephrase documents for pretraining, Maini â€™24).</li><li><strong>Humanâ€“AI collaboration:</strong> LLM drafts, humans edit/verify (creativity â†”ï¸ correctness).</li><li><strong>Symbolic/programmable data:</strong> pretrain on formal languages/grammars to improve generalization (Hu â€™24).</li></ul><hr><p>#Filter before you use it</p><ul><li><strong>Diversity filters:</strong> keep sets that are far apart (ROUGE-L, embedding cosine, semantic tags).</li><li><strong>Gradient diversity:</strong> prefer examples that produce different loss gradients â†’ more robust models.</li><li><strong>Quality filters:</strong> pick highest-reward responses (e.g., RAFT-style ranking).</li><li><strong>Correctness filters:</strong> keep chains of thought that reach the right answer; drop inconsistent traces.</li></ul><hr><h1 id=where-synthetic-data-fits-in-the-pipeline>Where synthetic data fits in the pipeline<a hidden class=anchor aria-hidden=true href=#where-synthetic-data-fits-in-the-pipeline>#</a></h1><p><strong>Pretraining</strong></p><ul><li>When real data plateaus: rephrase corpora, verbalize knowledge bases, or mix in <strong>formal languages</strong> for structure.</li><li>Domain adaptation to reduce hallucinations on niche topics.</li></ul><p><strong>Supervised fine-tuning (SFT)</strong></p><ul><li><strong>Distillation</strong> is cheap but can be â€œstyle-lockedâ€.</li><li><strong>Self-Instruct / Evol-Instruct</strong> to grow instruction variance.</li><li><strong>MAmmoTH-style</strong> transform/extract tasks (e.g., convert docs to QA).</li></ul><p><strong>RL & feedback</strong></p><ul><li>Minimal supervision, <strong>negative examples</strong>, adaptation to own token distribution.</li><li>Synthetic rewards (e.g., <strong>Prometheus</strong> evaluators; checklist-style rubrics).</li><li>Choosing a â€œjudgeâ€:<ol><li>agreement with <strong>human</strong> prefs (RewardBench),</li><li>agreement with <strong>benchmarks</strong> (re-ranking),</li><li>effectiveness <strong>inside RL</strong> loops.</li></ol></li><li>â€œTeacherâ€ qualities: good accuracy <strong>and</strong> low variance; even non-RL textual feedback can helpâ€”mainly when the base model is already strong.</li></ul><p><strong>Reasoning</strong></p><ul><li>Scale up inference to get longer CoT/PoT traces; pipelines like <strong>OpenThoughts</strong> curate reasoning data.</li><li>Even noisy reasoning data can help; more (curated) data â†’ better reasoning.</li></ul><p><strong>Code</strong></p><ul><li>CodeAlpaca, WaveCoder, WizardCoder, Magicoder, etc.</li><li>Train with <strong>execution feedback</strong> (tests, runtimes).</li><li>Useful data types: single-turn, simulated multi-turn, â€œfix-the-bugâ€, and near-duplicate (LeetCode-style) variants.</li></ul><p><strong>Tools & agents</strong></p><ul><li>Gorilla (API-calling), ToolLLM, ToRa (tool-integrated math), AgentTuning, CodeActInstruct, AgentE, â€œGPT-4-toolsâ€ style setups.</li></ul><p><strong>Multimodal / Multilingual</strong></p><ul><li>LLaVA-style visual instruction tuning; multilingual instruction pipelines.</li></ul><hr><h1 id=limitations--open-questions>Limitations & open questions<a hidden class=anchor aria-hidden=true href=#limitations--open-questions>#</a></h1><ul><li>Synthetic sets still trail real data in <strong>size, diversity, and distribution</strong>; production usage (e.g., Anthropicâ€™s Clio) shows gaps.</li><li>In controlled tests, synthetic often <strong>underperforms</strong>; artifacts creep in.</li><li><strong>Synthetic eval data</strong> can <strong>overestimate</strong> model performance.</li><li><strong>Model collapse</strong> risk under recursive self-training (Shumailov â€™23).</li><li>We should measure <strong>instance-level quality</strong>, not just dataset averages.</li><li>Governance: â€œdistillation-friendlyâ€ models, usage restrictions, provenance tracking.</li></ul><hr><h1 id=practical-checklist-what-id-do>Practical checklist (what Iâ€™d do)<a hidden class=anchor aria-hidden=true href=#practical-checklist-what-id-do>#</a></h1><ol><li><strong>Define the target</strong>: task format, difficulty mix, and metrics.</li><li><strong>Generate broad, then filter hard</strong>: diversity â†’ quality â†’ correctness.</li><li><strong>Mix with real data</strong>: keep an anchor set for sanity checks.</li><li><strong>Evaluate both ways</strong>: quick intrinsic dashboards + periodic extrinsic runs.</li><li><strong>Close the loop</strong>: human spot-edits, error mining, and focused regeneration.</li><li><strong>Watch drift</strong>: compare (P_{\text{true}}(x,y)) vs (P_{\text{synth}}(x,y)) over time.</li></ol></details></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ldomenichelli.github.io/posts/post5/><span class=title>Â« Prev</span><br><span>â„ï¸ HPLT Ã— NLPL Winter School</span>
</a><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next Â»</span><br><span>Embeddings space ğ–¦¹×‚ â‚ŠËšâŠ¹â‹†</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>â€¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>â€¢
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>