<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ACL Vienna 2025 | lucia's notes</title>
<meta name=keywords content="NLP,ACL,computational linguistics"><meta name=description content="How ACL went..."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post3/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="ACL Vienna 2025"><meta property="og:description" content="How ACL went..."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post3/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="ACL Vienna 2025"><meta name=twitter:description content="How ACL went..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ACL Vienna 2025","item":"https://ldomenichelli.github.io/posts/post3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ACL Vienna 2025","name":"ACL Vienna 2025","description":"How ACL went...","keywords":["NLP","ACL","computational linguistics"],"articleBody":"Eye-tracking tutorial I attended the â€œEye Tracking and NLPâ€ tutorial in Vienna (Julâ€¯27,â€¯2025). If you work on cognitivelyâ€‘informed models, evaluation beyond static labels, or humanâ€‘centered NLP, this oneâ€™s for you!!\nWhy gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (PubMed) Data is maturing: Multilingual, multiâ€‘lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made highâ€‘quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv) Models \u0026 evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned. Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology) Eyeâ€‘tracking 101 (super fast) ğŸ§ ğŸ‘ï¸ Fixations \u0026 saccades. Reading is a hopâ€‘andâ€‘pause routine: brief saccades (tens of ms) between ~200â€“250â€¯ms fixations; perception occurs mostly during fixations, not saccades. The classic eyeâ€‘mind assumption: minimal lag between whatâ€™s fixated and whatâ€™s processed. (andrewd.ces.clemson.edu, PubMed)\nPerceptual span. Highâ€‘acuity foveal vision is coneâ€‘rich, while parafoveal vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (NCBI, PubMed Central, Frontiers)\nReading measures youâ€™ll see in papers: skip rate, firstâ€‘fixation duration, gaze duration, regression rate, goâ€‘past duration, total fixation time. These map fixations to Areasâ€‘ofâ€‘Interest (AoIs) at token/region level. Hardware \u0026 sampling. For reading studies, stationary trackers with head stabilization and â‰¥200â€¯Hz sampling are typical to get characterâ€‘level precision and reliable on/offsets. Pipelines \u0026 data structure. Raw samples â†’ fixation detection â†’ map to AoIs â†’ compute measures per readerÃ—word. Remember: data is not i.i.d. (nested readers/texts), which affects stats and ML splits. Lowâ€‘tech alternatives. When eyeâ€‘tracking isnâ€™t feasible: Selfâ€‘Paced Reading (SPR), Maze, mouseâ€‘tracking can capture useful online signalsâ€”with different tradeâ€‘offs. (PubMed Central, SpringerLink, SpringerLink)\nDatasets \u0026 tools Iâ€™m bookmarking ğŸ“šğŸ§° MECO (Multilingual Eyeâ€‘movement Corpus): large, coordinated, crossâ€‘linguistic reading data; Waveâ€¯2 keeps expanding. (meco-read.com, PubMed Central) MultiplEYE (COST Action): enabling multilingual eyeâ€‘trackingâ€‘whileâ€‘reading at scale; infrastructure, protocols, and community. (multipleye.eu, Radboud Universiteit) OneStop Eye Movements: 360 native readers, 2.6M tokens; great for comprehensionâ€‘linked analyses. (lacclab.github.io) Provo, ZuCo, Dundee, CELERâ€¦ Useful complements for different tasks and populations. (PubMed) pymovements: openâ€‘source package to download datasets and preprocess gaze (event detection, angles/velocities, etc.). (arXiv, pymovements.readthedocs.io) Using gaze in NLP models ğŸ”§ğŸ¤ Wordâ€‘level alignment \u0026 embeddings. Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gazeâ€‘motivated positional encodings and human attention signals. Synthetic scanpaths help scale. Since human gaze is scarce, Eyettentionâ€‘style scanpath generators and followâ€‘ups inject synthetic gaze to fineâ€‘tune LMs, improving GLUE tasks (especially lowâ€‘resource). (arXiv, ACL Anthology, ACL Anthology)\nTaskâ€‘specific multitask learning. Training to predict reading measures jointly with downstream tasks (e.g., QA with question preview vs. ordinary reading) can induce more humanâ€‘like attention. (ACL Anthology)\nWhat to expect. Reported gains are real but often modest without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial. Examples \u0026 pointers: NER with gaze, compression/paraphrase, readability, parsingâ€”plus general â€œgazeâ€‘augmented PLMs.â€ (ACL Anthology, ACL Anthology, arXiv)\nUsing gaze to evaluate NLP ğŸ“ğŸ§ª Behavioral evaluation uses online human signalsâ€”complementing labels or preferences. We saw applications to MT (reading effort), summarization (human vs. model saliency), and readability (readingâ€‘ease metrics). (SpringerLink, ACL Anthology)\nCase study â€” Automatic Readability Assessment (ARA). A new eyeâ€‘trackingâ€‘based benchmark correlates model scores with reading speed, skip rate, regressions, and total fixation time, revealing weak spots of classic readability formulas. Promising direction for cognitive evaluation. (hundred.org)\nPsycholinguistics â†”ï¸ NLP: whatâ€™s being tested? ğŸ§ªğŸ” Surprisal \u0026 RTs. Foundational results show a strong relation between LM surprisal and reading times; this holds across languages and for many modern LMsâ€”with nuances. (lexplore.com, lexplore.com) Classics to know: Surprisal theory, Dependency Locality Theory, Uniform Information Density, Cueâ€‘based retrieval/ACTâ€‘Râ€”usually operationalized via parsers/LMs. (eyetechds.com) Controlled tests. Agreement phenomena with GPTâ€‘2 surprisal; embeddings as cognitive features for memory retrieval. (Appsource â€“ Business Apps, eyetechds.com) Are LLMs aligned with human reading? ğŸ¤–ğŸ§â€â™€ï¸ Itâ€™s complicated (and active in 2023â€“2025):\nBigger isnâ€™t always better: Larger Transformers can fit worse to human RTs than smaller ones (surprisalâ€‘RT link weakens with size). (ACL Anthology) â€¦but layer matters: Intermediate layers may reverse that trend. (dfki.de) Individual differences: Surprisal better predicts firstâ€‘pass RTs for lower verbal IQ readers; entropy better fits those with higher working memory. (PubMed) Text \u0026 decoding matter: PP varies across generation strategies and reading measures; evaluating produced texts against human reading is informative. (ACL Anthology, ACL Anthology) Add cognitive bias: Injecting recency biases (e.g., ALiBi) improves LM fit to reading times. (ACL Anthology, ACL Anthology) Modeling eye movements themselves ğŸ› ï¸ Cognitive models (fewer, interpretable parameters): Eâ€‘Z Reader, SWIFT, SEAM, OB1â€‘Reader. ML/NLP models (dataâ€‘hungry, highâ€‘capacity): Eyettention, ScanDLâ€¯2.0, SPâ€‘EyeGAN. The recent trend is to combine strengths (e.g., selfâ€‘supervised frameworks grounded in cognitive constraints). (PubMed, ScienceDirect, arXiv, ACM Digital Library, Zora, ACM Digital Library)\nHumanâ€‘centered applications ğŸŒ Language assessment (L2): Eye movements carry proficiency signals (e.g., EyeScoreâ€‘style similarity to L1 prototypes). Reading impairment screening/monitoring: Commercial tools (e.g., Lexplore) and research platforms point to scalable screening and longitudinal tracking. (eyetechusa.com) Reading comprehension modeling: Predicting comprehension from gaze during QA is an emerging task on OneStop. (arXiv) How to get started (my checklist) âœ… Pick a dataset that matches your question (MECO/OneStop/Provo/etc.). (meco-read.com, lacclab.github.io, PubMed) Mind the structure (reader/text effects) and choose proper splits/stats. Use a pipeline (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (arXiv) Decide your integration: (a) features/embeddings, (b) auxiliary losses (multitask), or (c) synthetic gaze + LM fineâ€‘tuning. (ACL Anthology, ACL Anthology) Evaluate cognitively: add behavioral metrics (e.g., ARA with eyeâ€‘tracking) alongside standard accuracy. (hundred.org) References \u0026 links ğŸ”— Tutorial slides: Eye Tracking and NLP (ACL 2025) â€” many figures and examples here are adapted from the tutorial. Foundations: Raynerâ€™s classic review on eye movements \u0026 cognition; eyeâ€‘mind assumption background. (andrewd.ces.clemson.edu, PubMed) Perceptual span \u0026 physiology: asymmetries and fovea/cone density. (Frontiers, NCBI) Datasets/initiatives: MECO, MultiplEYE, OneStop, Provo; toolkit pymovements. (meco-read.com, multipleye.eu, lacclab.github.io, PubMed, arXiv) Gaze for modeling: NER with gaze; synthetic scanpaths + GLUE; multitask QA. (ACL Anthology, ACL Anthology, ACL Anthology, ACL Anthology) Behavioral eval: MT (eyeâ€‘tracking), summarization with eyeâ€‘gaze, readability via eyeâ€‘tracking. (SpringerLink, ACL Anthology, hundred.org) Psycholinguistic links: Smith \u0026â€¯Levy; Demberg \u0026â€¯Keller; Shain etâ€¯al.; Wilcox etâ€¯al.; Ryu \u0026â€¯Lewis; Smith \u0026â€¯Vasishth. (lexplore.com, eyetechds.com, lexplore.com, Appsource â€“ Business Apps, eyetechds.com) Alignment \u0026 recency bias: Oh \u0026â€¯Schuler (TACL 2023); Kuribayashi etâ€¯al. (2025); Haller etâ€¯al. (2024); Bolliger etâ€¯al. (2024); deâ€¯Varda \u0026â€¯Marelli (2024); Clark etâ€¯al. (COLING 2025). (ACL Anthology, dfki.de, PubMed, ACL Anthology, ACL Anthology, ACL Anthology) Scanpath modeling: Eyettention; ScanDLâ€¯2.0; SPâ€‘EyeGAN; SEAM; OB1â€‘Reader. (arXiv, Zora, ACM Digital Library, arXiv, PubMed) ","wordCount":"1134","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post3/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>ACL Vienna 2025</h1><div class=post-description>How ACL went...</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#eyetracking-101-super-fast->Eyeâ€‘tracking 101 (super fast) ğŸ§ ğŸ‘ï¸</a></li><li><a href=#datasets--tools-im-bookmarking->Datasets & tools Iâ€™m bookmarking ğŸ“šğŸ§°</a></li><li><a href=#using-gaze-in-nlp-models->Using gaze <strong>in</strong> NLP models ğŸ”§ğŸ¤</a></li><li><a href=#using-gaze-to-evaluate-nlp->Using gaze to <strong>evaluate</strong> NLP ğŸ“ğŸ§ª</a></li><li><a href=#psycholinguistics--nlp-whats-being-tested->Psycholinguistics â†”ï¸ NLP: whatâ€™s being tested? ğŸ§ªğŸ”</a></li><li><a href=#are-llms-aligned-with-human-reading->Are LLMs aligned with human reading? ğŸ¤–ğŸ§â€â™€ï¸</a></li><li><a href=#modeling-eye-movements-themselves->Modeling eye movements themselves ğŸ› ï¸</a></li><li><a href=#humancentered-applications->Humanâ€‘centered applications ğŸŒ</a></li><li><a href=#how-to-get-started-my-checklist->How to get started (my checklist) âœ…</a></li><li><a href=#references--links->References & links ğŸ”—</a></li></ul></nav></div></details></div><div class=post-content><h1 id=eye-tracking-tutorial>Eye-tracking tutorial<a hidden class=anchor aria-hidden=true href=#eye-tracking-tutorial>#</a></h1><blockquote><p>I attended the â€œEye Tracking and NLPâ€ tutorial in Vienna (Julâ€¯27,â€¯2025). If you work on cognitivelyâ€‘informed models, evaluation beyond static labels, or humanâ€‘centered NLP, this oneâ€™s for you!!</p></blockquote><ul><li><strong>Why gaze?</strong> Eye movements reflect <em>online</em> processing (not just end products), letting us probe difficulty, attention, and strategies during reading. Thatâ€™s gold for modeling and evaluation. (<a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Data is maturing:</strong> Multilingual, multiâ€‘lab efforts (e.g., <strong>MECO</strong>, <strong>MultiplEYE</strong>) + tooling (e.g., <strong>pymovements</strong>) have made highâ€‘quality datasets and pipelines more accessible. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE â€“ Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Models & evals:</strong> Gaze can <strong>improve</strong> certain NLP tasks and also <strong>evaluate</strong> systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is taskâ€‘aligned.</li><li><strong>Open debates:</strong> How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding <strong>recency biases</strong> can help fit human behavior. (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h2 id=eyetracking-101-super-fast->Eyeâ€‘tracking 101 (super fast) ğŸ§ ğŸ‘ï¸<a hidden class=anchor aria-hidden=true href=#eyetracking-101-super-fast->#</a></h2><p><strong>Fixations & saccades.</strong> Reading is a hopâ€‘andâ€‘pause routine: brief saccades (tens of ms) between ~200â€“250â€¯ms fixations; perception occurs mostly during fixations, not saccades. The classic <strong>eyeâ€‘mind assumption</strong>: minimal lag between whatâ€™s fixated and whatâ€™s processed. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</p><p><strong>Perceptual span.</strong> Highâ€‘acuity <strong>foveal</strong> vision is coneâ€‘rich, while <strong>parafoveal</strong> vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (<a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3075059/?utm_source=chatgpt.com" title="Eye movements, the perceptual span, and reading speed - PMC">PubMed Central</a>, <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>)</p><p><strong>Reading measures youâ€™ll see in papers:</strong>
<em>skip rate, firstâ€‘fixation duration, gaze duration, regression rate, goâ€‘past duration, total fixation time</em>. These map fixations to Areasâ€‘ofâ€‘Interest (AoIs) at token/region level.</p><p><strong>Hardware & sampling.</strong> For reading studies, stationary trackers with head stabilization and <strong>â‰¥200â€¯Hz</strong> sampling are typical to get characterâ€‘level precision and reliable on/offsets.</p><p><strong>Pipelines & data structure.</strong> Raw samples â†’ fixation detection â†’ map to AoIs â†’ compute measures per readerÃ—word. Remember: <strong>data is not i.i.d.</strong> (nested readers/texts), which affects stats and ML splits.</p><p><strong>Lowâ€‘tech alternatives.</strong> When eyeâ€‘tracking isnâ€™t feasible: <strong>Selfâ€‘Paced Reading (SPR)</strong>, <strong>Maze</strong>, <strong>mouseâ€‘tracking</strong> can capture useful online signalsâ€”with different tradeâ€‘offs. (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12058810/?utm_source=chatgpt.com" title="A multiverse analysis of cleaning and analyzing procedures of eye ...">PubMed Central</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02536-8?utm_source=chatgpt.com" title="PoTeC: A German naturalistic eye-tracking-while-reading corpus">SpringerLink</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02517-x?utm_source=chatgpt.com" title="A Persian sentence reading corpus of eye movements">SpringerLink</a>)</p><hr><h2 id=datasets--tools-im-bookmarking->Datasets & tools Iâ€™m bookmarking ğŸ“šğŸ§°<a hidden class=anchor aria-hidden=true href=#datasets--tools-im-bookmarking->#</a></h2><ul><li><strong>MECO</strong> (Multilingual Eyeâ€‘movement Corpus): large, coordinated, crossâ€‘linguistic reading data; Waveâ€¯2 keeps expanding. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12246088/?utm_source=chatgpt.com" title="Wave 2 of the Multilingual Eye-Movement Corpus (MECO): New text ...">PubMed Central</a>)</li><li><strong>MultiplEYE (COST Action)</strong>: enabling multilingual eyeâ€‘trackingâ€‘whileâ€‘reading at scale; infrastructure, protocols, and community. (<a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE â€“ Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://www.ru.nl/en/research/research-projects/multipleye?utm_source=chatgpt.com" title="MultiplEYE | Radboud University">Radboud Universiteit</a>)</li><li><strong>OneStop Eye Movements</strong>: 360 native readers, 2.6M tokens; great for comprehensionâ€‘linked analyses. (<a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>)</li><li><strong>Provo, ZuCo, Dundee, CELERâ€¦</strong> Useful complements for different tasks and populations. (<a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>pymovements</strong>: openâ€‘source package to <strong>download</strong> datasets and <strong>preprocess</strong> gaze (event detection, angles/velocities, etc.). (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>, <a href="https://pymovements.readthedocs.io/en/v0.17.0/tutorials/preprocessing-raw-data.html?utm_source=chatgpt.com" title="Preprocessing Raw Gaze Data - the pymovements documentation!">pymovements.readthedocs.io</a>)</li></ul><hr><h2 id=using-gaze-in-nlp-models->Using gaze <strong>in</strong> NLP models ğŸ”§ğŸ¤<a hidden class=anchor aria-hidden=true href=#using-gaze-in-nlp-models->#</a></h2><p><strong>Wordâ€‘level alignment & embeddings.</strong> Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gazeâ€‘motivated positional encodings and human attention signals.</p><p><strong>Synthetic scanpaths help scale.</strong> Since human gaze is scarce, <strong>Eyettention</strong>â€‘style scanpath generators and followâ€‘ups inject <em>synthetic</em> gaze to fineâ€‘tune LMs, improving <strong>GLUE</strong> tasks (especially lowâ€‘resource). (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>)</p><p><strong>Taskâ€‘specific multitask learning.</strong> Training to predict reading measures jointly with downstream tasks (e.g., <strong>QA</strong> with question preview vs. ordinary reading) can induce more humanâ€‘like attention. (<a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)</p><p><strong>What to expect.</strong> Reported gains are real but often <strong>modest</strong> without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial.</p><p><strong>Examples & pointers:</strong> NER with gaze, compression/paraphrase, readability, parsingâ€”plus general â€œgazeâ€‘augmented PLMs.â€ (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.conll-1.8.pdf?utm_source=chatgpt.com" title="[PDF] A Comparative Study on Textual Saliency of Styles from Eye ...">ACL Anthology</a>, <a href="https://arxiv.org/pdf/2310.14676?utm_source=chatgpt.com" title="[PDF] arXiv:2310.14676v1 [cs.CL] 23 Oct 2023">arXiv</a>)</p><hr><h2 id=using-gaze-to-evaluate-nlp->Using gaze to <strong>evaluate</strong> NLP ğŸ“ğŸ§ª<a hidden class=anchor aria-hidden=true href=#using-gaze-to-evaluate-nlp->#</a></h2><p><strong>Behavioral evaluation</strong> uses <em>online</em> human signalsâ€”complementing labels or preferences. We saw applications to <strong>MT</strong> (reading effort), <strong>summarization</strong> (human vs. model saliency), and <strong>readability</strong> (readingâ€‘ease metrics). (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>)</p><p><strong>Case study â€” Automatic Readability Assessment (ARA).</strong> A new eyeâ€‘trackingâ€‘based benchmark correlates model scores with <strong>reading speed</strong>, <strong>skip rate</strong>, <strong>regressions</strong>, and <strong>total fixation time</strong>, revealing weak spots of classic readability formulas. Promising direction for <strong>cognitive evaluation</strong>. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</p><hr><h2 id=psycholinguistics--nlp-whats-being-tested->Psycholinguistics â†”ï¸ NLP: whatâ€™s being tested? ğŸ§ªğŸ”<a hidden class=anchor aria-hidden=true href=#psycholinguistics--nlp-whats-being-tested->#</a></h2><ul><li><strong>Surprisal & RTs.</strong> Foundational results show a strong relation between <strong>LM surprisal</strong> and <strong>reading times</strong>; this holds across languages and for many modern LMsâ€”with nuances. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>)</li><li><strong>Classics to know:</strong> <em>Surprisal theory</em>, <em>Dependency Locality Theory</em>, <em>Uniform Information Density</em>, <em>Cueâ€‘based retrieval/ACTâ€‘R</em>â€”usually operationalized via parsers/LMs. (<a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>)</li><li><strong>Controlled tests.</strong> Agreement phenomena with GPTâ€‘2 surprisal; embeddings as cognitive features for memory retrieval. (<a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource â€“ Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li></ul><hr><h2 id=are-llms-aligned-with-human-reading->Are LLMs aligned with human reading? ğŸ¤–ğŸ§â€â™€ï¸<a hidden class=anchor aria-hidden=true href=#are-llms-aligned-with-human-reading->#</a></h2><p>Itâ€™s <strong>complicated</strong> (and active in 2023â€“2025):</p><ul><li><strong>Bigger isnâ€™t always better:</strong> Larger Transformers can fit <em>worse</em> to human RTs than smaller ones (surprisalâ€‘RT link weakens with size). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>)</li><li><strong>â€¦but layer matters:</strong> <strong>Intermediate layers</strong> may reverse that trend. (<a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>)</li><li><strong>Individual differences:</strong> <strong>Surprisal</strong> better predicts firstâ€‘pass RTs for <strong>lower verbal IQ</strong> readers; <strong>entropy</strong> better fits those with <strong>higher working memory</strong>. (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li><li><strong>Text & decoding matter:</strong> PP varies across <strong>generation strategies</strong> and <strong>reading measures</strong>; evaluating <em>produced</em> texts against human reading is informative. (<a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14/?utm_source=chatgpt.com" title="On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>)</li><li><strong>Add cognitive bias:</strong> Injecting <strong>recency biases</strong> (e.g., <strong>ALiBi</strong>) improves LM fit to reading times. (<a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517.pdf?utm_source=chatgpt.com" title="[PDF] Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h2 id=modeling-eye-movements-themselves->Modeling eye movements themselves ğŸ› ï¸<a hidden class=anchor aria-hidden=true href=#modeling-eye-movements-themselves->#</a></h2><p><strong>Cognitive models</strong> (fewer, interpretable parameters): <strong>Eâ€‘Z Reader</strong>, <strong>SWIFT</strong>, <strong>SEAM</strong>, <strong>OB1â€‘Reader</strong>. <strong>ML/NLP models</strong> (dataâ€‘hungry, highâ€‘capacity): <strong>Eyettention</strong>, <strong>ScanDLâ€¯2.0</strong>, <strong>SPâ€‘EyeGAN</strong>. The recent trend is to combine strengths (e.g., selfâ€‘supervised frameworks grounded in cognitive constraints). (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0749596X23000955?utm_source=chatgpt.com" title="SEAM: An integrated activation-coupled model of sentence ...">ScienceDirect</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://dl.acm.org/doi/10.1145/3591131?utm_source=chatgpt.com" title="An Attention-based Dual-Sequence Model for Predicting Human ...">ACM Digital Library</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>)</p><hr><h2 id=humancentered-applications->Humanâ€‘centered applications ğŸŒ<a hidden class=anchor aria-hidden=true href=#humancentered-applications->#</a></h2><ul><li><strong>Language assessment (L2):</strong> Eye movements carry proficiency signals (e.g., <strong>EyeScore</strong>â€‘style similarity to L1 prototypes).</li><li><strong>Reading impairment screening/monitoring:</strong> Commercial tools (e.g., <strong>Lexplore</strong>) and research platforms point to scalable screening and longitudinal tracking. (<a href="https://eyetechusa.com/?utm_source=chatgpt.com" title="eye model, episcleral venomanometer, scleral depressors">eyetechusa.com</a>)</li><li><strong>Reading comprehension modeling:</strong> Predicting comprehension from gaze during QA is an emerging task on <strong>OneStop</strong>. (<a href="https://arxiv.org/html/2410.04484v1?utm_source=chatgpt.com" title="Fine-Grained Prediction of Reading Comprehension from Eye ...">arXiv</a>)</li></ul><hr><h2 id=how-to-get-started-my-checklist->How to get started (my checklist) âœ…<a hidden class=anchor aria-hidden=true href=#how-to-get-started-my-checklist->#</a></h2><ol><li><strong>Pick a dataset</strong> that matches your question (MECO/OneStop/Provo/etc.). (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>Mind the structure</strong> (reader/text effects) and choose proper splits/stats.</li><li><strong>Use a pipeline</strong> (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Decide your integration</strong>: (a) <strong>features/embeddings</strong>, (b) <strong>auxiliary losses</strong> (multitask), or (c) <strong>synthetic gaze</strong> + LM fineâ€‘tuning. (<a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>)</li><li><strong>Evaluate cognitively</strong>: add behavioral metrics (e.g., ARA with eyeâ€‘tracking) alongside standard accuracy. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li></ol><hr><h2 id=references--links->References & links ğŸ”—<a hidden class=anchor aria-hidden=true href=#references--links->#</a></h2><ul><li><strong>Tutorial slides:</strong> <em>Eye Tracking and NLP</em> (ACL 2025) â€” many figures and examples here are adapted from the tutorial.</li><li><strong>Foundations:</strong> Raynerâ€™s classic review on eye movements & cognition; eyeâ€‘mind assumption background. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Perceptual span & physiology:</strong> asymmetries and fovea/cone density. (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>, <a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>)</li><li><strong>Datasets/initiatives:</strong> MECO, MultiplEYE, OneStop, Provo; toolkit <strong>pymovements</strong>. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE â€“ Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Gaze for modeling:</strong> NER with gaze; synthetic scanpaths + GLUE; multitask QA. (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)</li><li><strong>Behavioral eval:</strong> MT (eyeâ€‘tracking), summarization with eyeâ€‘gaze, readability via eyeâ€‘tracking. (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>, <a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li><li><strong>Psycholinguistic links:</strong> Smith &â€¯Levy; Demberg &â€¯Keller; Shain etâ€¯al.; Wilcox etâ€¯al.; Ryu &â€¯Lewis; Smith &â€¯Vasishth. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>, <a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource â€“ Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li><li><strong>Alignment & recency bias:</strong> Oh &â€¯Schuler (TACL 2023); Kuribayashi etâ€¯al. (2025); Haller etâ€¯al. (2024); Bolliger etâ€¯al. (2024); deâ€¯Varda &â€¯Marelli (2024); Clark etâ€¯al. (COLING 2025). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li><li><strong>Scanpath modeling:</strong> Eyettention; ScanDLâ€¯2.0; SPâ€‘EyeGAN; SEAM; OB1â€‘Reader. (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li></ul><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://ldomenichelli.github.io/tags/nlp/>NLP</a></li><li><a href=https://ldomenichelli.github.io/tags/acl/>ACL</a></li><li><a href=https://ldomenichelli.github.io/tags/computational-linguistics/>Computational Linguistics</a></li></ul><nav class=paginav><a class=prev href=https://ldomenichelli.github.io/posts/post5/><span class=title>Â« Prev</span><br><span>â„ï¸ HPLT Ã— NLPL Winter School</span>
</a><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next Â»</span><br><span>Embeddings space ğ–¦¹×‚ â‚ŠËšâŠ¹â‹†</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>â€¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>â€¢
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>