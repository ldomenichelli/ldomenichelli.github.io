<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Time Series | lucia's notes</title>
<meta name=keywords content><meta name=description content="Predictive Models for Time Series Analysis
Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.

1. Introduction to Time Series Analysis
A time series is typically defined as:
$$
T = { x_1, x_2, \dots, x_m }
$$
where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post9/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post9/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="Time Series"><meta property="og:description" content="Predictive Models for Time Series Analysis
Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.

1. Introduction to Time Series Analysis
A time series is typically defined as:
$$
T = { x_1, x_2, \dots, x_m }
$$
where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post9/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="Time Series"><meta name=twitter:description content="Predictive Models for Time Series Analysis
Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.

1. Introduction to Time Series Analysis
A time series is typically defined as:
$$
T = { x_1, x_2, \dots, x_m }
$$
where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Time Series","item":"https://ldomenichelli.github.io/posts/post9/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Time Series","name":"Time Series","description":"Predictive Models for Time Series Analysis Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.\n1. Introduction to Time Series Analysis A time series is typically defined as: $$ T = { x_1, x_2, \\dots, x_m } $$ where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.\n","keywords":[],"articleBody":"Predictive Models for Time Series Analysis Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.\n1. Introduction to Time Series Analysis A time series is typically defined as: $$ T = { x_1, x_2, \\dots, x_m } $$ where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.\nUnivariate Time Series: A single variable measured over time. For instance, daily temperature readings. Multivariate Time Series: Multiple variables (channels) measured simultaneously, e.g., temperature, pressure, and humidity recorded at the same timestamps. Applications Forecasting: Predict future values based on historical patterns (e.g., stock prices, energy consumption). Classification / Regression: Predict class labels (e.g., device failure vs. normal) or numeric values (e.g., how many products will be sold). Clustering: Group time series exhibiting similar behavior or patterns (e.g., grouping customers by purchasing trends). Anomaly Detection: Detect unusual events or patterns in time (e.g., sudden sensor spikes). Pattern Mining: Identify repeated motifs (recurring subsequences) or rare discords. 2. Time Series Analytics Tasks Classification\nExample: Identify whether an ECG signal indicates normal heart activity or arrhythmia. Often uses distance-based approaches (DTW), shapelets, or feature-based transformations. Regression\nExample: Predict the amount of rainfall based on historical climate data. Can be framed as forecasting (single-step ahead) or a standard regression if the target is derived from the same temporal data. Forecasting\nExample: Project sales for the next quarter. Usually requires modeling temporal dependence (ARIMA, exponential smoothing, deep learning, etc.). Clustering\nExample: Group power-consumption patterns from different households to find typical usage profiles. May use DTW-based distance or other specialized measures. Anomaly Detection\nExample: Spot sudden temperature spikes in a production line sensor. Often involves statistical thresholding, machine learning models, or reconstruction-based methods (e.g., autoencoders). Pattern Mining\nExample: Detect repeating patterns (motifs) or unusual subsequences (discords) in ECG signals. 3. Time Series Visualization Common goals when plotting a time series:\nTrend: Does the data consistently move up, down, or show a long-term drift? Periodicity: Are there regular cycles, such as daily or monthly fluctuations? Seasonality: A special case of periodicity, often tied to known phenomena (e.g., yearly temperature cycles). Heteroskedasticity: Variance that changes over time (e.g., volatility clusters in financial data). Outliers: Points or subsequences that deviate significantly from the majority. Visual techniques might include standard line charts, rolling averages, or advanced dashboards that allow zooming and panning.\n4. Handling Missing Values Time series often have missing data due to sensor outages, data corruption, or irregular sampling.\nFilling with a constant value:\nForward fill (pad): Use the last known observation until a new one appears. Backward fill: Use the next known observation for previous gaps. Mean/median: Replace missing with overall mean or median. Nearest known value: Often used when sampling frequency is high. Linear Interpolation\nConnect neighboring known values with a straight line and fill in the gap. Forecasting-based Interpolation\nTrain a model on the known data and predict the missing points. Helps when the data exhibits predictable trends or seasonality. Random Imputation\nImpute from the distribution of known data. Sometimes used for simulation or bootstrapping. Careful selection of an imputation strategy is crucial—incorrect handling can introduce bias or distort subsequent analyses.\n5. Time Series Anomalies (Outliers) Outliers are observations that deviate substantially from the general behavior of the data. In time series, outliers might be due to measurement errors, system malfunctions, or truly significant (and possibly critical) events.\nTypes of Outliers Point Outlier: A single data point that is unusually large or small. Subsequence Outlier: A contiguous segment showing atypical behavior. Instance Outlier: An entire time series that differs markedly from others in a dataset. Common Outlier Detection Methods Histogram/Boxplot\nQuick visual approach; points beyond typical whiskers or outside certain standard deviations. IQR Filter\nLower bound:\n$$ Q1 - 1.5 \\times \\mathrm{IQR} $$ Upper bound:\n$$ Q3 + 1.5 \\times \\mathrm{IQR} $$ Data beyond these bounds may be considered outliers. Hampel Filter\nUses median and Median Absolute Deviation (MAD): $$ I = [\\text{median} - 3 \\times \\text{MAD},; \\text{median} + 3 \\times \\text{MAD}] $$ Grubbs’ Test\nIteratively detects one outlier at a time, assuming normality of data. Sometimes, domain knowledge is key to deciding whether to remove outliers or treat them as important signals.\n6. Normalizations Due to varying scales, offsets, or trends in time series data, normalization or transformation steps can be essential:\nOffset Translation\nMean Removal: Subtract the global mean from all values. Min–Max Normalization: Scale data to a ([0,1]) or ([-1,1]) range. Amplitude Scaling\nZ-Score Normalization: ( (x - \\mu) / \\sigma ). Helps unify variance. Linear Trend Removal\nDetrending: Fit and subtract a linear or polynomial trend. Mean Smoothing\nMoving Average: Helps reduce short-term volatility and highlight trends. Log Transformations\nUseful for data with exponential growth or multiplicative seasonality. Differencing\nReplace each value ( x_t ) with ( x_t - x_{t-1} ). Stabilizes mean if a strong linear trend is present. 7. Time Series Components A time series can often be decomposed into:\nLevel (baseline or average) Trend (long-term increase or decrease) Seasonality (repetitive, cyclical patterns) Noise (random, unexplained fluctuations) Additive Model $$ Y_t = \\text{Level} + \\text{Trend} + \\text{Seasonality} + \\text{Noise} $$\nMultiplicative Model $$ Y_t = \\text{Level} \\times \\text{Trend} \\times \\text{Seasonality} \\times \\text{Noise} $$\nFor instance, monthly sales data might have a rising trend, a strong seasonal pattern (e.g., holiday peaks), and some residual noise.\n8. Stationarity A time series is stationary if its statistical properties (mean, variance, autocovariance) remain constant over time. Many classic time series models (e.g., ARIMA) assume stationarity.\nCriteria:\nConstant Mean Constant Variance Autocovariance depends only on lag (i.e.,\n$$ \\mathrm{Cov}(x_t, x_{t+h}) \\text{ is independent of } t $$ ). Making a Series Stationary Detrending (subtracting a fitted linear or polynomial trend) Log Transform (reduces multiplicative effects) Differencing (subtract consecutive observations to remove trends) Seasonal Decomposition (removing seasonal components) Stationarity Test:\nAugmented Dickey–Fuller (ADF): If p-value is below a chosen threshold (e.g., 0.05), likely the series is stationary. 9. Time Series Similarities (Distances) Shape-based Similarity Euclidean Distance: Simple, but sensitive to misalignments or variable speeds. Dynamic Time Warping (DTW): Allows stretching/compressing in time, aligning sequences that are similar but out of phase. Sakoe–Chiba Band or Itakura Parallelogram can constrain warping paths, reducing computation. Structural-based Similarity Focuses on comparing broader patterns (e.g., overall shape, location of peaks/valleys). 10. Time Series Approximations \u0026 Dimensionality Reduction For very long or high-frequency time series, approximation methods reduce storage/computational demands:\nPAA (Piecewise Aggregate Approximation)\nDivide series into fixed-size segments; each segment is represented by its mean.\nSAX (Symbolic Aggregate Approximation)\nPAA + discretization into symbols from a finite alphabet.\nDFT (Discrete Fourier Transform)\nDecompose series into sums of sinusoidal components.\nSFA (Symbolic Fourier Approximation)\nDFT + discretization.\nSVD / PCA\nDimensionality reduction capturing principal variations. Often used across a collection of time series (e.g., multiple sensors). 11. Classification \u0026 Regression Instance-based (Memory-based) k-NN uses a distance measure (e.g., DTW). May store training data in memory and compare new time series to nearest neighbors. Linear / Logistic Models Often require stationarity or specific feature engineering to handle temporal correlations. Tree-based Approaches Decision Trees, Random Forests, Gradient Boosted Trees, etc. Work well with tabular features extracted from time windows (though must consider autocorrelation). Ensemble Methods Bagging, Boosting Proximity Forest, Time Series Forest (specialized ensemble methods). Interval-based Methods Time Series Forest Random Interval Spectral Ensemble (RISE) Supervised Time Series Forest\nThese extract features (mean, variance, slope, etc.) from various intervals of a time series. 12. Dictionary-based and Shapelet-based Models Dictionary-based Approaches Convert time series into “documents” of discrete symbols and then analyze them with bag-of-words or similar text mining techniques:\nBag of Patterns (BOP) Bag of SFA Symbols (BOSS) WEASEL (Word ExtrAction for time SEries cLassification) Shapelet-based Models A shapelet is a small subsequence that is highly representative or discriminative of a specific class.\nExtraction: Identify the most discriminative subsequences. Transformation: Convert each full time series to a vector of distances to shapelets. Classification: Train any standard classifier (e.g., SVM, Random Forest) on shapelet-distance features. 13. Multivariate Time Series Multiple channels measured simultaneously:\nIndependent Assumption: Model each channel separately if they don’t interact strongly. Concatenation: Flatten all channels into one univariate series (loses some cross-channel info). Advanced Methods: MUSE (extension of WEASEL) integrates multiple channels. Neural networks (e.g., LSTM) that handle multiple input features. 14. Deep Learning Methods CNNs\nExploit convolution + pooling layers to automatically learn local features from raw data.\nRNNs / LSTMs\nModel long-term dependencies, capturing temporal context across many time steps.\nInception Networks\nUse multi-scale filters in parallel (adapted from computer vision).\n![[Pasted image 20250409113111.png]]\nTapNet, Multivariate LSTM-FCN, etc.\nMerge RNNs/CNNs with fully-connected layers for robust feature extraction.\nKernel-based Models ROCKET (RandOm Convolutional KErnel Transform) MiniRocket, MultiRocket Hydra, MultiROCKET-Hydra They transform time series via numerous random convolutional kernels, then feed into a linear model. Offers strong performance with high efficiency.\n15. Hybrid Models HIVE-COTE (Hierarchical Vote Collective of Transformation-based Ensembles)\nCombines diverse transformation modules (e.g., shapelets, dictionary methods, intervals). TS-CHIEF (Time Series Combination of Heterogeneous and Integrated Embeddings Forest)\nRandomized decision trees using multiple embedded approaches. These ensembles often achieve state-of-the-art classification accuracy on benchmark datasets by combining complementary representations.\n16. Explainable AI With complex models (deep networks, large ensembles), interpretability can be challenging:\nFeature Attribution: Methods like Grad-CAM, integrated gradients, or saliency maps adapted to time series. Shapelet-based: Provides subsequences that are inherently interpretable. Surrogate Models: Train a simpler, interpretable model (e.g., decision tree) to mimic the predictions of a black-box. Rule Extraction: Derive approximate rules or patterns from complex models. 17. Time Series Forecasting Forecasting means predicting future observations from historical data. Errors are typically measured by:\nMAE (Mean Absolute Error) RMSE (Root Mean Squared Error) MAPE (Mean Absolute Percentage Error) Simple Methods Average Method: Forecast is the mean of all past data. Naïve Method: Forecast is just the last observed value. Drift Method: Linear extrapolation from the first to the last observed point. These serve as baselines. More sophisticated models can outperform them, but these are often used as references.\n18. Exponential Smoothing Family SES (Simple Exponential Smoothing)\nSuitable for data with no trend or seasonality. Recent observations get higher weights.\nHolt’s Method\nExtends SES with a trend component.\nHolt–Winters Method\nIncludes seasonality (additive or multiplicative).\n19. ARIMA-based Models ARIMA (AutoRegressive Integrated Moving Average) captures autocorrelation in time series.\nAR(p) Model $$ y_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + e_t $$\nMA(q) Model $$ y_t = c + e_t + \\theta_1 e_{t-1} + \\dots + \\theta_q e_{t-q} $$\nARIMA(p,d,q) Incorporates differencing of order (d) to handle non-stationarity.\nSARIMA Incorporates seasonal terms:\nAutoARIMA can automatically select (p, d, q) (and seasonal orders). Prophet (by Facebook/Meta) is another robust tool that handles multiple seasonalities, holidays, regressors, etc. 20. Forecasting via Reduced Regression This “reduction” approach converts forecasting into a supervised learning problem:\nChoose a window size ( w ). E.g., use the last ( w ) observations as features. Predict the next value (or multiple future values). Use standard regression methods: Random forests, linear regression, XGBoost, etc. For multi-step forecasting, one can repeat this approach or predict multiple future time steps at once.\n21. Forecasting via Deep Learning RNN/LSTM\nCapture long-term sequential dependencies. Variants (GRU, Bi-LSTM) may improve performance. Sequence-to-Sequence (Seq2Seq) Networks\nOriginally used for machine translation, can handle multi-step forecasting. Temporal Fusion Transformers\nCombine attention mechanisms with recurrent networks to handle complex time series with covariates. Neural networks excel with large datasets and can learn nonlinear patterns that simpler models might miss.\nAdditional Considerations Hyperparameter Tuning: Time series models often have multiple hyperparameters (e.g., ARIMA orders, number of hidden units in an LSTM). Automated searches like grid search or Bayesian optimization can help. Model Validation: Standard cross-validation splits might not apply directly because of the temporal order. Techniques like rolling forecasting origin or walk-forward validation preserve the time structure. Performance Metrics: Selecting metrics that align with business goals or practical considerations (e.g., if small absolute errors or relative errors matter more). Domain Knowledge: Often crucial in deciding how to handle missing data, outliers, or interpret model outputs. Summary Time series analysis and forecasting encompass a broad spectrum of methods, from simple baselines (Naïve, Average) and classic models (ARIMA, exponential smoothing) to advanced machine learning techniques (deep learning, shapelets, kernel-based ensembles). The choice of method depends on:\nData Characteristics: Stationarity, seasonality, presence of trends, magnitude of noise. Task Requirements: Single-step forecasting, anomaly detection, classification, etc. Computational Constraints: For large-scale or high-frequency data, efficient approximation or specialized algorithms may be needed. Explainability: Simpler models or shapelet-based approaches may provide clearer insights, while deep models might yield higher accuracy but be harder to interpret. Overall, success in time series analysis hinges on proper preprocessing (missing-value handling, outlier management, normalization, feature engineering) and a careful choice of models and validation strategies.\n","wordCount":"2149","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post9/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>Time Series</h1><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-introduction-to-time-series-analysis>1. Introduction to Time Series Analysis</a><ul><li><a href=#applications>Applications</a></li></ul></li><li><a href=#2-time-series-analytics-tasks>2. Time Series Analytics Tasks</a></li><li><a href=#3-time-series-visualization>3. Time Series Visualization</a></li><li><a href=#4-handling-missing-values>4. Handling Missing Values</a></li><li><a href=#5-time-series-anomalies-outliers>5. Time Series Anomalies (Outliers)</a><ul><li><a href=#types-of-outliers>Types of Outliers</a></li><li><a href=#common-outlier-detection-methods>Common Outlier Detection Methods</a></li></ul></li><li><a href=#6-normalizations>6. Normalizations</a></li><li><a href=#7-time-series-components>7. Time Series Components</a><ul><li><a href=#additive-model>Additive Model</a></li><li><a href=#multiplicative-model>Multiplicative Model</a></li></ul></li><li><a href=#8-stationarity>8. Stationarity</a><ul><li><a href=#making-a-series-stationary>Making a Series Stationary</a></li></ul></li><li><a href=#9-time-series-similarities-distances>9. Time Series Similarities (Distances)</a><ul><li><a href=#shape-based-similarity>Shape-based Similarity</a></li><li><a href=#structural-based-similarity>Structural-based Similarity</a></li></ul></li><li><a href=#10-time-series-approximations--dimensionality-reduction>10. Time Series Approximations & Dimensionality Reduction</a></li><li><a href=#11-classification--regression>11. Classification & Regression</a><ul><li><a href=#instance-based-memory-based>Instance-based (Memory-based)</a></li><li><a href=#linear--logistic-models>Linear / Logistic Models</a></li><li><a href=#tree-based-approaches>Tree-based Approaches</a></li><li><a href=#ensemble-methods>Ensemble Methods</a><ul><li><a href=#interval-based-methods>Interval-based Methods</a></li></ul></li></ul></li><li><a href=#12-dictionary-based-and-shapelet-based-models>12. Dictionary-based and Shapelet-based Models</a><ul><li><a href=#dictionary-based-approaches>Dictionary-based Approaches</a></li><li><a href=#shapelet-based-models>Shapelet-based Models</a></li></ul></li><li><a href=#13-multivariate-time-series>13. Multivariate Time Series</a></li><li><a href=#14-deep-learning-methods>14. Deep Learning Methods</a><ul><li><a href=#kernel-based-models>Kernel-based Models</a></li></ul></li><li><a href=#15-hybrid-models>15. Hybrid Models</a></li><li><a href=#16-explainable-ai>16. Explainable AI</a></li><li><a href=#17-time-series-forecasting>17. Time Series Forecasting</a><ul><li><a href=#simple-methods>Simple Methods</a></li></ul></li><li><a href=#18-exponential-smoothing-family>18. Exponential Smoothing Family</a></li><li><a href=#19-arima-based-models>19. ARIMA-based Models</a><ul><li><a href=#arp-model>AR(p) Model</a></li><li><a href=#maq-model>MA(q) Model</a></li><li><a href=#arimapdq>ARIMA(p,d,q)</a></li><li><a href=#sarima>SARIMA</a></li></ul></li><li><a href=#20-forecasting-via-reduced-regression>20. Forecasting via Reduced Regression</a></li><li><a href=#21-forecasting-via-deep-learning>21. Forecasting via Deep Learning</a></li><li><a href=#additional-considerations>Additional Considerations</a></li><li><a href=#summary>Summary</a></li></ul></nav></div></details></div><div class=post-content><h1 id=predictive-models-for-time-series-analysis>Predictive Models for Time Series Analysis<a hidden class=anchor aria-hidden=true href=#predictive-models-for-time-series-analysis>#</a></h1><p>Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.</p><hr><h2 id=1-introduction-to-time-series-analysis>1. Introduction to Time Series Analysis<a hidden class=anchor aria-hidden=true href=#1-introduction-to-time-series-analysis>#</a></h2><p>A <strong>time series</strong> is typically defined as:
$$
T = { x_1, x_2, \dots, x_m }
$$
where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.</p><ul><li><strong>Univariate Time Series</strong>: A single variable measured over time. For instance, daily temperature readings.</li><li><strong>Multivariate Time Series</strong>: Multiple variables (channels) measured simultaneously, e.g., temperature, pressure, and humidity recorded at the same timestamps.</li></ul><p><input type=checkbox id=zoomCheck-2e7a3 hidden>
<label for=zoomCheck-2e7a3><img class=zoomCheck loading=lazy decoding=async src=./img/1.png alt=1></label></p><h3 id=applications>Applications<a hidden class=anchor aria-hidden=true href=#applications>#</a></h3><ul><li><strong>Forecasting</strong>: Predict future values based on historical patterns (e.g., stock prices, energy consumption).</li><li><strong>Classification / Regression</strong>: Predict class labels (e.g., device failure vs. normal) or numeric values (e.g., how many products will be sold).</li><li><strong>Clustering</strong>: Group time series exhibiting similar behavior or patterns (e.g., grouping customers by purchasing trends).</li><li><strong>Anomaly Detection</strong>: Detect unusual events or patterns in time (e.g., sudden sensor spikes).</li><li><strong>Pattern Mining</strong>: Identify repeated motifs (recurring subsequences) or rare discords.</li></ul><hr><h2 id=2-time-series-analytics-tasks>2. Time Series Analytics Tasks<a hidden class=anchor aria-hidden=true href=#2-time-series-analytics-tasks>#</a></h2><ol><li><p><strong>Classification</strong></p><ul><li>Example: Identify whether an ECG signal indicates normal heart activity or arrhythmia.</li><li>Often uses distance-based approaches (DTW), shapelets, or feature-based transformations.</li></ul></li><li><p><strong>Regression</strong></p><ul><li>Example: Predict the amount of rainfall based on historical climate data.</li><li>Can be framed as forecasting (single-step ahead) or a standard regression if the target is derived from the same temporal data.</li></ul></li><li><p><strong>Forecasting</strong></p><ul><li>Example: Project sales for the next quarter.</li><li>Usually requires modeling temporal dependence (ARIMA, exponential smoothing, deep learning, etc.).</li></ul></li><li><p><strong>Clustering</strong></p><ul><li>Example: Group power-consumption patterns from different households to find typical usage profiles.</li><li>May use DTW-based distance or other specialized measures.</li></ul></li><li><p><strong>Anomaly Detection</strong></p><ul><li>Example: Spot sudden temperature spikes in a production line sensor.</li><li>Often involves statistical thresholding, machine learning models, or reconstruction-based methods (e.g., autoencoders).</li></ul></li><li><p><strong>Pattern Mining</strong></p><ul><li>Example: Detect repeating patterns (motifs) or unusual subsequences (discords) in ECG signals.</li></ul></li></ol><hr><h2 id=3-time-series-visualization>3. Time Series Visualization<a hidden class=anchor aria-hidden=true href=#3-time-series-visualization>#</a></h2><p>Common goals when plotting a time series:</p><ul><li><strong>Trend</strong>: Does the data consistently move up, down, or show a long-term drift?</li><li><strong>Periodicity</strong>: Are there regular cycles, such as daily or monthly fluctuations?</li><li><strong>Seasonality</strong>: A special case of periodicity, often tied to known phenomena (e.g., yearly temperature cycles).</li><li><strong>Heteroskedasticity</strong>: Variance that changes over time (e.g., volatility clusters in financial data).</li><li><strong>Outliers</strong>: Points or subsequences that deviate significantly from the majority.</li></ul><p>Visual techniques might include standard line charts, rolling averages, or advanced dashboards that allow zooming and panning.</p><hr><h2 id=4-handling-missing-values>4. Handling Missing Values<a hidden class=anchor aria-hidden=true href=#4-handling-missing-values>#</a></h2><p>Time series often have missing data due to sensor outages, data corruption, or irregular sampling.</p><ul><li><p><strong>Filling with a constant value</strong>:</p><ul><li><strong>Forward fill (pad)</strong>: Use the last known observation until a new one appears.</li><li><strong>Backward fill</strong>: Use the next known observation for previous gaps.</li><li><strong>Mean/median</strong>: Replace missing with overall mean or median.</li><li><strong>Nearest known value</strong>: Often used when sampling frequency is high.</li></ul></li><li><p><strong>Linear Interpolation</strong></p><ul><li>Connect neighboring known values with a straight line and fill in the gap.</li></ul></li><li><p><strong>Forecasting-based Interpolation</strong></p><ul><li>Train a model on the known data and predict the missing points. Helps when the data exhibits predictable trends or seasonality.</li></ul></li><li><p><strong>Random Imputation</strong></p><ul><li>Impute from the distribution of known data. Sometimes used for simulation or bootstrapping.</li></ul></li></ul><p>Careful selection of an imputation strategy is crucial—incorrect handling can introduce bias or distort subsequent analyses.</p><hr><h2 id=5-time-series-anomalies-outliers>5. Time Series Anomalies (Outliers)<a hidden class=anchor aria-hidden=true href=#5-time-series-anomalies-outliers>#</a></h2><p>Outliers are observations that deviate substantially from the general behavior of the data. In time series, outliers might be due to measurement errors, system malfunctions, or truly significant (and possibly critical) events.</p><h3 id=types-of-outliers>Types of Outliers<a hidden class=anchor aria-hidden=true href=#types-of-outliers>#</a></h3><ul><li><strong>Point Outlier</strong>: A single data point that is unusually large or small.</li><li><strong>Subsequence Outlier</strong>: A contiguous segment showing atypical behavior.</li><li><strong>Instance Outlier</strong>: An entire time series that differs markedly from others in a dataset.</li></ul><h3 id=common-outlier-detection-methods>Common Outlier Detection Methods<a hidden class=anchor aria-hidden=true href=#common-outlier-detection-methods>#</a></h3><ul><li><p><strong>Histogram/Boxplot</strong></p><ul><li>Quick visual approach; points beyond typical whiskers or outside certain standard deviations.</li></ul></li><li><p><strong>IQR Filter</strong></p><ul><li>Lower bound:<br>$$
Q1 - 1.5 \times \mathrm{IQR}
$$</li><li>Upper bound:<br>$$
Q3 + 1.5 \times \mathrm{IQR}
$$</li><li>Data beyond these bounds may be considered outliers.</li></ul></li><li><p><strong>Hampel Filter</strong></p><ul><li>Uses median and Median Absolute Deviation (MAD):
$$
I = [\text{median} - 3 \times \text{MAD},; \text{median} + 3 \times \text{MAD}]
$$</li></ul></li><li><p><strong>Grubbs’ Test</strong></p><ul><li>Iteratively detects one outlier at a time, assuming normality of data.</li></ul></li></ul><p>Sometimes, domain knowledge is key to deciding whether to remove outliers or treat them as important signals.</p><hr><h2 id=6-normalizations>6. Normalizations<a hidden class=anchor aria-hidden=true href=#6-normalizations>#</a></h2><p>Due to varying scales, offsets, or trends in time series data, normalization or transformation steps can be essential:</p><ul><li><p><strong>Offset Translation</strong></p><ul><li><strong>Mean Removal</strong>: Subtract the global mean from all values.</li><li><strong>Min–Max Normalization</strong>: Scale data to a ([0,1]) or ([-1,1]) range.</li></ul></li><li><p><strong>Amplitude Scaling</strong></p><ul><li><strong>Z-Score Normalization</strong>: ( (x - \mu) / \sigma ). Helps unify variance.</li></ul></li><li><p><strong>Linear Trend Removal</strong></p><ul><li><strong>Detrending</strong>: Fit and subtract a linear or polynomial trend.</li></ul></li><li><p><strong>Mean Smoothing</strong></p><ul><li><strong>Moving Average</strong>: Helps reduce short-term volatility and highlight trends.</li></ul></li><li><p><strong>Log Transformations</strong></p><ul><li>Useful for data with exponential growth or multiplicative seasonality.</li></ul></li><li><p><strong>Differencing</strong></p><ul><li>Replace each value ( x_t ) with ( x_t - x_{t-1} ). Stabilizes mean if a strong linear trend is present.</li></ul></li></ul><hr><h2 id=7-time-series-components>7. Time Series Components<a hidden class=anchor aria-hidden=true href=#7-time-series-components>#</a></h2><p>A time series can often be decomposed into:</p><ul><li><strong>Level</strong> (baseline or average)</li><li><strong>Trend</strong> (long-term increase or decrease)</li><li><strong>Seasonality</strong> (repetitive, cyclical patterns)</li><li><strong>Noise</strong> (random, unexplained fluctuations)</li></ul><h3 id=additive-model>Additive Model<a hidden class=anchor aria-hidden=true href=#additive-model>#</a></h3><p>$$
Y_t = \text{Level} + \text{Trend} + \text{Seasonality} + \text{Noise}
$$</p><h3 id=multiplicative-model>Multiplicative Model<a hidden class=anchor aria-hidden=true href=#multiplicative-model>#</a></h3><p>$$
Y_t = \text{Level} \times \text{Trend} \times \text{Seasonality} \times \text{Noise}
$$</p><p>For instance, monthly sales data might have a rising trend, a strong seasonal pattern (e.g., holiday peaks), and some residual noise.</p><hr><h2 id=8-stationarity>8. Stationarity<a hidden class=anchor aria-hidden=true href=#8-stationarity>#</a></h2><p>A time series is <strong>stationary</strong> if its statistical properties (mean, variance, autocovariance) remain constant over time. Many classic time series models (e.g., ARIMA) assume stationarity.</p><p><strong>Criteria</strong>:</p><ol><li><strong>Constant Mean</strong></li><li><strong>Constant Variance</strong></li><li><strong>Autocovariance</strong> depends only on lag (i.e.,<br>$$ \mathrm{Cov}(x_t, x_{t+h}) \text{ is independent of } t $$ ).</li></ol><h3 id=making-a-series-stationary>Making a Series Stationary<a hidden class=anchor aria-hidden=true href=#making-a-series-stationary>#</a></h3><ul><li><strong>Detrending</strong> (subtracting a fitted linear or polynomial trend)</li><li><strong>Log Transform</strong> (reduces multiplicative effects)</li><li><strong>Differencing</strong> (subtract consecutive observations to remove trends)</li><li><strong>Seasonal Decomposition</strong> (removing seasonal components)</li></ul><p><strong>Stationarity Test</strong>:</p><ul><li><strong>Augmented Dickey–Fuller (ADF)</strong>: If p-value is below a chosen threshold (e.g., 0.05), likely the series is stationary.</li></ul><hr><h2 id=9-time-series-similarities-distances>9. Time Series Similarities (Distances)<a hidden class=anchor aria-hidden=true href=#9-time-series-similarities-distances>#</a></h2><h3 id=shape-based-similarity>Shape-based Similarity<a hidden class=anchor aria-hidden=true href=#shape-based-similarity>#</a></h3><ul><li><strong>Euclidean Distance</strong>: Simple, but sensitive to misalignments or variable speeds.</li><li><strong>Dynamic Time Warping (DTW)</strong>: Allows stretching/compressing in time, aligning sequences that are similar but out of phase.<ul><li><strong>Sakoe–Chiba Band</strong> or <strong>Itakura Parallelogram</strong> can constrain warping paths, reducing computation.</li></ul></li></ul><h3 id=structural-based-similarity>Structural-based Similarity<a hidden class=anchor aria-hidden=true href=#structural-based-similarity>#</a></h3><ul><li>Focuses on comparing broader patterns (e.g., overall shape, location of peaks/valleys).</li></ul><hr><h2 id=10-time-series-approximations--dimensionality-reduction>10. Time Series Approximations & Dimensionality Reduction<a hidden class=anchor aria-hidden=true href=#10-time-series-approximations--dimensionality-reduction>#</a></h2><p>For very long or high-frequency time series, approximation methods reduce storage/computational demands:</p><ol><li><p><strong>PAA (Piecewise Aggregate Approximation)</strong></p><ul><li>Divide series into fixed-size segments; each segment is represented by its mean.<br><input type=checkbox id=zoomCheck-579e1 hidden>
<label for=zoomCheck-579e1><img class=zoomCheck loading=lazy decoding=async src=./img/2.png alt=2></label></li></ul></li><li><p><strong>SAX (Symbolic Aggregate Approximation)</strong></p><ul><li>PAA + discretization into symbols from a finite alphabet.<br><input type=checkbox id=zoomCheck-d6034 hidden>
<label for=zoomCheck-d6034><img class=zoomCheck loading=lazy decoding=async src=./img/3.png alt=3></label></li></ul></li><li><p><strong>DFT (Discrete Fourier Transform)</strong></p><ul><li>Decompose series into sums of sinusoidal components.<br><input type=checkbox id=zoomCheck-1c412 hidden>
<label for=zoomCheck-1c412><img class=zoomCheck loading=lazy decoding=async src=./img/4.png alt=4></label></li></ul></li><li><p><strong>SFA (Symbolic Fourier Approximation)</strong></p><ul><li>DFT + discretization.<br><input type=checkbox id=zoomCheck-b130c hidden>
<label for=zoomCheck-b130c><img class=zoomCheck loading=lazy decoding=async src=./img/5.png alt=5></label></li></ul></li><li><p><strong>SVD / PCA</strong></p><ul><li>Dimensionality reduction capturing principal variations.</li><li>Often used across a collection of time series (e.g., multiple sensors).</li></ul></li></ol><hr><h2 id=11-classification--regression>11. Classification & Regression<a hidden class=anchor aria-hidden=true href=#11-classification--regression>#</a></h2><h3 id=instance-based-memory-based>Instance-based (Memory-based)<a hidden class=anchor aria-hidden=true href=#instance-based-memory-based>#</a></h3><ul><li><strong>k-NN</strong> uses a distance measure (e.g., DTW).</li><li>May store training data in memory and compare new time series to nearest neighbors.</li></ul><h3 id=linear--logistic-models>Linear / Logistic Models<a hidden class=anchor aria-hidden=true href=#linear--logistic-models>#</a></h3><ul><li>Often require stationarity or specific feature engineering to handle temporal correlations.</li></ul><h3 id=tree-based-approaches>Tree-based Approaches<a hidden class=anchor aria-hidden=true href=#tree-based-approaches>#</a></h3><ul><li><strong>Decision Trees</strong>, <strong>Random Forests</strong>, <strong>Gradient Boosted Trees</strong>, etc.</li><li>Work well with tabular features extracted from time windows (though must consider autocorrelation).</li></ul><h3 id=ensemble-methods>Ensemble Methods<a hidden class=anchor aria-hidden=true href=#ensemble-methods>#</a></h3><ul><li><strong>Bagging</strong>, <strong>Boosting</strong></li><li><strong>Proximity Forest</strong>, <strong>Time Series Forest</strong> (specialized ensemble methods).</li></ul><h4 id=interval-based-methods>Interval-based Methods<a hidden class=anchor aria-hidden=true href=#interval-based-methods>#</a></h4><ul><li><strong>Time Series Forest</strong></li><li><strong>Random Interval Spectral Ensemble (RISE)</strong></li><li><strong>Supervised Time Series Forest</strong><br>These extract features (mean, variance, slope, etc.) from various intervals of a time series.</li></ul><hr><h2 id=12-dictionary-based-and-shapelet-based-models>12. Dictionary-based and Shapelet-based Models<a hidden class=anchor aria-hidden=true href=#12-dictionary-based-and-shapelet-based-models>#</a></h2><h3 id=dictionary-based-approaches>Dictionary-based Approaches<a hidden class=anchor aria-hidden=true href=#dictionary-based-approaches>#</a></h3><p>Convert time series into “documents” of discrete symbols and then analyze them with bag-of-words or similar text mining techniques:</p><ul><li><strong>Bag of Patterns (BOP)</strong></li><li><strong>Bag of SFA Symbols (BOSS)</strong></li><li><strong>WEASEL</strong> (Word ExtrAction for time SEries cLassification)</li></ul><h3 id=shapelet-based-models>Shapelet-based Models<a hidden class=anchor aria-hidden=true href=#shapelet-based-models>#</a></h3><p>A <strong>shapelet</strong> is a small subsequence that is highly representative or discriminative of a specific class.</p><ol><li><strong>Extraction</strong>: Identify the most discriminative subsequences.</li><li><strong>Transformation</strong>: Convert each full time series to a vector of distances to shapelets.</li><li><strong>Classification</strong>: Train any standard classifier (e.g., SVM, Random Forest) on shapelet-distance features.</li></ol><p><input type=checkbox id=zoomCheck-b47b4 hidden>
<label for=zoomCheck-b47b4><img class=zoomCheck loading=lazy decoding=async src=./img/6.png alt=6></label></p><hr><h2 id=13-multivariate-time-series>13. Multivariate Time Series<a hidden class=anchor aria-hidden=true href=#13-multivariate-time-series>#</a></h2><p>Multiple channels measured simultaneously:</p><ul><li><strong>Independent Assumption</strong>: Model each channel separately if they don’t interact strongly.</li><li><strong>Concatenation</strong>: Flatten all channels into one univariate series (loses some cross-channel info).</li><li><strong>Advanced Methods</strong>:<ul><li><strong>MUSE</strong> (extension of WEASEL) integrates multiple channels.</li><li>Neural networks (e.g., LSTM) that handle multiple input features.</li></ul></li></ul><p><input type=checkbox id=zoomCheck-61039 hidden>
<label for=zoomCheck-61039><img class=zoomCheck loading=lazy decoding=async src=./img/7.png alt=7></label></p><hr><h2 id=14-deep-learning-methods>14. Deep Learning Methods<a hidden class=anchor aria-hidden=true href=#14-deep-learning-methods>#</a></h2><ul><li><p><strong>CNNs</strong><br>Exploit convolution + pooling layers to automatically learn local features from raw data.<br><input type=checkbox id=zoomCheck-4c10f hidden>
<label for=zoomCheck-4c10f><img class=zoomCheck loading=lazy decoding=async src=./img/8.png alt=8></label></p></li><li><p><strong>RNNs / LSTMs</strong><br>Model long-term dependencies, capturing temporal context across many time steps.</p></li><li><p><strong>Inception Networks</strong><br>Use multi-scale filters in parallel (adapted from computer vision).<br>![[Pasted image 20250409113111.png]]</p></li><li><p><strong>TapNet</strong>, <strong>Multivariate LSTM-FCN</strong>, etc.<br>Merge RNNs/CNNs with fully-connected layers for robust feature extraction.</p></li></ul><h3 id=kernel-based-models>Kernel-based Models<a hidden class=anchor aria-hidden=true href=#kernel-based-models>#</a></h3><ul><li><strong>ROCKET</strong> (RandOm Convolutional KErnel Transform)</li><li><strong>MiniRocket</strong>, <strong>MultiRocket</strong></li><li><strong>Hydra</strong>, <strong>MultiROCKET-Hydra</strong></li></ul><p>They transform time series via numerous random convolutional kernels, then feed into a linear model. Offers strong performance with high efficiency.</p><hr><h2 id=15-hybrid-models>15. Hybrid Models<a hidden class=anchor aria-hidden=true href=#15-hybrid-models>#</a></h2><ul><li><p><strong>HIVE-COTE</strong> (Hierarchical Vote Collective of Transformation-based Ensembles)</p><ul><li>Combines diverse transformation modules (e.g., shapelets, dictionary methods, intervals).</li></ul></li><li><p><strong>TS-CHIEF</strong> (Time Series Combination of Heterogeneous and Integrated Embeddings Forest)</p><ul><li>Randomized decision trees using multiple embedded approaches.</li></ul></li></ul><p>These ensembles often achieve state-of-the-art classification accuracy on benchmark datasets by combining complementary representations.</p><hr><h2 id=16-explainable-ai>16. Explainable AI<a hidden class=anchor aria-hidden=true href=#16-explainable-ai>#</a></h2><p>With complex models (deep networks, large ensembles), interpretability can be challenging:</p><ul><li><strong>Feature Attribution</strong>: Methods like Grad-CAM, integrated gradients, or saliency maps adapted to time series.</li><li><strong>Shapelet-based</strong>: Provides subsequences that are inherently interpretable.</li><li><strong>Surrogate Models</strong>: Train a simpler, interpretable model (e.g., decision tree) to mimic the predictions of a black-box.</li><li><strong>Rule Extraction</strong>: Derive approximate rules or patterns from complex models.</li></ul><hr><h2 id=17-time-series-forecasting>17. Time Series Forecasting<a hidden class=anchor aria-hidden=true href=#17-time-series-forecasting>#</a></h2><p>Forecasting means predicting future observations from historical data. Errors are typically measured by:</p><ul><li><strong>MAE</strong> (Mean Absolute Error)</li><li><strong>RMSE</strong> (Root Mean Squared Error)</li><li><strong>MAPE</strong> (Mean Absolute Percentage Error)</li></ul><h3 id=simple-methods>Simple Methods<a hidden class=anchor aria-hidden=true href=#simple-methods>#</a></h3><ol><li><strong>Average Method</strong>: Forecast is the mean of all past data.</li><li><strong>Naïve Method</strong>: Forecast is just the last observed value.</li><li><strong>Drift Method</strong>: Linear extrapolation from the first to the last observed point.</li></ol><p>These serve as baselines. More sophisticated models can outperform them, but these are often used as references.</p><hr><h2 id=18-exponential-smoothing-family>18. Exponential Smoothing Family<a hidden class=anchor aria-hidden=true href=#18-exponential-smoothing-family>#</a></h2><ol><li><p><strong>SES (Simple Exponential Smoothing)</strong><br>Suitable for data with no trend or seasonality. Recent observations get higher weights.</p></li><li><p><strong>Holt’s Method</strong><br>Extends SES with a <strong>trend</strong> component.<br><input type=checkbox id=zoomCheck-7fcd0 hidden>
<label for=zoomCheck-7fcd0><img class=zoomCheck loading=lazy decoding=async src=./img/9.png alt=9></label></p></li><li><p><strong>Holt–Winters Method</strong><br>Includes <strong>seasonality</strong> (additive or multiplicative).<br><input type=checkbox id=zoomCheck-8839e hidden>
<label for=zoomCheck-8839e><img class=zoomCheck loading=lazy decoding=async src=./img/10.png alt=10></label></p></li></ol><hr><h2 id=19-arima-based-models>19. ARIMA-based Models<a hidden class=anchor aria-hidden=true href=#19-arima-based-models>#</a></h2><p><strong>ARIMA</strong> (AutoRegressive Integrated Moving Average) captures autocorrelation in time series.</p><h3 id=arp-model>AR(p) Model<a hidden class=anchor aria-hidden=true href=#arp-model>#</a></h3><p>$$
y_t = c + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + e_t
$$</p><h3 id=maq-model>MA(q) Model<a hidden class=anchor aria-hidden=true href=#maq-model>#</a></h3><p>$$
y_t = c + e_t + \theta_1 e_{t-1} + \dots + \theta_q e_{t-q}
$$</p><h3 id=arimapdq>ARIMA(p,d,q)<a hidden class=anchor aria-hidden=true href=#arimapdq>#</a></h3><p>Incorporates differencing of order (d) to handle non-stationarity.</p><p><input type=checkbox id=zoomCheck-d6946 hidden>
<label for=zoomCheck-d6946><img class=zoomCheck loading=lazy decoding=async src=./img/11.png alt=11></label></p><h3 id=sarima>SARIMA<a hidden class=anchor aria-hidden=true href=#sarima>#</a></h3><p>Incorporates seasonal terms:</p><ul><li><strong>AutoARIMA</strong> can automatically select (p, d, q) (and seasonal orders).</li><li><strong>Prophet</strong> (by Facebook/Meta) is another robust tool that handles multiple seasonalities, holidays, regressors, etc.</li></ul><hr><h2 id=20-forecasting-via-reduced-regression>20. Forecasting via Reduced Regression<a hidden class=anchor aria-hidden=true href=#20-forecasting-via-reduced-regression>#</a></h2><p>This “<strong>reduction</strong>” approach converts forecasting into a supervised learning problem:</p><ol><li><strong>Choose a window size</strong> ( w ).<ul><li>E.g., use the last ( w ) observations as features.</li></ul></li><li><strong>Predict the next value</strong> (or multiple future values).</li><li><strong>Use standard regression methods</strong>: Random forests, linear regression, XGBoost, etc.</li></ol><p>For multi-step forecasting, one can repeat this approach or predict multiple future time steps at once.</p><hr><h2 id=21-forecasting-via-deep-learning>21. Forecasting via Deep Learning<a hidden class=anchor aria-hidden=true href=#21-forecasting-via-deep-learning>#</a></h2><ul><li><strong>RNN/LSTM</strong><br>Capture long-term sequential dependencies. Variants (GRU, Bi-LSTM) may improve performance.</li><li><strong>Sequence-to-Sequence (Seq2Seq) Networks</strong><br>Originally used for machine translation, can handle multi-step forecasting.</li><li><strong>Temporal Fusion Transformers</strong><br>Combine attention mechanisms with recurrent networks to handle complex time series with covariates.</li></ul><p>Neural networks excel with large datasets and can learn nonlinear patterns that simpler models might miss.</p><hr><h2 id=additional-considerations>Additional Considerations<a hidden class=anchor aria-hidden=true href=#additional-considerations>#</a></h2><ul><li><strong>Hyperparameter Tuning</strong>: Time series models often have multiple hyperparameters (e.g., ARIMA orders, number of hidden units in an LSTM). Automated searches like grid search or Bayesian optimization can help.</li><li><strong>Model Validation</strong>: Standard cross-validation splits might not apply directly because of the temporal order. Techniques like <strong>rolling forecasting origin</strong> or <strong>walk-forward validation</strong> preserve the time structure.</li><li><strong>Performance Metrics</strong>: Selecting metrics that align with business goals or practical considerations (e.g., if small absolute errors or relative errors matter more).</li><li><strong>Domain Knowledge</strong>: Often crucial in deciding how to handle missing data, outliers, or interpret model outputs.</li></ul><hr><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>Time series analysis and forecasting encompass a broad spectrum of methods, from simple baselines (Naïve, Average) and classic models (ARIMA, exponential smoothing) to advanced machine learning techniques (deep learning, shapelets, kernel-based ensembles). The choice of method depends on:</p><ul><li><strong>Data Characteristics</strong>: Stationarity, seasonality, presence of trends, magnitude of noise.</li><li><strong>Task Requirements</strong>: Single-step forecasting, anomaly detection, classification, etc.</li><li><strong>Computational Constraints</strong>: For large-scale or high-frequency data, efficient approximation or specialized algorithms may be needed.</li><li><strong>Explainability</strong>: Simpler models or shapelet-based approaches may provide clearer insights, while deep models might yield higher accuracy but be harder to interpret.</li></ul><p>Overall, success in time series analysis hinges on proper preprocessing (missing-value handling, outlier management, normalization, feature engineering) and a careful choice of models and validation strategies.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ldomenichelli.github.io/posts/post8/><span class=title>« Prev</span><br><span>Statistical Learning and Large Data</span>
</a><a class=next href=https://ldomenichelli.github.io/posts/post4/><span class=title>Next »</span><br><span>Topological Data Analysis</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>