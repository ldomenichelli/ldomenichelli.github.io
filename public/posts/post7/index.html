<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Geometric Deep Learning | lucia's notes</title>
<meta name=keywords content="geometric,deep learning"><meta name=description content="Some notes on geometric deep learning"><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post7/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post7/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="Geometric Deep Learning"><meta property="og:description" content="Some notes on geometric deep learning"><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post7/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="Geometric Deep Learning"><meta name=twitter:description content="Some notes on geometric deep learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Geometric Deep Learning","item":"https://ldomenichelli.github.io/posts/post7/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Geometric Deep Learning","name":"Geometric Deep Learning","description":"Some notes on geometric deep learning","keywords":["geometric","deep learning"],"articleBody":"🏗️ Representation Learning for NLP All neural‑network (NN) architectures create vector representations—also called embeddings—of the input.\nThese vectors pack statistical and semantic cues that let the model classify, translate, or generate text.\nThe network learns better representations through feedback from a loss function.\nTransformers build features for each word with an attention mechanism that asks:\n“How important is every other word in the sentence to this word?”\n🔗 GNNs—Representing Graphs Graph Neural Networks (GNNs) or Graph Convolutional Networks (GCNs) embed nodes and edges.\nThey rely on neighbourhood aggregation / message passing:\nGNN update the hidden feature h of node i at layer l via a non-linear transformation of the node’s own feature $h_i^l$ added to the aggregation of feature $h_j^l$ from each neighboring node $j \\in N(i)$\n$$h_{i}^{\\ell+1} = \\sigma \\Big( U^{\\ell} h_{i}^{\\ell} + \\sum_{j \\in \\mathcal{N}(i)} \\left( V^{\\ell} h_{j}^{\\ell} \\right) \\Big)$$ where $U^l$ and $V^l$ are learnable weight matrices of the GNN layer and $\\sigma$ is a non-linear function such as ReLU.\nSymbol Meaning $(h_i^\\ell$) feature of node i at layer ($\\ell$) $(\\mathcal{N}(i))$ neighbours of i $(U^\\ell, V^\\ell)$ learnable weights ($\\sigma)$ non‑linearity (e.g., ReLU) Stacking layers lets information flow across the whole graph.\n🧩 Where Transformers Meet GNNs Replace the plain sum with a weighted sum via attention → you get a Graph Attention Network (GAT).\nAdd layer‑norm and an MLP, and voilà — a Graph Transformer!\nWhat we have with transformers instead, is this, for an hidden feature $h$:\n$$h_{i}^{\\ell+1} = \\text{Attention} \\left( Q^{\\ell} h_{i}^{\\ell} \\ , K^{\\ell} h_{j}^{\\ell} \\ , V^{\\ell} h_{j}^{\\ell} \\right)$$ with: $$\\ h_{i}^{\\ell+1} = \\sum_{j \\in \\mathcal{S}} w_{ij} \\left( V^{\\ell} h_{j}^{\\ell} \\right)$$ $$ \\text{where} \\ w_{ij} = \\text{softmax}_j \\left ( Q^{\\ell} h^{\\ell}_i \\cdot K^{\\ell} h^{\\ell}_j \\right)$$\n📝 Sentences as Graphs—But with Caveats Think of a sentence as a fully‑connected graph where every word links to every other.\nTransformers = GNNs with multi‑head attention acting as the aggregation rule.\nYet fully connected graphs mean quadratic growth in edges, which makes learning very long‑range word relations hard.\n🤔 Are Transformers Learning Neural Syntax? Studies suggest attention heads latch onto task‑specific syntax:\nAttention can surface the most relevant word pairs in a sentence. Different heads specialise in different syntactic cues. Graph‑theoretic view: can GNNs on full graphs reveal which edges matter most by inspecting the aggregation weights? This might expose the hidden structure driving model accuracy.\n📚 References Chaitanya K. Joshi, “Transformers are Graph Neural Networks,” The Gradient (2020). 🎥 YouTube Talk ","wordCount":"405","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post7/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script><script src="/assets/js/analytics.js" async></script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>Geometric Deep Learning</h1><div class=post-description>Some notes on geometric deep learning</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#representationlearning-for-nlp>🏗️ Representation Learning for NLP</a><ul><li><a href=#gnnsrepresenting-graphs>🔗 GNNs—Representing Graphs</a></li><li><a href=#where-transformers-meet-gnns>🧩 Where Transformers Meet GNNs</a></li></ul></li></ul></li><li><a href=#sentences-as-graphsbut-with-caveats>📝 Sentences as Graphs—But with Caveats</a><ul><li><a href=#are-transformers-learning-neural-syntax>🤔 Are Transformers Learning <em>Neural Syntax</em>?</a></li></ul></li><li><a href=#references>📚 References</a></li></ul></nav></div></details></div><div class=post-content><h3 id=representationlearning-for-nlp>🏗️ Representation Learning for NLP<a hidden class=anchor aria-hidden=true href=#representationlearning-for-nlp>#</a></h3><p>All neural‑network (NN) architectures create <strong>vector representations</strong>—also called <em>embeddings</em>—of the input.<br>These vectors pack statistical <em>and</em> semantic cues that let the model classify, translate, or generate text.</p><p>The network <em>learns</em> better representations through feedback from a <strong>loss function</strong>.</p><blockquote><p><strong>Transformers</strong> build features for each word with an <strong>attention mechanism</strong> that asks:<br><em>“How important is every other word in the sentence to this word?”</em></p></blockquote><h4 id=gnnsrepresenting-graphs>🔗 GNNs—Representing Graphs<a hidden class=anchor aria-hidden=true href=#gnnsrepresenting-graphs>#</a></h4><p>Graph Neural Networks (GNNs) or Graph Convolutional Networks (GCNs) embed <strong>nodes and edges</strong>.<br>They rely on <strong>neighbourhood aggregation / message passing</strong>:</p><p>GNN update the hidden feature <em>h</em> of node <em>i</em> at layer <em>l</em> via a non-linear transformation of the node&rsquo;s own feature $h_i^l$ added to the aggregation of feature $h_j^l$ from each neighboring node $j \in N(i)$</p><p>$$h_{i}^{\ell+1} = \sigma \Big( U^{\ell} h_{i}^{\ell} + \sum_{j \in \mathcal{N}(i)} \left( V^{\ell} h_{j}^{\ell} \right) \Big)$$
where $U^l$ and $V^l$ are learnable weight matrices of the GNN layer and $\sigma$ is a non-linear function such as ReLU.</p><table><thead><tr><th>Symbol</th><th>Meaning</th></tr></thead><tbody><tr><td>$(h_i^\ell$)</td><td>feature of node <em>i</em> at layer ($\ell$)</td></tr><tr><td>$(\mathcal{N}(i))$</td><td>neighbours of <em>i</em></td></tr><tr><td>$(U^\ell, V^\ell)$</td><td>learnable weights</td></tr><tr><td>($\sigma)$</td><td>non‑linearity (e.g., ReLU)</td></tr></tbody></table><p>Stacking layers lets information flow across the whole graph.</p><hr><h4 id=where-transformers-meet-gnns>🧩 Where Transformers Meet GNNs<a hidden class=anchor aria-hidden=true href=#where-transformers-meet-gnns>#</a></h4><p>Replace the plain sum with a <strong>weighted sum via attention</strong> → you get a <strong>Graph Attention Network (GAT)</strong>.<br>Add layer‑norm and an MLP, and voilà — a <strong>Graph Transformer</strong>!</p><p>What we have with transformers instead, is this, for an hidden feature $h$:</p><p>$$h_{i}^{\ell+1} = \text{Attention} \left( Q^{\ell} h_{i}^{\ell} \ , K^{\ell} h_{j}^{\ell} \ , V^{\ell} h_{j}^{\ell} \right)$$
with: $$\ h_{i}^{\ell+1} = \sum_{j \in \mathcal{S}} w_{ij} \left( V^{\ell} h_{j}^{\ell} \right)$$
$$ \text{where} \ w_{ij} = \text{softmax}_j \left ( Q^{\ell} h^{\ell}_i \cdot K^{\ell} h^{\ell}_j \right)$$</p><hr><h2 id=sentences-as-graphsbut-with-caveats>📝 Sentences as Graphs—But with Caveats<a hidden class=anchor aria-hidden=true href=#sentences-as-graphsbut-with-caveats>#</a></h2><p>Think of a sentence as a <strong>fully‑connected graph</strong> where every word links to every other.<br>Transformers = <em>GNNs with multi‑head attention</em> acting as the aggregation rule.</p><p>Yet fully connected graphs mean <strong>quadratic growth in edges</strong>, which makes learning <em>very</em> long‑range word relations hard.</p><hr><h3 id=are-transformers-learning-neural-syntax>🤔 Are Transformers Learning <em>Neural Syntax</em>?<a hidden class=anchor aria-hidden=true href=#are-transformers-learning-neural-syntax>#</a></h3><p>Studies suggest attention heads latch onto task‑specific syntax:</p><ul><li>Attention can surface the <em>most relevant</em> word pairs in a sentence.</li><li>Different heads specialise in different syntactic cues.</li></ul><p>Graph‑theoretic view: can GNNs on full graphs <strong>reveal which edges matter most</strong> by inspecting the aggregation weights? This might expose the hidden structure driving model accuracy.</p><hr><h2 id=references>📚 References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>Chaitanya K. Joshi, <strong>“Transformers are Graph Neural Networks,”</strong> <em>The Gradient</em> (2020).</li><li>🎥 <a href="https://www.youtube.com/watch?v=qAF3ZHmkXUw">YouTube Talk</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://ldomenichelli.github.io/tags/geometric/>Geometric</a></li><li><a href=https://ldomenichelli.github.io/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=prev href=https://ldomenichelli.github.io/posts/post1/><span class=title>« Prev</span><br><span>Embeddings space 𖦹ׂ ₊˚⊹⋆</span>
</a><a class=next href=https://ldomenichelli.github.io/posts/post2/><span class=title>Next »</span><br><span>Optimal Transport 🕷️ and Wasserstein distance</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>