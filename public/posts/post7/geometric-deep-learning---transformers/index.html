<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Geometric Deep Learning | lucia's notes</title>
<meta name=keywords content="geometric,deep learning"><meta name=description content='some notes on geometric deep learning"'><meta name=author content="Lucia"><link rel=canonical href=http://localhost:1313/posts/post7/geometric-deep-learning---transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/post7/geometric-deep-learning---transformers/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="Geometric Deep Learning"><meta property="og:description" content='some notes on geometric deep learning"'><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/post7/geometric-deep-learning---transformers/"><meta property="og:image" content="http://localhost:1313/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/logo.png"><meta name=twitter:title content="Geometric Deep Learning"><meta name=twitter:description content='some notes on geometric deep learning"'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Geometric Deep Learning","item":"http://localhost:1313/posts/post7/geometric-deep-learning---transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Geometric Deep Learning","name":"Geometric Deep Learning","description":"some notes on geometric deep learning\"","keywords":["geometric","deep learning"],"articleBody":"Representation learning for NLP All NN architechtures build representaions of input data as vectors/embeddings, which encode useful statistical and semantic information about the data that can be used to classify or traslate something.\nThe NN learns to build beter representations by receiving a feedback, usually via error/loss function. Transformers build features of each word using an [attention mechanism]to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word\nGNN build representations of graphs GNN or GCN build representations of nodes and edges in graph data. They do so through neighbourhood aggregation (or message passing), where each node gathers features from its neightbours yo update its representations of the local graph structure arpund it. Stacking several GNN layers enables the model to propagate each node‚Äôs feature over the entire graph.\nGNN upade the hidden feature h of node i at layer l via a non-linear transformation of the node‚Äôs own feature $h_i^l$ added to the aggregation of feature $h_j^l$ from each neighboring node $j \\in N(i)$\n$$h_{i}^{\\ell+1} = \\sigma \\Big( U^{\\ell} h_{i}^{\\ell} + \\sum_{j \\in \\mathcal{N}(i)} \\left( V^{\\ell} h_{j}^{\\ell} \\right) \\Big)$$ where $U^l$ and $V^l$ are learnable weight matrices of the GNN layer and $\\sigma$ is a non-linear function such as ReLU.\nWhere does the connection with Trasformers appear? The summation over the neightbourhood nodes can be replaced by other input size-invariant functions, such as a wehighted sum via an attention mechanism. That way we would have a GAT, Graph Attention Network. Add normalization and the feed-forward MLP, and voila, we have a Graph Transformer!\n![[Pasted image 20241207183239.png]]\nWhat we have with transformers istead, is this, for an hidden feature $h$:\n$$h_{i}^{\\ell+1} = \\text{Attention} \\left( Q^{\\ell} h_{i}^{\\ell} \\ , K^{\\ell} h_{j}^{\\ell} \\ , V^{\\ell} h_{j}^{\\ell} \\right)$$ with: $$\\ h_{i}^{\\ell+1} = \\sum_{j \\in \\mathcal{S}} w_{ij} \\left( V^{\\ell} h_{j}^{\\ell} \\right)$$ $$ \\text{where} \\ w_{ij} = \\text{softmax}_j \\left ( Q^{\\ell} h^{\\ell}_i \\cdot K^{\\ell} h^{\\ell}_j \\right)$$\n![[Pasted image 20241207184230.png]]\nSentences are fully connected word graphs ‚Ä¶ or not? Consider a sentence as a fully-connected graph, where each word is connected to every other word. Now, we can use a GNN to build features for each node (word) in the graph (sentence). Broadly, this is what Transformers are doing: they are GNNs with multi-head attention as the neighbourhood aggregation function.\nBefore statistical NLP and ML, linguists like Noam Chomsky focused on developing formal theories of linguistic structure, such as syntax trees/graphs. Tree LSTMs already tried this, but maybe Transformers/GNNs are better architectures for bringing together the two worlds of linguistic theory and statistical NLP?\nLong term dependencies fully-connected graphs is that they make learning very long-term dependencies between words difficult . This is simply due to how the number of edges in the graph scales quadratically with the number of nodes üò¢.\nAre Transformers learning neural syntax? There have been several notable studies exploring what Transformers might be learning, especially within the NLP community. These include insights into how attention mechanisms across all word pairs in a sentence can identify the most relevant pairs, enabling Transformers to capture something akin to a task-specific syntax. Moreover, different heads in multi-head attention appear to focus on distinct syntactic features, offering specialized perspectives on linguistic structure.\nFraming this in terms of graph theory: can the use of Graph Neural Networks (GNNs) on full graphs help us uncover the most significant edges? Specifically, can we infer their roles based on how GNNs aggregate information across neighborhoods at each layer? This approach could provide deeper insights into the connections driving model performance.\nReferences Chaitanya K. Joshi, ‚ÄúTransformers are Graph Neural Networks‚Äù, The Gradient, 2020. [Youtube Video] (https://www.youtube.com/watch?v=qAF3ZHmkXUw) ","wordCount":"603","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/post7/geometric-deep-learning---transformers/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script><script src="/assets/js/analytics.js" async></script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/posts/ title=notes><span>notes</span></a></li><li><a href=http://localhost:1313/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/></a></div><h1 class=post-title>Geometric Deep Learning</h1><div class=post-description>some notes on geometric deep learning"</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#representation-learning-for-nlp>Representation learning for NLP</a><ul><li><a href=#gnn-build-representations-of-graphs>GNN build representations of graphs</a></li></ul></li></ul></li><li><a href=#sentences-are-fully-connected-word-graphs--or-not>Sentences are fully connected word graphs &mldr; or not?</a><ul><li><ul><li><a href=#long-term-dependencies>Long term dependencies</a></li><li><a href=#are-transformers-learning-_neural-syntax_>Are Transformers learning <em>neural syntax</em>?</a></li></ul></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><h3 id=representation-learning-for-nlp>Representation learning for NLP<a hidden class=anchor aria-hidden=true href=#representation-learning-for-nlp>#</a></h3><p>All NN architechtures build <em>representaions</em> of input data as vectors/embeddings, which encode useful statistical and semantic information about the data that can be used to classify or traslate something.</p><p>The NN <em>learns</em> to build beter representations by receiving a feedback, usually via error/loss function.
Transformers build features of each word using an [attention mechanism]to figure out how important <strong>all the other words</strong> in the sentence are w.r.t. to the aforementioned word</p><h4 id=gnn-build-representations-of-graphs>GNN build representations of graphs<a hidden class=anchor aria-hidden=true href=#gnn-build-representations-of-graphs>#</a></h4><p><em>GNN</em> or <em>GCN</em> build representations of nodes and edges in graph data. They do so through <strong>neighbourhood aggregation</strong> (or message passing), where each node gathers features from its neightbours yo update its representations of the local graph structure arpund it.
Stacking several GNN layers enables the model to propagate each node&rsquo;s feature over the entire graph.</p><p>GNN upade the hidden feature <em>h</em> of node <em>i</em> at layer <em>l</em> via a non-linear transformation of the node&rsquo;s own feature $h_i^l$ added to the aggregation of feature $h_j^l$ from each neighboring node $j \in N(i)$</p><p>$$h_{i}^{\ell+1} = \sigma \Big( U^{\ell} h_{i}^{\ell} + \sum_{j \in \mathcal{N}(i)} \left( V^{\ell} h_{j}^{\ell} \right) \Big)$$
where $U^l$ and $V^l$ are learnable weight matrices of the GNN layer and $\sigma$ is a non-linear function such as ReLU.</p><p>Where does the connection with Trasformers appear?
The summation over the neightbourhood nodes can be replaced by other input size-invariant functions, such as a <strong>wehighted sum via an attention mechanism</strong>. That way we would have a <em>GAT</em>, <strong>Graph Attention Network</strong>. Add normalization and the feed-forward MLP, and voila, we have a <strong>Graph Transformer</strong>!</p><p>![[Pasted image 20241207183239.png]]</p><p>What we have with transformers istead, is this, for an hidden feature $h$:</p><p>$$h_{i}^{\ell+1} = \text{Attention} \left( Q^{\ell} h_{i}^{\ell} \ , K^{\ell} h_{j}^{\ell} \ , V^{\ell} h_{j}^{\ell} \right)$$
with: $$\ h_{i}^{\ell+1} = \sum_{j \in \mathcal{S}} w_{ij} \left( V^{\ell} h_{j}^{\ell} \right)$$
$$ \text{where} \ w_{ij} = \text{softmax}_j \left ( Q^{\ell} h^{\ell}_i \cdot K^{\ell} h^{\ell}_j \right)$$</p><p>![[Pasted image 20241207184230.png]]</p><h2 id=sentences-are-fully-connected-word-graphs--or-not>Sentences are fully connected word graphs &mldr; or not?<a hidden class=anchor aria-hidden=true href=#sentences-are-fully-connected-word-graphs--or-not>#</a></h2><p>Consider a sentence as a fully-connected graph, where each word is connected to every other word. Now, we can use a GNN to build features for each node (word) in the graph (sentence).
Broadly, this is what Transformers are doing: they are <strong>GNNs with multi-head attention</strong> as the neighbourhood aggregation function.</p><p>Before statistical NLP and ML, linguists like Noam Chomsky focused on developing formal theories of <a href=https://en.wikipedia.org/wiki/Syntactic_Structures>linguistic structure</a>, such as <strong>syntax trees/graphs</strong>. <a href=https://arxiv.org/abs/1503.00075>Tree LSTMs</a> already tried this, but maybe Transformers/GNNs are better architectures for bringing together the two worlds of linguistic theory and statistical NLP?</p><h4 id=long-term-dependencies>Long term dependencies<a hidden class=anchor aria-hidden=true href=#long-term-dependencies>#</a></h4><p>fully-connected graphs is that they make <strong>learning very long-term dependencies between words difficult</strong> . This is simply due to how the number of edges in the graph <em>scales quadratically</em> with the number of nodes üò¢.</p><h4 id=are-transformers-learning-_neural-syntax_>Are Transformers learning <em>neural syntax</em>?<a hidden class=anchor aria-hidden=true href=#are-transformers-learning-_neural-syntax_>#</a></h4><p>There have been several <a href=https://pair-code.github.io/interpretability/bert-tree/>notable studies</a> exploring what Transformers might be learning, especially within the NLP community. These include <a href=https://arxiv.org/abs/1905.05950>insights</a> into how attention mechanisms across all word pairs in a sentence can identify the most relevant pairs, enabling Transformers to capture something akin to a <strong>task-specific syntax</strong>. Moreover, different heads in multi-head attention appear to focus on distinct syntactic features, offering specialized perspectives on linguistic structure.</p><p>Framing this in terms of graph theory: can the use of Graph Neural Networks (GNNs) on full graphs help us <a href=https://arxiv.org/abs/2002.04999>uncover the most significant edges</a>? Specifically, can we infer their roles based on how GNNs aggregate information across neighborhoods at each layer? This approach could provide deeper insights into the connections driving model performance.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>Chaitanya K. Joshi, &ldquo;Transformers are Graph Neural Networks&rdquo;, The Gradient, 2020.</li><li>[Youtube Video] (<a href="https://www.youtube.com/watch?v=qAF3ZHmkXUw">https://www.youtube.com/watch?v=qAF3ZHmkXUw</a>)</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/geometric/>Geometric</a></li><li><a href=http://localhost:1313/tags/deep-learning/>Deep Learning</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/post5/><span class=title>Next ¬ª</span><br><span>HPLT & NLPL Winter School</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=http://localhost:1313/>lucia's notes</a></span>
<span>‚Ä¢
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>‚Ä¢
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>