<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Lucia</title>
<meta name=keywords content><meta name=description content="In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity).
Anisotropy has been widely observed among self-supervised models based on Transformers -> recent literature says it could be related to optimizing the [[cross entropy loss[https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html]]
Anisotropy in pretrained BERTs

CharacterBERT
CANINE -> s downsampling contextualized character representations via astrided convolution before feeding them to aTransformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c).
MANTa-LM-> is based on a differentiable segmentation and embed- ding module added before an encoder-decoder model in the style of T5
ByT5 -> is a version of T5 trained at byte - level.

Metric used is COSINE SIMILARITY"><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post2/><link crossorigin=anonymous href=/assets/css/stylesheet.545ff313fe3387bb6faa83d75fcde7d20949cbe2fb53935708ada17d12aff612.css integrity="sha256-VF/zE/4zh7tvqoPXX83n0glJy+L7U5NXCK2hfRKv9hI=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content><meta property="og:description" content="In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity).
Anisotropy has been widely observed among self-supervised models based on Transformers -> recent literature says it could be related to optimizing the [[cross entropy loss[https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html]]
Anisotropy in pretrained BERTs

CharacterBERT
CANINE -> s downsampling contextualized character representations via astrided convolution before feeding them to aTransformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c).
MANTa-LM-> is based on a differentiable segmentation and embed- ding module added before an encoder-decoder model in the style of T5
ByT5 -> is a version of T5 trained at byte - level.

Metric used is COSINE SIMILARITY"><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post2/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="Lucia"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content><meta name=twitter:description content="In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity).
Anisotropy has been widely observed among self-supervised models based on Transformers -> recent literature says it could be related to optimizing the [[cross entropy loss[https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html]]
Anisotropy in pretrained BERTs

CharacterBERT
CANINE -> s downsampling contextualized character representations via astrided convolution before feeding them to aTransformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c).
MANTa-LM-> is based on a differentiable segmentation and embed- ding module added before an encoder-decoder model in the style of T5
ByT5 -> is a version of T5 trained at byte - level.

Metric used is COSINE SIMILARITY"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"","item":"https://ldomenichelli.github.io/posts/post2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Anisotropy has been widely observed among self-supervised models based on Transformers -\u0026gt; recent literature says it could be related to optimizing the [[cross entropy loss[https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html]]\nAnisotropy in pretrained BERTs CharacterBERT CANINE -\u0026gt; s downsampling contextualized character representations via astrided convolution before feeding them to aTransformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c). MANTa-LM-\u0026gt; is based on a differentiable segmentation and embed- ding module added before an encoder-decoder model in the style of T5 ByT5 -\u0026gt; is a version of T5 trained at byte - level. Metric used is COSINE SIMILARITY\n","keywords":[],"articleBody":"In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Anisotropy has been widely observed among self-supervised models based on Transformers -\u003e recent literature says it could be related to optimizing the [[cross entropy loss[https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html]]\nAnisotropy in pretrained BERTs CharacterBERT CANINE -\u003e s downsampling contextualized character representations via astrided convolution before feeding them to aTransformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c). MANTa-LM-\u003e is based on a differentiable segmentation and embed- ding module added before an encoder-decoder model in the style of T5 ByT5 -\u003e is a version of T5 trained at byte - level. Metric used is COSINE SIMILARITY\nNeither of these architectures should suffer from out-of-vocabulary tokens in the process of creating representations. The models that predict at word or sub-word level (CharacterBERT and CANINE-s) could have the cross-entropy loss systematically pushing away rare item representations. However, it is rather unclear why it would imply an embedding drift at deeper layers. Hence, if anisotropy was only caused by the presence of unused or rare subwords, those character-level models should be much less prone to this issue. All models were tested on the [[WikiText-103 corpus]] ![[Pasted image 20241007160113.png]] All models show high levels of anisotropy, even character-based ones!\nLinguistic properties only? Authors proceed to try speech and vision models to show anisotropy is not related to subtokens themselves!\nHuBERT MiT DEiT BEiT ![[Pasted image 20241007161746.png]] Speech models ![[Pasted image 20241007161804.png]] Vision models Video goes a little better (?)\nConvolutional NN (vision based) ResNet-\u003e Apparently this models have isotropic representations! *EfficientNet ConvNeXt VAN This could partially be explained by the fact that the batch normalization (Ioffe and Szegedy, 2015) used in some of these models mitigates a posteriori the drift effect by re- moving the mean component of the representations. However, the ConvNeXt model also seems to use isotropic representations while not using batch normalization, which shows that this is not the only factor in the isotropic behavior of these models. ![[Pasted image 20241007162858.png]] Correlation ? They also find that spearman’s correlation doesn’t often correlate with the drift (norm) of the token embeddings!\nExploring the representation drift Here, with no assumption on data distribution and no training of the Transformer block. They add some bias b to the input representation x . Specifically, we study the average norm of the input representations $E(||xi + b||2$ ) against the average norm of the output representations $E(||T (xi + b)||{2}$ )$ . We also retrieve the self-attention scores before the softmax operation.\n![[Pasted image 20241007164834.png]]No matter the bias, output representations have higher cosine similarity .\n![[Pasted image 20241007165048.png]] There is a fixed point! $$ E_{x,bN }∗ (||xi + b_N ∗ ||) = E_{x,bN} ∗ (||T (x_i + b_{N ∗})||)$$ This hints that the model’s representations stabilize when their norm is close to this fixed point!\nThe transformers Block We look closely at the self-attention operation. As the norm of the average $x$ of the input increases , also $Q$ and $K$ increase. ![[Pasted image 20241007170230.png]]\nImpact of the drift we retrieve softmax values in the self-attention block and for each position, they ex- tract the maximum, the median and the minimum. as the input bias norm increases, the self-attention softmax distributions tend to become ess entropic, evolving towards higher maximal probabilities and lower minimal probabilities. ![[Pasted image 20241008160615.png]]\n","wordCount":"573","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post2/"},"publisher":{"@type":"Organization","name":"Lucia","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="Lucia's Notes (Alt + H)"><img src=https://ldomenichelli.github.io/logo.svg alt="Site icon in header" aria-label=logo height=35>Lucia's Notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=Random><span>Random</span></a></li><li><a href=https://ldomenichelli.github.io/games/ title=Games><span>Games</span></a></li><li><a href=#ZgotmplZ title="Suggested pages"><span>Suggested pages</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title></h1><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#anisotropy-in-pretrained-berts>Anisotropy in pretrained BERTs</a></li></ul></li><li><a href=#linguistic-properties-only>Linguistic properties only?</a><ul><li><a href=#convolutional-nn-vision-based>Convolutional NN (vision based)</a></li></ul></li><li><a href=#correlation->Correlation ?</a></li><li><a href=#exploring-the-representation-drift>Exploring the representation drift</a></li><li><a href=#the-transformers-block>The transformers Block</a></li><li><a href=#impact-of-the-drift>Impact of the drift</a></li></ul></nav></div></details></div><div class=post-content><p>In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity).
Anisotropy has been widely observed among self-supervised models based on Transformers -> recent literature says it could be related to optimizing the [[cross entropy loss[https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html]]</p><h3 id=anisotropy-in-pretrained-berts>Anisotropy in pretrained BERTs<a hidden class=anchor aria-hidden=true href=#anisotropy-in-pretrained-berts>#</a></h3><ul><li><em>CharacterBERT</em></li><li><em>CANINE</em> -> s downsampling contextualized character representations via astrided convolution before feeding them to aTransformers. It can be trained either with a subword-based objective (CANINE-s) or with a character-level one (CANINE-c).</li><li><em>MANTa-LM</em>-> is based on a differentiable segmentation and embed- ding module added before an encoder-decoder model in the style of T5</li><li><em>ByT5</em> -> is a version of T5 trained at byte - level.</li></ul><p><strong>Metric used is COSINE SIMILARITY</strong></p><p>Neither of these architectures should suffer from out-of-vocabulary tokens in the process of creating representations. The models that predict at word or sub-word level (CharacterBERT and CANINE-s) could have the cross-entropy loss systematically
pushing away rare item representations. However, it is rather unclear why it would imply an embedding drift at deeper layers. Hence, if anisotropy was only caused by the presence of unused or rare subwords, those character-level models should be
much less prone to this issue.
All models were tested on the [[WikiText-103 corpus]]
![[Pasted image 20241007160113.png]]
All models show high levels of anisotropy, even character-based ones!</p><h2 id=linguistic-properties-only>Linguistic properties only?<a hidden class=anchor aria-hidden=true href=#linguistic-properties-only>#</a></h2><p>Authors proceed to try speech and vision models to show anisotropy is not related to subtokens themselves!</p><ul><li><em>HuBERT</em></li><li><em>MiT</em></li><li><em>DEiT</em></li><li><em>BEiT</em></li></ul><p>![[Pasted image 20241007161746.png]]
Speech models
![[Pasted image 20241007161804.png]]
Vision models
Video goes a little better (?)</p><h3 id=convolutional-nn-vision-based>Convolutional NN (vision based)<a hidden class=anchor aria-hidden=true href=#convolutional-nn-vision-based>#</a></h3><ul><li><em>ResNet</em>-> Apparently this models have isotropic representations!</li><li>*EfficientNet</li><li><em>ConvNeXt</em></li><li><em>VAN</em>
This could partially be explained by the fact that the batch normalization (Ioffe and Szegedy, 2015) used in some of these models mitigates a posteriori the drift effect by re-
moving the mean component of the representations. However, the ConvNeXt model also seems to use isotropic representations while not using batch normalization, which shows that this is not the only factor in the isotropic behavior of these models.
![[Pasted image 20241007162858.png]]</li></ul><h2 id=correlation->Correlation ?<a hidden class=anchor aria-hidden=true href=#correlation->#</a></h2><p>They also find that spearman&rsquo;s correlation doesn&rsquo;t often correlate with the drift (norm) of the token embeddings!</p><h2 id=exploring-the-representation-drift>Exploring the representation drift<a hidden class=anchor aria-hidden=true href=#exploring-the-representation-drift>#</a></h2><p>Here, with no assumption on data distribution and no training of the Transformer block.
They add some bias <em>b</em> to the input representation <em>x</em> .
Specifically, we study the average norm of the input representations $E(||xi + b||<em>2$ ) against
the average norm of the output representations $E(||T (xi + b)||</em>{2}$
)$ . We also retrieve the self-attention scores before the softmax operation.</p><p>![[Pasted image 20241007164834.png]]No matter the bias, output representations have higher cosine similarity .</p><p>![[Pasted image 20241007165048.png]]
There is a fixed point!
$$ E_{x,bN }∗ (||xi + b_N ∗ ||) = E_{x,bN} ∗ (||T (x_i + b_{N ∗})||)$$
This hints that the model’s representations stabilize when their norm is close to this fixed point!</p><h2 id=the-transformers-block>The transformers Block<a hidden class=anchor aria-hidden=true href=#the-transformers-block>#</a></h2><p>We look closely at the self-attention operation. As the norm of the average $x$ of the input increases , also $Q$ and $K$ increase.
![[Pasted image 20241007170230.png]]</p><h2 id=impact-of-the-drift>Impact of the drift<a hidden class=anchor aria-hidden=true href=#impact-of-the-drift>#</a></h2><p>we retrieve softmax values in the self-attention block and for each position, they ex-
tract the maximum, the median and the minimum. as the input bias norm increases, the
self-attention softmax distributions tend to become ess entropic, evolving towards higher maximal probabilities and lower minimal probabilities.
![[Pasted image 20241008160615.png]]</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next »</span><br><span>Coming soon</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2024 <a href=https://ldomenichelli.github.io/>Lucia</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>