[{"content":"What is Abalone? Have you ever tried pushing your opponent off a cliff‚Ä¶ in a friendly way? If that sounds intriguing, let me introduce you to Abalone, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent L√©vi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\nThe game‚Äôs name comes from the abalone, a type of mollusk known for its \u0026ldquo;ear-shaped\u0026rdquo; shell. The Italian name for the game, aliotide, combines the Latin prefix \u0026ldquo;ab-\u0026rdquo; (meaning \u0026ldquo;from\u0026rdquo; or \u0026ldquo;away\u0026rdquo;) and the English word \u0026ldquo;alone,\u0026rdquo; hinting at the isolated struggle between two opponents.\nIn Abalone, two players compete on a hexagonal board with 14 marbles each: white for one player, black for the other. The aim is straightforward: be the first to push six of your opponent\u0026rsquo;s marbles off the board. But as you\u0026rsquo;ll soon discover, achieving this goal requires careful planning, tactical moves, and a deep understanding of positioning.\nGame Components and Setup The game is played on a hexagonal board with 61 circular positions arranged in rows:\nFigure 1: Board layout\nEach player starts with 14 marbles, and the initial setup (shown below) places these marbles in a specific formation, primed for strategic movement.\nFigure 2: Initial setup\nEach position on the board is labeled using a grid system with letters and numbers, allowing players to communicate moves easily.\nBasic Rules and Movements In Abalone, players take turns making a single move per turn. Here are the core rules:\nMoving a Marble: On your turn, you can move one marble to any adjacent, empty spot on the board. Line and Lateral Moves: You can also move a line of two or three marbles as long as they are aligned in the same direction. This can be done in two ways: In-Line Movement: Move all marbles in the line forward in the same direction. Lateral Movement: Shift all marbles in the line to the side without changing their orientation. Sumito (Pushing Marbles): If your marbles outnumber the adjacent marbles of your opponent in a line, you can push them. For instance, two marbles can push one, and three can push two. Pushing is only possible if there‚Äôs an empty spot behind the opposing marble(s) for them to move into. Special Positioning: \u0026ldquo;Pac\u0026rdquo; and Strategic Pushes The concept of pac is crucial for mastering Abalone. When white and black marbles are aligned in equal numbers, neither player can push the other (this creates a \u0026ldquo;pac\u0026rdquo; or standoff). This means that understanding numerical superiority and positioning is essential.\nFor example, in a scenario where three black marbles face three white marbles, no push is possible. However, a setup with three black marbles against two white marbles allows the black player to push forward. This rule creates opportunities to set up defenses and traps, making the game highly tactical.\nKey Strategies for Winning To succeed in Abalone, keep these strategies in mind:\nControl the Center: Marbles near the edges are at greater risk of being pushed out. Maintaining a central position allows flexibility and reduces the likelihood of getting cornered. Set Up Sumito Opportunities: Since pushing depends on having more marbles in a line, positioning your marbles strategically to outnumber opponents in key areas is vital. Avoid Isolation: Isolated marbles are easy targets for a Sumito. Keep your marbles grouped to maintain pushing power and defend against your opponent‚Äôs moves. Force a Pac: Sometimes, creating a standoff situation (pac) can disrupt your opponent‚Äôs plans, giving you time to reposition your marbles. Now, enjoy this 90\u0026rsquo;s commercial or play the game online while I\u0026rsquo;m writing the rest of the rules!\n","permalink":"https://ldomenichelli.github.io/games/abalone/","summary":"\u003ch2 id=\"what-is-abalone\"\u003eWhat is Abalone?\u003c/h2\u003e\n\u003cp\u003eHave you ever tried pushing your opponent off a cliff‚Ä¶ in a friendly way? If that sounds intriguing, let me introduce you to \u003cem\u003eAbalone\u003c/em\u003e, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent L√©vi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\u003c/p\u003e","title":"Abalone"},{"content":" me fr fr.\nWelcome to my study space! I\u0026rsquo;m Lucia, a first-year PhD student in AI at UniPi.\nHere, I collect personal notes on various topics I‚Äôm learning. They‚Äôre written for me, but might be helpful to others, too. Enjoy reading!\n","permalink":"https://ldomenichelli.github.io/about/","summary":"who am i?","title":"about this site"},{"content":"Achi: A Traditional African Game Achi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as Care), and others. It is classified among alignment games, which also include games like Tapatan, Tant Fant, Shisima, and Pong Hau K\u0026rsquo;i.\nObjective: The goal of Achi is to form an uninterrupted line of three pieces of the same color, placed on the same horizontal, vertical, or diagonal line. Once a player achieves this, the game ends immediately, and that player wins.\nEquipment: The game consists of:\nA board with 9 intersections (as shown in the diagram). 8 pieces: 4 white and 4 black. Pieces are placed on the intersections, not within the squares of the grid. Players: The game is played by two players, who take turns.\nRules: There are two main phases in the game: the Placement Phase and the Movement Phase.\n1. Placement Phase: During the Placement Phase, players take turns placing one piece at a time on any available intersection on the board. Pieces cannot be moved during this phase. This phase ends once all 8 pieces (4 for each player) have been placed on the board. After this phase, there will be only one empty intersection left on the board. The initial setup leaves the board empty, and either player can start. 2. Movement Phase: Once all pieces are placed, the game transitions into the Movement Phase. From the fifth move onwards, players can move their pieces. A piece may be moved to an adjacent intersection (orthogonally or diagonally), as long as the target intersection is empty. Players continue to move their pieces, aiming to align three of their pieces in a row (horizontally, vertically, or diagonally). The game ends as soon as a player forms a line of three pieces of their color, either during the Placement Phase or the Movement Phase. ","permalink":"https://ldomenichelli.github.io/games/achi/","summary":"\u003ch1 id=\"achi-a-traditional-african-game\"\u003eAchi: A Traditional African Game\u003c/h1\u003e\n\u003cp\u003eAchi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as \u003cem\u003eCare\u003c/em\u003e), and others. It is classified among alignment games, which also include games like \u003cem\u003eTapatan\u003c/em\u003e, \u003cem\u003eTant Fant\u003c/em\u003e, \u003cem\u003eShisima\u003c/em\u003e, and \u003cem\u003ePong Hau K\u0026rsquo;i\u003c/em\u003e.\u003c/p\u003e","title":"Achi"},{"content":"Adugo: The Game of the Jaguar and Dogs Adugo, also known as \u0026ldquo;Jaguar and Dogs,\u0026rdquo; is a traditional game played by the Bororo people in Brazil. This game simulates a hunting scenario and shares similarities with other strategic games from Southeast Asia.\nOverview of the Game Adugo is played on a board where the black piece represents the jaguar, and the white pieces represent the dogs. The game is an asymmetrical strategy challenge, where each side has distinct goals and abilities. How to Play Setup: The board consists of intersecting lines forming a grid. The jaguar starts on one side of the board, while the dogs are positioned on the opposite side. First Move: The jaguar always moves first. Movement Rules: Jaguar: Can move one intersection at a time, along orthogonal or diagonal lines. Dogs: Each dog moves one intersection at a time, along the same lines. Capturing: The jaguar can capture dogs by jumping over them, akin to the rules in checkers. A capture is only possible if the spot directly behind the dog is vacant. Winning the Game Jaguar\u0026rsquo;s Goal: Capture all the dogs. Dogs\u0026rsquo; Goal: Surround and trap the jaguar so it cannot move. CN6gs2sN 329681219498\n","permalink":"https://ldomenichelli.github.io/games/adugo/","summary":"\u003ch1 id=\"adugo-the-game-of-the-jaguar-and-dogs\"\u003eAdugo: The Game of the Jaguar and Dogs\u003c/h1\u003e\n\u003cp\u003eAdugo, also known as \u0026ldquo;Jaguar and Dogs,\u0026rdquo; is a traditional game played by the \u003ca href=\"https://pib.socioambiental.org/en/Povo:Bororo\"\u003eBororo\u003c/a\u003e people in Brazil. This game simulates a hunting scenario and shares similarities with other strategic games from Southeast Asia.\u003c/p\u003e\n\u003ch2 id=\"overview-of-the-game\"\u003eOverview of the Game\u003c/h2\u003e\n\u003cp\u003eAdugo is played on a board where the black piece represents the \u003cstrong\u003ejaguar\u003c/strong\u003e, and the white pieces represent the \u003cstrong\u003edogs\u003c/strong\u003e. The game is an asymmetrical strategy challenge, where each side has distinct goals and abilities.\n\n\n\n\n\n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-06b52\" hidden\u003e\n    \u003clabel for=\"zoomCheck-06b52\"\u003e\n        \u003cimg class=\"zoomCheck\" loading=\"lazy\" decoding=\"async\"\n            src=\"img/plancia.png\" alt=\"plancia\"\n             /\u003e\n    \u003c/label\u003e\n\n\u003c/p\u003e","title":"Adugo"},{"content":"Representation learning for NLP All NN architechtures build representaions of input data as vectors/embeddings, which encode useful statistical and semantic information about the data that can be used to classify or traslate something.\nThe NN learns to build beter representations by receiving a feedback, usually via error/loss function. Transformers build features of each word using an [attention mechanism]to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word\nGNN build representations of graphs GNN or GCN build representations of nodes and edges in graph data. They do so through neighbourhood aggregation (or message passing), where each node gathers features from its neightbours yo update its representations of the local graph structure arpund it. Stacking several GNN layers enables the model to propagate each node\u0026rsquo;s feature over the entire graph.\nGNN upade the hidden feature h of node i at layer l via a non-linear transformation of the node\u0026rsquo;s own feature $h_i^l$ added to the aggregation of feature $h_j^l$ from each neighboring node $j \\in N(i)$\n$$h_{i}^{\\ell+1} = \\sigma \\Big( U^{\\ell} h_{i}^{\\ell} + \\sum_{j \\in \\mathcal{N}(i)} \\left( V^{\\ell} h_{j}^{\\ell} \\right) \\Big)$$ where $U^l$ and $V^l$ are learnable weight matrices of the GNN layer and $\\sigma$ is a non-linear function such as ReLU.\nWhere does the connection with Trasformers appear? The summation over the neightbourhood nodes can be replaced by other input size-invariant functions, such as a wehighted sum via an attention mechanism. That way we would have a GAT, Graph Attention Network. Add normalization and the feed-forward MLP, and voila, we have a Graph Transformer!\n![[Pasted image 20241207183239.png]]\nWhat we have with transformers instead, is this, for an hidden feature $h$:\n$$h_{i}^{\\ell+1} = \\text{Attention} \\left( Q^{\\ell} h_{i}^{\\ell} \\ , K^{\\ell} h_{j}^{\\ell} \\ , V^{\\ell} h_{j}^{\\ell} \\right)$$ with: $$\\ h_{i}^{\\ell+1} = \\sum_{j \\in \\mathcal{S}} w_{ij} \\left( V^{\\ell} h_{j}^{\\ell} \\right)$$ $$ \\text{where} \\ w_{ij} = \\text{softmax}_j \\left ( Q^{\\ell} h^{\\ell}_i \\cdot K^{\\ell} h^{\\ell}_j \\right)$$\n![[Pasted image 20241207184230.png]]\nSentences are fully connected word graphs \u0026hellip; or not? Consider a sentence as a fully-connected graph, where each word is connected to every other word. Now, we can use a GNN to build features for each node (word) in the graph (sentence). Broadly, this is what Transformers are doing: they are GNNs with multi-head attention as the neighbourhood aggregation function.\nBefore statistical NLP and ML, linguists like Noam Chomsky focused on developing formal theories of linguistic structure, such as syntax trees/graphs. Tree LSTMs already tried this, but maybe Transformers/GNNs are better architectures for bringing together the two worlds of linguistic theory and statistical NLP?\nLong term dependencies fully-connected graphs is that they make learning very long-term dependencies between words difficult . This is simply due to how the number of edges in the graph scales quadratically with the number of nodes üò¢.\nAre Transformers learning neural syntax? There have been several notable studies exploring what Transformers might be learning, especially within the NLP community. These include insights into how attention mechanisms across all word pairs in a sentence can identify the most relevant pairs, enabling Transformers to capture something akin to a task-specific syntax. Moreover, different heads in multi-head attention appear to focus on distinct syntactic features, offering specialized perspectives on linguistic structure.\nFraming this in terms of graph theory: can the use of Graph Neural Networks (GNNs) on full graphs help us uncover the most significant edges? Specifically, can we infer their roles based on how GNNs aggregate information across neighborhoods at each layer? This approach could provide deeper insights into the connections driving model performance.\nReferences Chaitanya K. Joshi, \u0026ldquo;Transformers are Graph Neural Networks\u0026rdquo;, The Gradient, 2020. [Youtube Video] (https://www.youtube.com/watch?v=qAF3ZHmkXUw) ","permalink":"https://ldomenichelli.github.io/posts/post7/","summary":"\u003ch3 id=\"representation-learning-for-nlp\"\u003eRepresentation learning for NLP\u003c/h3\u003e\n\u003cp\u003eAll NN architechtures build \u003cem\u003erepresentaions\u003c/em\u003e of input data as vectors/embeddings, which encode useful statistical and semantic information about the data that can be used to classify or traslate something.\u003c/p\u003e\n\u003cp\u003eThe NN \u003cem\u003elearns\u003c/em\u003e to build beter representations by receiving a feedback, usually via error/loss function.\nTransformers build features of each word using an [attention mechanism]to figure out how important \u003cstrong\u003eall the other words\u003c/strong\u003e in the sentence are w.r.t. to the aforementioned word\u003c/p\u003e","title":"Geometric Deep Learning"},{"content":"Day 1 Here are my notes from the HPLT \u0026amp; NLPL Winter School taken in February 2025 in Skeikampen, NOR.\nCommon Crawl is a comprehensive open-access repository of web data, updated monthly and curated by a non-profit organization. Its principal aim is to facilitate large-scale data analysis by providing a massive corpus of HTML pages, text content, and associated metadata captured from the open internet. Researchers across various domains‚Äîfrom computational linguistics to social network analysis‚Äîleverage this resource to study the evolution of online discourse, monitor changes in hyperlink structures, and train advanced machine learning models.\nFrom a scientific perspective, Common Crawl serves as a practical embodiment of the vast, interconnected nature of the World Wide Web. By offering a parsed version of each webpage‚Äôs HTML source, it provides structural information essential for tasks like hyperlink analysis and website categorization. At the same time, the raw text captures shifts in language usage over time, enabling longitudinal studies on semantic change, sentiment, and topic modeling. Because the dataset is regularly updated, it can be used to detect emerging trends or abrupt changes‚Äîsuch as sudden spikes in online references to a new technology or event.\nResearchers and institutions often turn to Common Crawl to eliminate the need for building and maintaining their own large-scale web crawlers‚Äîa process that would be both costly and difficult to manage. The ability to directly download large data segments means that specialized methods in natural language processing can be developed and tested on a real-world corpus that reflects the genuine complexities and variety of internet text. This helps advance algorithmic research in areas like named entity recognition, topic classification, or question-answering systems. Moreover, the data is openly licensed, allowing teams of any size to collaborate, reproduce findings, and ensure transparency in methods. By pooling efforts around a shared resource, researchers can more rapidly refine existing techniques, propose new computational models, and conduct experiments that push the limits of current technology.\nCommon Crawl Factuality and Hallucinations in LLMs Second talk was about Factuality and Hallucinations and was presented by Anna Rogers, University of Cophenhagen. Here are the slides she shared:\nIt discusses the nature of \u0026ldquo;bullshit\u0026rdquo; in LLMs, comparing them with human hallucination abilities, and examines proposed solutions such as RAG (Retrieval-Augmented Generation) and CoT (Chain-of-Thought), highlighting their limitations. Finally, it explores the negative impact of LLMs on the information ecosystem, with the proliferation of spam and synthetic content, and addresses the challenges in identifying and countering these issues. RAG (Retrieval-Augmented Generation) and CoT (Chain-of-Thought) are two approaches proposed to improve the factual accuracy of LLMs, but neither fully solves the problem.\nRAG is a technique that involves providing LLMs with a database or external context from which to draw information during text generation. The idea is that by offering a reference context, the LLM will be able to produce more factual and accurate responses. The RAG process involves:\nRetrieval: Extracting relevant information from an external database. Generation: Using the retrieved information as context to generate a response. However, RAG is not a perfect solution and presents several issues:\nIt can worsen the situation if the knowledge graph is divided in such a way that test questions have no supporting evidence, causing the model to still give correct answers (5-8%) even when instructed to respond \u0026ldquo;False.\u0026rdquo; The evaluation of a RAG system\u0026rsquo;s quality is complex and depends on various criteria such as retrieval accuracy, the relevance of the generated content to the query, fidelity to sources, and correctness against underlying truth. Evaluation metrics are often calculated by another LLM (e.g., GPT-3.5-turbo), which could introduce potential biases. RAG does not guarantee that the LLM will convey information from the database faithfully. There are citation issues, as a RAG model might misquote a source, which doesn‚Äôt necessarily help the model produce better results. CoT is a prompting technique that involves providing the LLM with step-by-step reasoning examples, aiming to encourage it to follow a similar thought process during text generation. The idea is that by providing intermediate reasoning, the LLM will generate more accurate and understandable responses. The advantages of CoT include:\nIn many cases, CoT works better than standard prompting. It can improve model accuracy on certain tasks. It may reduce sensitivity to biases, but this effect varies significantly depending on the type of bias and model. However, CoT also has limitations:\nIn biased scenarios, zero-shot CoT prompting can worsen results. The \u0026ldquo;explanations\u0026rdquo; produced by CoT are not always faithful to the actual thought process of the model. CoT can be used to \u0026ldquo;jailbreak\u0026rdquo; the LLM and bypass restrictions, for example, to generate harmful content. Day 2 Firt talk of the day was by Guilherme Penedo (Hugging Face) on FineWeb2, a recent multilingual web based dataset for LLM pretraining. The talk is centered around adaptig the processing pipelines used in English to different languages. The second talk was presented by Jenia Jitsev \u0026amp; Marianna Nezhurina, interesting talk about power law and the problmes in quantifying generalization. ","permalink":"https://ldomenichelli.github.io/posts/post5/","summary":"\u003ch1 id=\"day-1\"\u003e\u003cem\u003eDay 1\u003c/em\u003e\u003c/h1\u003e\n\u003cp\u003eHere are my notes from the HPLT \u0026amp; NLPL Winter School taken in February 2025 in Skeikampen, NOR.\u003c/p\u003e\n\u003cp\u003eCommon Crawl is a comprehensive open-access repository of web data, updated monthly and curated by a non-profit organization. Its principal aim is to facilitate large-scale data analysis by providing a massive corpus of HTML pages, text content, and associated metadata captured from the open internet. Researchers across various domains‚Äîfrom computational linguistics to social network analysis‚Äîleverage this resource to study the evolution of online discourse, monitor changes in hyperlink structures, and train advanced machine learning models.\u003c/p\u003e","title":"HPLT \u0026 NLPL Winter School"},{"content":"Decomposing and Understanding Neural Computation Mechanistic interpretability seeks to ‚Äúreverse engineer‚Äù neural networks by decomposing them into human‚Äêreadable components‚Äîfeatures, circuits, and computational motifs‚Äîand mapping out their causal interactions.\n1. Decomposing the Network 1.1 Dimensionality Reduction Methods Traditional Methods:\nTechniques like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) have been used to group hidden state patterns. Although they provide a coarse overview of the representation structure, these methods often fall short in capturing the rich, nonlinear computations in modern deep networks. Recent works (e.g., Friedman et al., 2024) revisit these techniques with enhanced insights but acknowledge that more nuanced methods are necessary. 1.2 Sparse Dictionary Learning (SDL) Core Idea:\nThe superposition hypothesis posits that neural networks encode many more features than their nominal dimensions by using sparse, nearly orthogonal directions. In this framework, any hidden activation $\\mathbf{x}$ is approximated as a sparse linear combination of dictionary elements: $$ \\mathbf{x} \\approx \\sum_i c_i , \\mathbf{d}_i $$ where the encoder produces sparse coefficients $c_i$ and the decoder‚Äôs weights form the dictionary $\\mathbf{D}$\nSparse Autoencoders (SAEs):\nVariants such as Sparse Autoencoders, Transcoders, and Crosscoders have been used to extract \u0026ldquo;monosemantic\u0026rdquo; features that are far more interpretable than raw neuron activations. Recent research (e.g., Huben et al., 2024; Braun et al., 2024) demonstrates that these methods help overcome the polysemanticity of individual neurons.\nScalability and Advanced Techniques:\nAdvances like end-to-end sparse dictionary learning and Switch Sparse Autoencoders (see Gao et al., 2024) address both the reconstruction quality and computational cost. Efficient online methods and routing-based approaches reduce memory and FLOP bottlenecks, making it feasible to apply these techniques to large, frontier models.\nFeature Geometry and Superposition:\nSDL not only extracts individual features but also reveals the geometric structure of the representation space. Notably, many features appear in ‚Äúopposite pairs‚Äù (e.g., encoding positive versus negative evidence for a concept), which aids in disentangling the superposition that otherwise blurs individual neuron interpretations.\n2. From Components to Circuits 2.1 Proceduralizing Mechanistic Interpretability Task Definition \u0026amp; Decomposition:\nResearchers begin by selecting a target behavior (e.g., performing arithmetic, identifying indirect objects) and decomposing the network into a directed acyclic graph (DAG). Nodes in this graph can be architectural components (such as attention heads and MLP layers) or dictionary features.\nIdentifying Task-Relevant Subgraphs:\nTechniques such as activation patching and attribution patching help isolate which nodes or edges are critical for a behavior. For example, the ACDC algorithm (Conmy et al., 2023) successfully rediscovered known circuits in GPT-2 Small by selecting a small subset of edges from tens of thousands.\nIterative Description and Validation:\nHypotheses are formed about each component\u0026rsquo;s function (e.g., \u0026ldquo;What triggers this component\u0026rsquo;s activation?\u0026rdquo; and \u0026ldquo;How does it influence downstream computations?\u0026rdquo;). These are then validated via targeted causal interventions.\n2.2 Automated Circuit Discovery Efficiency Improvements:\nTraditional activation patching is computationally expensive. Newer methods such as Edge Attribution Patching compute gradients over all edges in one backward pass, greatly improving efficiency.\nContextual Decomposition for Transformers (CD-T):\nCD-T provides a set of equations to isolate the contribution of each node in the network. It can yield circuits at various levels‚Äîfrom fine-grained (individual attention heads at specific positions) to global subgraphs‚Äîwhile reducing discovery time from hours to seconds.\n3. Intrinsic Interpretability: Describing Functional Roles Mechanistic interpretability also aims to embed interpretability into model training:\n3.1 Understanding Component Activation Collecting High-Activation Examples:\nGathering inputs that maximally activate a component can reveal recurring themes. However, caution is required since human priors might bias interpretation.\nAttribution Methods and Feature Synthesis:\nMethods such as integrated gradients, activation patching, and synthesis of maximally activating inputs help quantify the contribution of input features. These provide first-order approximations that must be carefully validated.\n3.2 Analyzing Downstream Effects Direct Effects via Logit Lens:\nThe logit lens technique projects intermediate activations directly into the output space, offering a direct measure of a component‚Äôs contribution. Variants (e.g., tuned logit lens) can improve decoding accuracy.\nCausal Interventions:\nTechniques such as ablation, path patching, and interchange interventions allow researchers to manipulate a component‚Äôs activation and observe corresponding changes in model behavior. These methods are essential for establishing true causal relationships.\nSteering and Patchscopes:\nMore advanced methods actively ‚Äústeer‚Äù the model by forcing specific activations and then measuring the resultant behavior. This not only aids in understanding but also opens the door for model control and alignment applications.\n4. Validation and Benchmarking 4.1 Establishing Causal Relationships Multiple Validation Methods:\nCombining causal interventions with predictions about downstream behavior is crucial. If a hypothesized circuit is causal, interventions should predictably alter the model‚Äôs output.\nTestbeds with Known Ground Truth:\nSome studies embed simple algorithms into network weights, allowing the recovery of known structures as a means of validating interpretability methods.\n4.2 Automated Benchmarks Evaluation Metrics:\nAutomated benchmarks‚Äîsuch as those reporting ROC AUC scores for circuit recovery‚Äîare essential for comparing different interpretability methods. Recent work shows that methods like CD-T can achieve high fidelity (e.g., 97% ROC AUC) while maintaining low runtime. 5. Broader Implications and Future Directions 5.1 AI Safety and Alignment Understanding and Controlling Model Behavior:\nBy uncovering causal circuits that lead to harmful or biased outputs, mechanistic interpretability offers a pathway to targeted interventions (e.g., model editing or fine-tuning) for safer AI.\nReal-Time Monitoring:\nWhite-box evaluation methods that rely on internal activations promise the potential for real-time monitoring and anomaly detection, providing early warnings of unsafe behavior.\n5.2 Theoretical and Scalability Challenges Robust Theoretical Foundations:\nDespite the practical success of techniques like SDL, many methods still lack rigorous theoretical guarantees. Bridging the gap between heuristic approaches and formal theory is critical.\nScalability to Large Models:\nAs models become larger and more complex, ensuring that interpretability methods scale without sacrificing accuracy is a major research frontier. Techniques such as online dictionary learning and modular training (e.g., Brain-Inspired Modular Training) are promising directions.\nMitigating Interpretability Illusions:\nIt is vital to avoid ‚Äúcherry-picking‚Äù results that seem intuitive but may not generalize. Rigorous validation methods and comprehensive benchmarks are needed to confirm that discovered circuits reflect true causal mechanisms.\n5.3 Future Research Directions Extending to Multimodal and RL Models:\nFuture work should extend mechanistic interpretability techniques beyond language models to include vision, multimodal systems, and reinforcement learning agents.\nIntrinsic Interpretability by Design:\nDeveloping model architectures that are inherently modular, sparse, and interpretable will make them easier to reverse engineer from the start.\nAutomated End-to-End Analysis:\nContinued progress in automated circuit discovery (e.g., using CD-T or Edge Attribution Patching) will reduce human labor and increase the reliability of interpretations.\nReferences and Further Reading Automated Circuit Discovery:\nConmy et al., \u0026ldquo;Towards Automated Circuit Discovery for Mechanistic Interpretability\u0026rdquo; (arXiv:2304.14997) Hsu et al., \u0026ldquo;Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition\u0026rdquo; (arXiv:2407.00886) Sparse Autoencoders and Dictionary Learning:\nHuben et al., \u0026ldquo;Sparse Autoencoders Find Highly Interpretable Features in Language Models\u0026rdquo; (ICLR 2024 poster) Braun et al., \u0026ldquo;Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning\u0026rdquo; (arXiv:2405.12241) Mechanistic Interpretability Reviews and AI Safety:\nBereska \u0026amp; Gavves, \u0026ldquo;Mechanistic Interpretability for AI Safety ‚Äî A Review\u0026rdquo; (TMLR 2024) Chris Olah‚Äôs work on mechanistic interpretability at Anthropic, as featured in TIME (Sep 2024) ","permalink":"https://ldomenichelli.github.io/posts/post6/","summary":"\u003ch1 id=\"decomposing-and-understanding-neural-computation\"\u003eDecomposing and Understanding Neural Computation\u003c/h1\u003e\n\u003cp\u003eMechanistic interpretability seeks to ‚Äúreverse engineer‚Äù neural networks by decomposing them into human‚Äêreadable components‚Äîfeatures, circuits, and computational motifs‚Äîand mapping out their causal interactions.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-decomposing-the-network\"\u003e1. Decomposing the Network\u003c/h2\u003e\n\u003ch3 id=\"11-dimensionality-reduction-methods\"\u003e1.1 Dimensionality Reduction Methods\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTraditional Methods:\u003c/strong\u003e\u003cbr\u003e\nTechniques like Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) have been used to group hidden state patterns. Although they provide a coarse overview of the representation structure, these methods often fall short in capturing the rich, nonlinear computations in modern deep networks. Recent works (e.g., Friedman et al., 2024) revisit these techniques with enhanced insights but acknowledge that more nuanced methods are necessary.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"12-sparse-dictionary-learning-sdl\"\u003e1.2 Sparse Dictionary Learning (SDL)\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eCore Idea:\u003c/strong\u003e\u003cbr\u003e\nThe \u003cstrong\u003esuperposition hypothesis\u003c/strong\u003e posits that neural networks encode many more features than their nominal dimensions by using sparse, nearly orthogonal directions. In this framework, any hidden activation $\\mathbf{x}$ is approximated as a sparse linear combination of dictionary elements:\n$$\n\\mathbf{x} \\approx \\sum_i c_i , \\mathbf{d}_i\n$$\nwhere the encoder produces sparse coefficients $c_i$  and the decoder‚Äôs weights form the dictionary $\\mathbf{D}$\u003c/p\u003e","title":"Notes on Mechanistic Interpretability"},{"content":"To qualify as a distance, a measure must satisfy the following properties:\nSymmetry: $ d(P, Q) = d(Q, P) )$ Triangle inequality: $ d(P, Q) + d(Q, R) \\geq d(P, R) $ However, in practice, we often deal with weaker notions of distances, commonly referred to as divergences.\nExample: KL Divergence The Kullback-Leibler (KL) divergence is defined as: $$ D_{\\text{KL}}(P || Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx $$\nProperties of KL Divergence Not Symmetric: $$ D_{\\text{KL}}(P || Q) \\neq D_{\\text{KL}}(Q || P) $$ Infinite for Different Supports: $$ D_{\\text{KL}}(P || Q) \\to \\infty \\quad \\text{if } P \\text{ and } Q \\text{ have different supports.} $$\nAddressing These Challenges Solution 1: Smoothing Distributions To avoid the issue of different supports, one solution is to smooth the distributions to match their supports.\nSolution 2: Use a Different Divergence An alternative approach is to use a divergence that naturally handles different supports and adheres to desirable distance properties. One such measure is the Wasserstein Distance.\nWasserstein Distance The Wasserstein distance, rooted in optimal transport theory, addresses the shortcomings of KL divergence by offering:\nSymmetry Triangle inequality A meaningful geometry of the space of distributions. Intuition Behind Optimal Transport The Wasserstein distance can be understood as the minimum \u0026ldquo;cost\u0026rdquo; to transform one distribution into another. Imagine redistributing the \u0026ldquo;mass\u0026rdquo; of one distribution $P$ to match another distribution $Q$. Each unit of mass has a transportation cost proportional to the distance it is moved.\nFormally, the $p$-Wasserstein distance is defined as:\n$$ W_p(P, Q) = \\left( \\inf_{\\gamma \\in \\Pi(P, Q)} \\int | x - y |^p d\\gamma(x, y) \\right)^{1/p} $$ Here:\n$ \\Pi(P, Q) $: The set of all couplings (joint distributions) with marginals $P$ and $Q$. $ | x - y | $: The cost of moving mass from $x$ to $y$. Properties of Wasserstein Distance Captures the geometric relationship between distributions. Finite even for distributions with disjoint supports. Offers meaningful insights in contexts like generative modeling and comparing empirical distributions. Optimal transport and the Wasserstein distance are widely used in fields such as:\nMachine Learning: Generative models (e.g., GANs with Wasserstein loss). Economics: Resource allocation problems. Physics: Modeling fluid dynamics. Image Processing: Comparing distributions of pixel intensities. By leveraging the principles of optimal transport, we gain a robust and versatile framework for comparing and transforming probability distributions.\nMore in written notes: ","permalink":"https://ldomenichelli.github.io/posts/post2/","summary":"\u003cp\u003eTo qualify as a \u003cstrong\u003edistance\u003c/strong\u003e, a measure must satisfy the following properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSymmetry\u003c/strong\u003e: $ d(P, Q) = d(Q, P) )$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTriangle inequality\u003c/strong\u003e: $ d(P, Q) + d(Q, R) \\geq d(P, R) $\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHowever, in practice, we often deal with \u003cstrong\u003eweaker notions of distances\u003c/strong\u003e, commonly referred to as \u003cstrong\u003edivergences\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"example-kl-divergence\"\u003eExample: KL Divergence\u003c/h3\u003e\n\u003cp\u003eThe Kullback-Leibler (KL) divergence is defined as:\n$$\nD_{\\text{KL}}(P || Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx\n$$\u003c/p\u003e\n\u003ch4 id=\"properties-of-kl-divergence\"\u003eProperties of KL Divergence\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eNot Symmetric\u003c/strong\u003e:\n$$\nD_{\\text{KL}}(P || Q) \\neq D_{\\text{KL}}(Q || P)\n$$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInfinite for Different Supports\u003c/strong\u003e:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e$$\nD_{\\text{KL}}(P || Q) \\to \\infty \\quad \\text{if } P \\text{ and } Q \\text{ have different supports.}\n$$\u003c/p\u003e","title":"Optimal Transport üï∑Ô∏è and Wasserstein distance"},{"content":"L‚Äôuomo beve il T√® perch√© lo angoscia l‚Äôuomo. Il T√® beve l‚Äôuomo, l‚Äôerba pi√π amara.\n-Guido Ceronetti\nTea transcends being just a beverage‚Äîit\u0026rsquo;s a ritual, a tradition, and a bridge between nature and culture. With over 3,000 varieties, tea offers an incredible diversity of flavors, aromas, and benefits. Despite its vastness, every true tea originates from one remarkable plant: Camellia sinensis. The wide variety arises from how the leaves are processed, combined, or infused. This guide delves into the unique categories of true teas, mixed teas, and herbal tisanes, as well as the distinctions between Japanese and Chinese tea traditions. Steps to produce tea (stolen from infograph)\nTrue Teas: the Classics True teas are made solely from the Camellia sinensis plant. What sets each type apart is the processing technique, which influences the flavor, color, and aroma. Below is a detailed comparison of the six main types of true teas:\nTea Type Processing Highlights Flavor Profile Health Benefits Popular Varieties White Tea Minimally processed; only withering and drying. Light, floral, and mildly sweet. High in antioxidants, supports skin and immunity. Silver Needle, Bai Mudan. Yellow Tea Gently heated, slow oxidation wrapped in cloth. Smooth, sweet, with honey-like notes. Rare; aids digestion, boosts focus. Huo Shan Huang Ya. Green Tea Oxidation halted early using steaming or pan-firing. Grassy, fresh, occasionally nutty. Boosts metabolism, brain health, and heart health. Sencha, Matcha, Dragon Well. Oolong Tea Partially oxidized; rolled repeatedly for complexity. Floral, fruity, and nutty. Reduces cholesterol, aids digestion. Tieguanyin, Da Hong Pao. Black Tea Fully oxidized for a robust flavor and dark color. Strong, malty, and bold. Improves energy, supports heart health. Assam, Darjeeling, Keemun. Pu-erh Tea Fermented and aged, often for years. Earthy, rich, and woody. Aids digestion, lowers cholesterol, boosts gut health. Sheng Pu-erh, Shou Pu-erh. Green Tea ü´ñ Green tea is celebrated for its vibrant flavor, delicate processing, and scientifically-backed health benefits. Unlike black or oolong teas, green tea undergoes minimal oxidation. Processing typically involves steaming (common in Japanese teas) or pan-firing (typical in Chinese teas) to halt oxidation and preserve natural antioxidants like catechins. This careful crafting ensures green tea retains its signature grassy, vegetal, or nutty character.\nGreen Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Sencha Japan Steamed; preserves chlorophyll and freshness Fresh, mildly sweet, slightly grassy Boosts metabolism, enhances energy, and supports cardiovascular health. Matcha Japan Shade-grown; ground into a fine powder Creamy, vegetal, umami-rich High in antioxidants; promotes focus, detoxification, and relaxation. Dragon Well China Pan-fired; leaves flattened into iconic shape Nutty, smooth, subtly sweet Encourages relaxation, supports heart health, and aids digestion. Gunpowder Green China Leaves rolled into small, tight pellets Bold, slightly smoky, robust Improves energy, promotes digestion, and has antioxidative effects. Jasmine Green China Scented with jasmine blossoms Floral, sweet, aromatic Calms the mind, reduces stress, and supports skin health with antioxidant-rich polyphenols. Oolong Tea üåø Oolong tea occupies a unique position between green and black teas, combining the fresh, floral notes of the former with the robust, complex flavors of the latter. This partially oxidized tea is prized for its intricate processing, which involves repeated rolling, shaping, and drying of the leaves. The oxidation level of oolong tea can range from 10% to 80%, creating a diverse spectrum of flavors and aromas.\nThe careful crafting of oolong emphasizes its layered profile. Rolling and firing the leaves multiple times during production intensifies the complexity, giving oolong its characteristic floral, fruity, and nutty notes. Different regions and methods yield distinct types, offering a broad range of sensory experiences.\nOolong Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Tieguanyin China (Anxi, Fujian) Lightly oxidized; tightly rolled, jade-green leaves Floral, creamy, sweet Boosts skin health, supports digestion, and reduces stress. Da Hong Pao China (Wuyi Mountains) Heavily oxidized; roasted for depth Roasted, woody, slightly mineral Promotes heart health, improves energy, and lowers cholesterol. Oriental Beauty Taiwan Naturally oxidized by leafhoppers; less rolled Fruity, honey-like, mellow Rich in antioxidants; supports metabolism and enhances relaxation. Milk Oolong Taiwan Lightly oxidized; steamed for creaminess Buttery, smooth, subtly floral Aids in hydration, improves focus, and provides a soothing experience. Phoenix Dan Cong China (Guangdong) Medium oxidized; leaves twisted into long shapes Fruity, floral, and aromatic Supports gut health, aids weight management, and calms the nervous system. Oolong tea\u0026rsquo;s allure lies in its balance‚Äîa harmony between freshness and depth, floral lightness and roasted warmth. It invites tea enthusiasts to explore its range, from the creamy smoothness of Milk Oolong to the bold richness of Da Hong Pao. Whether sipped for relaxation or paired with food, oolong tea is a testament to the artistry and science of tea-making.\nBlack Tea Black tea is the most oxidized of all true teas, resulting in its signature dark color and robust flavor. During production, the leaves are fully oxidized after being withered and rolled, a process that intensifies their malty, brisk, and sometimes sweet notes. This oxidation also enhances the development of theaflavins and thearubigins, compounds responsible for black tea\u0026rsquo;s characteristic taste and many of its health benefits.\nWith its bold profile and high caffeine content compared to green or white tea, black tea has become a staple in cultures worldwide, whether as a standalone beverage or as a base for blends like chai or Earl Grey.\nBlack Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Assam India (Assam Valley) Fully oxidized; rolled for even processing Strong, malty, brisk Boosts energy, supports cardiovascular health, and improves focus. Darjeeling India (Darjeeling) Lightly oxidized compared to other black teas Floral, muscatel, slightly astringent Rich in antioxidants; aids digestion and supports immune health. Keemun China (Anhui Province) Slowly oxidized; carefully dried Smooth, smoky, slightly sweet Reduces stress, promotes relaxation, and enhances heart health. Ceylon Sri Lanka Fully oxidized; grown at varying altitudes Bold, citrusy, and brisk Improves digestion, boosts energy, and supports metabolism. Lapsang Souchong China (Fujian Province) Smoked over pinewood fires Smoky, rich, and earthy Provides warmth, reduces inflammation, and promotes relaxation. Black tea represents strength, both in flavor and character. Its ability to harmonize with other ingredients while standing strong on its own makes it a versatile and enduring favorite. From the malty richness of Assam to the smoky intrigue of Lapsang Souchong, black tea offers a bold sensory experience steeped in tradition and global significance.\nWhite Tea: The Purest Brew White tea is the least processed of all true teas, known for its delicate flavor and light, airy characteristics. Harvested primarily as young buds and leaves, it undergoes minimal oxidation, with processing typically limited to gentle withering and drying. This careful handling allows white tea to retain a high concentration of polyphenols, particularly catechins, and its characteristic light, floral aroma.\nOften regarded as the most natural tea, white tea is celebrated for its subtlety and nuanced sweetness. It embodies simplicity, offering a refreshing and soothing experience that has been cherished for centuries.\nWhite Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Silver Needle (Bai Hao Yinzhen) China (Fujian Province) Handpicked young buds; minimally processed Light, sweet, floral High in antioxidants; supports skin health and reduces oxidative stress. White Peony (Bai Mudan) China (Fujian Province) Young buds with some leaves; sun-dried Fruity, floral, slightly robust Aids in relaxation, supports immune function, and boosts heart health. Shou Mei China (Fujian or Guangxi) Older leaves; naturally withered and dried Earthy, nutty, and full-bodied Promotes digestion, supports metabolism, and improves hydration. Darjeeling White India (Darjeeling) Lightly processed from young Darjeeling leaves Delicate, floral, with muscatel notes Enhances focus, reduces inflammation, and provides gentle energy. White tea is a testament to the beauty of simplicity. Its light, soothing nature makes it a perfect choice for moments of calm and introspection. Whether you savor the delicate sweetness of Silver Needle or the slightly robust notes of White Peony, white tea offers an unparalleled experience that bridges tradition and wellness.\nMixed Teas Mixed teas combine the foundation of true teas with additional ingredients, resulting in endless flavor possibilities. For example, Earl Grey is a black tea infused with bergamot oil, creating a citrusy aroma that has become a British staple. Meanwhile, Masala Chai, a spiced blend of black tea with cinnamon, cloves, and ginger, offers a warming, aromatic treat deeply rooted in Indian culture.\nMixed Tea Base Unique Ingredients Flavor Notes Earl Grey Black tea Bergamot oil Citrusy and floral. Masala Chai Black tea Spices: cinnamon, cardamom, cloves, ginger, milk Spicy, rich, and warming. Jasmine Tea Green or black Jasmine blossoms Lightly floral and sweet. Thai Iced Tea Black tea Sweetened condensed milk Sweet, creamy, and refreshing. Mint Tea Green tea Fresh mint leaves Cooling and refreshing. Lychee Tea Black tea Lychee fruit essence Tropical, fruity, and sweet. These blends highlight how tea can be endlessly customized, whether for cultural rituals or personal enjoyment.\nHerbal Tisanes: Beyond Camellia Sinensis Herbal tisanes are caffeine-free infusions made from flowers, fruits, herbs, or spices. Though not technically \u0026ldquo;tea,\u0026rdquo; they provide a world of flavors and wellness benefits. For example, Chamomile is renowned for its calming properties, making it a popular bedtime drink. Similarly, Hibiscus offers a tart, cranberry-like taste packed with vitamin C.\nHerbal Tisane Main Ingredient Flavor Profile Health Benefits Mate Yerba mate leaves Smoky and earthy. Boosts energy and focus naturally. Rooibos Rooibos plant (South Africa) Sweet and nutty. Rich in antioxidants, aids relaxation. Chamomile Chamomile flowers Light and floral. Promotes sleep, reduces anxiety. Hibiscus Hibiscus petals Tart and cranberry-like. Supports heart health, boosts immunity. Lemongrass Lemongrass stalks Citrusy and refreshing. Aids digestion, reduces inflammation. Japanese vs. Chinese Tea üë≤üèº While Japan and China share a long history of tea cultivation, their approaches highlight distinct cultural philosophies.\nTea traditions in Japan and China are deeply intertwined with their histories and cultural values, but their approaches to tea production and consumption reflect vastly different philosophies. While both nations share a reverence for tea, their practices diverge in ways that make each tradition distinct and uniquely beautiful.\nIn Japan, tea culture revolves almost entirely around green tea, celebrated for its fresh, grassy flavors. Japanese tea processing prioritizes a steaming method, which halts oxidation and preserves the vibrant green color of the leaves. This results in teas with clean, vegetal profiles and an umami richness. Matcha, a powdered green tea, stands at the heart of Japan\u0026rsquo;s iconic tea ceremony, where every gesture reflects mindfulness and harmony. Similarly, other green teas like sencha and gyokuro reflect Japan\u0026rsquo;s emphasis on simplicity and precision. Japanese tea farms are meticulously managed, often employing shading techniques that enhance sweetness and umami in the leaves. Modern Japan has also embraced convenience, with bottled green tea and matcha-flavored products widely available, ensuring tea remains part of everyday life.\nIn contrast, Chinese tea culture is vast and varied, encompassing green, white, oolong, black, and Pu-erh teas, each with its own regional specialties and processing techniques. Unlike Japan‚Äôs steaming process, Chinese teas are often pan-fried or baked, creating nutty, toasty, and floral flavors. For example, Dragon Well (Longjing) green tea has a smooth, roasted nuttiness, while oolong teas like Tieguanyin showcase intricate floral aromas. China\u0026rsquo;s tea production takes full advantage of its diverse geography, with each region contributing distinct flavors influenced by local soil and climate. Whether it‚Äôs the earthy complexity of Pu-erh from Yunnan or the refined elegance of Keemun black tea from Anhui, Chinese teas reflect the terroir of their origins.\nCulturally, Japanese tea is rooted in Zen philosophy, focusing on the meditative aspects of preparation and drinking. The Japanese tea ceremony, or chanoyu, emphasizes simplicity, quietness, and the spiritual connection between host and guest. In contrast, Chinese tea practices celebrate variety and experimentation. The gongfu tea ceremony, often performed with Yixing clay teapots or a gaiwan, focuses on extracting the perfect flavor through multiple infusions. Chinese tea culture encourages savoring the changing notes of the tea with each steeping, turning every session into a sensory exploration.\nEven the flavors differ fundamentally between the two traditions. Japanese teas tend to be grassy, umami-rich, and vegetal, with a focus on freshness. Matcha‚Äôs creamy, bittersweet profile embodies this characteristic perfectly, as does the clean, savory taste of gyokuro. On the other hand, Chinese teas span a broader spectrum, from the delicate sweetness of white teas to the smoky, earthy richness of Pu-erh. This diversity makes Chinese tea a journey of discovery, where each cup offers a new story.\nWhile Japan modernizes its tea industry with bottled teas and matcha lattes, China retains its traditional focus on loose-leaf teas and tea houses, where time slows down for the appreciation of aroma, texture, and flavor. Both cultures, however, uphold tea as a reflection of nature, craftsmanship, and human connection, reminding us that tea is far more than a drink‚Äîit is an experience.\nUltimately, Japanese and Chinese teas reflect their respective cultures‚Äô approaches to life: Japan‚Äôs emphasis on precision and purity contrasts beautifully with China‚Äôs celebration of diversity and depth. Whether you prefer the grassy umami of Japanese green tea or the complex, evolving flavors of Chinese oolong, both traditions invite you to explore the art of tea in your own way.\nAspect Japanese Tea Chinese Tea Primary Type Green tea (e.g., Matcha, Sencha) Green, white, oolong, black, Pu-erh Processing Steaming (preserves grassy notes) Pan-frying or baking (toasty, nutty notes) Cultural Focus Zen-inspired simplicity, mindfulness Variety and exploration of flavors Popular Ceremony Matcha-based tea ceremony (chanoyu) Gongfu ceremony (multiple infusions) Flavor Notes Grassy, umami-rich, fresh Broad range: floral, earthy, fruity ","permalink":"https://ldomenichelli.github.io/random/tea/","summary":"\u003cp\u003e\u003cem\u003eL‚Äôuomo beve il T√® perch√© lo angoscia l‚Äôuomo.\nIl T√® beve l‚Äôuomo, l‚Äôerba pi√π amara.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e-Guido Ceronetti\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eTea transcends being just a beverage‚Äîit\u0026rsquo;s a ritual, a tradition, and a bridge between nature and culture. With over \u003cstrong\u003e3,000 varieties\u003c/strong\u003e, tea offers an incredible diversity of flavors, aromas, and benefits. Despite its vastness, every true tea originates from one remarkable plant: \u003cstrong\u003eCamellia sinensis\u003c/strong\u003e. The wide variety arises from how the leaves are processed, combined, or infused. This guide delves into the unique categories of \u003cstrong\u003etrue teas\u003c/strong\u003e, \u003cstrong\u003emixed teas\u003c/strong\u003e, and \u003cstrong\u003eherbal tisanes\u003c/strong\u003e, as well as the distinctions between Japanese and Chinese tea traditions.\n\n\n\n\n\n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-a3854\" hidden\u003e\n    \u003clabel for=\"zoomCheck-a3854\"\u003e\n        \u003cimg class=\"zoomCheck\" loading=\"lazy\" decoding=\"async\"\n            src=\"img/step.png\" alt=\"tea\"\n             /\u003e\n    \u003c/label\u003e\n\n\n\u003cem\u003eSteps to produce tea (stolen from infograph)\u003c/em\u003e\u003c/p\u003e","title":"Pensieri del t√® üçµ"},{"content":"Simplicity Bias in Neural Networks Neural networks (NNs) exhibit a fascinating property often referred to as simplicity bias. As described by Chizat and Bach in their work:\n\u0026ldquo;NNs are fundamentally Bayesian: an NN is biased, at initialization, towards simple functions.\u0026rdquo;\n(Chizat \u0026amp; Bach)\nThis idea sheds light on why NNs tend to generalize well despite their enormous capacity to memorize complex datasets. Understanding Simplicity Bias The Setup Fix a Neural Network:\nLet $ \\Theta$ denote the space of its parameters, where each point $ \\theta \\in \\Theta$ represents a specific configuration of weights and biases of the network.\nFunctions Represented by the Network:\nThe neural network represents a mapping: $$ F_\\Theta : \\Theta \\to \\mathcal{F} $$ where $ \\mathcal{F}$ is the space of functions that the network can learn.\nA Specific Function $F$:\nAny $ F \\in \\mathcal{F}$ is a specific function the model can realize for a given choice of $\\theta$.\nComplexity of a Function:\nThe complexity $ \\mathcal{C}(F)$ measures how \u0026ldquo;simple\u0026rdquo; or \u0026ldquo;complex\u0026rdquo; the function $F$ is. Formally:\n$$ \\mathcal{C}(F) : \\mathcal{F} \\to [0, +\\infty) $$\nLinking Parameters to Simplicity The simplicity bias emerges from the connection between the parameter space $Theta$ and the complexity of functions $ \\mathcal{C}(F) $. Specifically:\nProbability Distribution on $\\Theta $:\nLet $P$ be a probability measure on $ \\Theta$, which describes the initialization randomness of the network.\nRandom Sampling of Functions:\nUsing $P$, we can randomly pick parameters $ \\theta$ and obtain corresponding functions $ F(\\theta)$. This makes $ F$ a random variable.\nSimplicity Bias as a \u0026ldquo;Pointwise\u0026rdquo; Statement The simplicity bias can now be formally expressed as: $$ \\mathcal{C}(F) \\leq \\mathcal{C}(G) \\implies P(F = f) \u0026gt; P(G = g) $$ This indicates that the neural network is more likely to represent simpler functions under random initialization.\nWritten notes Notes for papers:\nNeural Networks Learn Statistics of Increasing Complexity (Belrose, Pope, Quirke, Mallen) -A distributional simplicity bias in the learning dynamics of transformers(Rende, Gerace, Laio, Goldt) ","permalink":"https://ldomenichelli.github.io/posts/post3/","summary":"\u003ch1 id=\"simplicity-bias-in-neural-networks\"\u003eSimplicity Bias in Neural Networks\u003c/h1\u003e\n\u003cp\u003eNeural networks (NNs) exhibit a fascinating property often referred to as \u003cstrong\u003esimplicity bias\u003c/strong\u003e. As described by Chizat and Bach in their work:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;NNs are fundamentally Bayesian: an NN is biased, at initialization, towards simple functions.\u0026rdquo;\u003cbr\u003e\n\u003cem\u003e(Chizat \u0026amp; Bach)\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2 id=\"this-idea-sheds-light-on-why-nns-tend-to-generalize-well-despite-their-enormous-capacity-to-memorize-complex-datasets\"\u003eThis idea sheds light on why NNs tend to generalize well despite their enormous capacity to memorize complex datasets.\u003c/h2\u003e\n\u003ch2 id=\"understanding-simplicity-bias\"\u003eUnderstanding Simplicity Bias\u003c/h2\u003e\n\u003ch3 id=\"the-setup\"\u003eThe Setup\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eFix a Neural Network:\u003c/strong\u003e\u003cbr\u003e\nLet $ \\Theta$ denote the space of its parameters, where each point $ \\theta \\in \\Theta$ represents a specific configuration of weights and biases of the network.\u003c/p\u003e","title":"Simplicity Bias"},{"content":"Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)‚Äîthe number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\nThe Role of Dimensionality Reduction Dimensionality reduction has gained prominence in machine learning and data science for its ability to simplify complex datasets while preserving essential structure. Methods in this domain are broadly categorized into:\nLinear Techniques:\nPrincipal Component Analysis (PCA) and its variants (e.g., Bouveyron et al. 2011) focus on orthogonal transformations to project data onto a lower-dimensional space. Non-linear Techniques (Manifold Learning):\nThese methods include Isometric Mapping (Tenenbaum et al. 2000), Locally Linear Embedding (Roweis and Saul 2000), and Hessian Eigenmapping (Donoho and Grimes 2003). They aim to capture the underlying structure of data that lies on a non-linear manifold. While most methods require users to specify a target dimension, some techniques adaptively infer it based on the dataset\u0026rsquo;s intrinsic dimensionality. This adaptability underscores the significance of ID estimation. Models and Methods for Estimating Intrinsic Dimensionality Over the years, researchers have developed diverse models to estimate intrinsic dimensionality. These fall into several categories:\nTopological Approaches: Analyze the tangent space of the manifold using local samples (e.g., Fukunaga and Olsen 1971; Verveer and Duin 1995).\nFractal Measures: Use metrics like the Correlation Dimension (Faloutsos and Kamel 1994) to estimate ID based on space-filling properties of data.\nGraph-based Methods: Employ $k$-nearest neighbors and density metrics to infer ID (Costa and Hero 2004).\nParametric Models: Leverage statistical models to estimate ID, such as those by Levina and Bickel (2004).\nGlobal vs. Local Intrinsic Dimensionality Intrinsic dimensionality measures can be broadly classified into global and local approaches:\nGlobal Measures: Analyze the dataset as a whole, treating all objects uniformly. These measures are well-suited for homogeneous datasets with a single dominant manifold.\nLocal Measures: Focus on the $k$-nearest neighbors of a specific point. These methods are essential for heterogeneous datasets comprising multiple, overlapping manifolds. Notable local ID models include:\nExpansion Dimension (ED) (Karger and Ruhl 2002). Generalized Expansion Dimension (GED) (Houle et al. 2012). Local Intrinsic Dimensionality (LID) (Houle 2013). Local ID measures are particularly relevant in applications such as similarity search, where they can estimate query complexity or optimize search termination. They are also applied in outlier detection and density estimation.\nBalancing Local and Global Insights Machine learning techniques often face challenges like overfitting when relying heavily on local information. To mitigate this, methods such as Local Tangent Space Alignment (LTSA) (Zhang and Zha 2004) combine local and global perspectives by aligning neighborhoods of points while penalizing overfitting during optimization. This balance enables a more comprehensive understanding of the data structure.\nContinuous Intrinsic Dimension In this section, we explore Local Intrinsic Dimensionality (LID), a model that extends intrinsic dimensionality to continuous distributions of distances, as proposed by Houle (2013). LID quantifies the local intrinsic dimensionality (ID) of a feature space by focusing exclusively on the distribution of inter-point distances.\nDefining the Distribution of Distances Let $(\\mathbb{R}^m, \\text{dist})$ represent a domain equipped with a non-negative distance function dist. Consider the distribution of distances with respect to a fixed reference point. This distribution can be modeled as a random variable $\\mathbf{X}$, with support $[0, \\infty)$. The probability density function (PDF) of $\\mathbf{X}$ is denoted by $f_{\\mathbf{X}}$, where $f_{\\mathbf{X}}$ is a non-negative, Lebesgue-integrable function. For any $a, b \\in [0, \\infty)$ such that $a \\leq b$, the probability is given by:\n$$ \\operatorname{Pr}[a \\leq \\mathbf{X} \\leq b] = \\int_a^b f_{\\mathbf{X}}(x) , \\mathrm{d}x. $$\nThe corresponding cumulative density function (CDF) $F_{\\mathbf{X}}$ is defined as:\n$$ F_{\\mathbf{X}}(x) = \\operatorname{Pr}[\\mathbf{X} \\leq x] = \\int_0^x f_{\\mathbf{X}}(u) , \\mathrm{d}u. $$\nFor values where $\\mathbf{X}$ is absolutely continuous at $x$, the CDF $F_{\\mathbf{X}}$ is differentiable at $x$, and its first-order derivative is $f_{\\mathbf{X}}(x)$.\nThe Local Continuous Intrinsic Dimension The local intrinsic dimension at distance $x$ is defined as follows:\nDefinition 1 (Houle, 2013):\nGiven an absolutely continuous random distance variable $\\mathbf{X}$, for any distance threshold $x$ such that $F_{\\mathbf{X}}(x) \u0026gt; 0$, the local continuous intrinsic dimension $\\mathrm{ID}_{\\mathbf{X}}(x)$ of $\\mathbf{X}$ at distance $x$ is:\n$$ \\mathrm{ID}{\\mathbf{X}}(x) \\triangleq \\lim{\\epsilon \\to 0^+} \\frac{\\ln F_{\\mathbf{X}}((1+\\epsilon) x) - \\ln F_{\\mathbf{X}}(x)}{\\ln (1+\\epsilon)} $$\nwherever the limit exists.\nRelation to the Generalized Expansion Dimension The LID definition builds upon the generalized expansion dimension (GED) proposed by Houle et al. (2012a). GED measures dimensionality by comparing neighborhood radii $x$ and $(1+\\epsilon)x$, replacing neighborhood cardinalities with the expected number of neighbors.\nIn essence, the LID formulation quantifies the discriminative power of a distance measure. Both LID and GED share the same closed-form representation, reflecting their foundational equivalence in characterizing local dimensionality.\nTheorem 1 (Houle 2013) Let X be an absolutely continuous random distance variable. If $F_X$ is both positive and differentiable at x, then:\n$$ \\text{ID}_X(x) = \\frac{x f_X(x)}{F_X(x)}. $$\nLocal intrinsic dimensionality (Local ID) has potential for wide application thanks to its very general treatment of distances as a continuous random variable. Direct estimation of $ID_X(x)$, however, requires the knowledge of the distribution of $X$. Extreme value theory, which we survey in the following section, allows the estimation of the limit of $x \\to 0$ without any explicit assumptions of the data distribution other than continuity.\n3. Extreme Value Theory Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes.\nDefinition 2 Let $\\mu \\in R$ and $\\sigma \u0026gt; 0$. The family of generalized extreme value distributions $F_{GEV}$ covers distributions whose cumulative distribution functions have the form:\n$$ F_{GEV} = \\exp \\left( - \\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-\\frac{1}{\\xi}} \\right) \u0026amp; \\text{if } \\xi \\neq 0, \\ \\exp \\left( -\\exp \\left( -\\frac{x - \\mu}{\\sigma} \\right) \\right) \u0026amp; \\text{if } \\xi = 0. $$\nA distribution $G \\in F_{GEV}$ has support:\n$$ \\text{supp}(G) = \\begin{cases} [\\mu - \\frac{\\sigma}{\\xi}, \\infty) \u0026amp; \\text{when } \\xi \u0026gt; 0, \\ (-\\infty, \\mu - \\frac{\\sigma}{\\xi}] \u0026amp; \\text{when } \\xi \u0026lt; 0, \\ (-\\infty, \\infty) \u0026amp; \\text{if } \\xi = 0. \\end{cases} $$\nIts best-known theorem, attributed in parts to Fisher and Tippett (1928), and Gnedenko (1943), states that the maximum of n independent identically-distributed random variables (after proper renormalization) converges in distribution to a generalized extreme value distribution as $n \\to \\infty$ .\nTheorem 2 (Fisher-Tippett-Gnedenko)\nLet $(X_i)_{i \\in N}$ be a sequence of independent identically-distributed random variables and let $M_n=max X_i$ If there exist a sequence of positive constants $a_n$, $n \\in N$ and a sequence of constants $b_n$ , $n \\in N$ , such that:\n$$ \\lim_{n \\to \\infty} P\\left( \\frac{M_n - b_n}{a_n} \\leq x \\right) = F(x), $$\nthen F(x) belongs to the generalized extreme value family.\nIsotropy A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportion al to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\nOther measures Avg cosine similarity : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution. Partition Isotropy score : Proposed by Arora et al. $$Z(c):= \\sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$ Intrinsic Dimensionality : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy. Linear dimensionality estimate: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions. *Non linear dimensionality estimate: - [MLE] \u0026ndash;\u0026gt; Levina2005\n- [Moment\u0026rsquo;s Method]\u0026ndash;\u0026gt;() - [Two_NN] Variance Explained ratio: measures how much total variance is explained by the first $k$ principal components of data. It requieres an a priori number of PC to examine. Estimating Intrinsic Dimension of a Dataset by MLE from skdim.id import MLE Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 \u0026hellip; X_n$ in $\\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\\mathbb{R}^m$ , with unknown $m \\leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\\mathbb{R}^m$ are mapped to close neighbors in the embedding.\nThe basic idea is to fix a point $x$ , assume $f(x) \\sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:\n$$ N(t, x) = \\sum_{i=1}^n \\mathbf{1} {X_i \\in S_x(t)} $$\nwhich counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\\lambda(t)$ of the process $N(t)$ as:\n$$ \\lambda(t) = f(x) V(m) m t^{m-1} $$\nThis follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \\frac{d}{dt} \\left[ V(m) t^m \\right]$ is the surface area of the sphere $S_x(t)$. Letting $\\theta = \\log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:\n$$ L(m, \\theta) = \\int_0^R \\log \\lambda(t) , d N(t) - \\int_0^R \\lambda(t) , dt $$\nThis is an exponential family for which MLEs exist with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$ and are unique. The MLEs must satisfy the likelihood equations\n$$ \\frac{\\partial L}{\\partial \\theta} = \\int_0^R d N(t) - \\int_0^R \\lambda(t) , dt = N(R) - e^\\theta V(m) R^m = 0, $$\n$$ \\frac{\\partial L}{\\partial m} = \\left( \\frac{1}{m} + \\frac{V^{\\prime}(m)}{V(m)} \\right) N(R) + \\int_0^R \\log t , d N(t)\ne^\\theta V(m) R^m \\left( \\log R + \\frac{V^{\\prime}(m)}{V(m)} \\right) = 0. $$ Substituting we get:\n$$ m_R(x) = \\left[ \\frac{1}{N(R, x)} \\sum_{j=1}^{N(R, x)} \\log \\frac{R}{T_j(x)} \\right]^{-1}. $$\nIn practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:\n$$ m_k(x) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\log \\frac{T_k(x)}{T_j(x)} \\right]^{-1} $$\nNote that we omit the last (zero) term in the sum. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),\nFor some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. We will assume that all the data points come from the same \u0026ldquo;manifold,\u0026rdquo; and therefore average over all observations.\nThe choice of $k$ clearly affects the estimate. It can be the case that a dataset has different intrinsic dimensions at different scales, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.\n$$ m_k = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}_k(X_i) $$\n$$ m = \\frac{1}{k_2 - k_1 + 1} \\sum_{k=k_1}^{k_2} \\hat{m}_k. $$\nThe only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.\nEstimating Intrinsic Dimension of a Dataset by NN Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \\ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.\nThe volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by $$ \\Delta v_l=\\omega_d\\left(r_l^d-r_{l-1}^d\\right), $$ where $d$ is the dimensionality of the space in which the points are embedded and $\\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\\rho$ : $$ P\\left(\\Delta v_l \\in[v, v+d v]\\right)=\\rho e^{-\\rho v} d v $$\nConsider two shells $\\Delta v_1$ and $\\Delta v_2$, and let $R$ be the quantity $\\frac{\\Delta v_i}{\\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :\n$$ P(R \\in[\\bar{R}, \\bar{R}+d \\bar{R}]) = \\int_0^{\\infty} d v_i \\int_0^{\\infty} d v_j \\rho^2 e^{-\\rho\\left(v_i+v_j\\right)} \\left{\\frac{v_j}{v_i} \\in[\\bar{R}, \\bar{R}+d \\bar{R}]\\right} = d \\bar{R} \\frac{1}{(1+\\bar{R})^2}. $$\nwhere 1 represents the indicator function. Dividing by $d \\bar{R}$ we obtain the pdf for $R$ : $$ g(R)=\\frac{1}{(1+R)^2} $$\nThe pdf does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\\mu \\doteq \\frac{r_2}{r_1} \\in[1,+\\infty) . R$ and $\\mu$ are related by equality: $$ R=\\mu^d-1 $$\nThis equation allows to find an explicit formula for the distribution of $\\mu$ : $$ f(\\mu)=d \\mu^{-d-1} 1_{[1,+\\infty]}(\\mu), $$ while the cumulative distribution (cdf) is obtained by integration: $$ F(\\mu)=\\left(1-\\mu^{-d}\\right) 1_{[1,+\\infty]}(\\mu) . $$\nFunctions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.\nThe derivation presented above leads to a simple observation. The value of the intrinsic dimension $d$ can be estimated through the following equation: $$ \\frac{\\log (1-F(\\mu))}{\\log (\\mu)}=d $$\nRemarkably the density $\\rho$ does not appear in this equation, since the $\\operatorname{cdf} F$ is independent of $\\rho$. If we consider the set $S \\subset \\mathbb{R}^2, S \\doteq{(\\log (\\mu),-\\log (1-F(\\mu)))}$, $S$ is contained in a straight line $l \\doteq{(x, y) \\mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:\nCompute the pairwise distances for each point in the dataset $i=1, \\ldots, N$. For each point i find the two shortest distances $r_1$ and $r_2$. For each point i compute $\\mu_i=\\frac{r_2}{r_1}$. Compute the empirical cumulate $F^{e m p}(\\mu)$ by sorting the values of $\\mu$ in an ascending order through a permutation $\\sigma$, then define $F^{e m p}\\left(\\mu_{\\sigma(i)}\\right) \\doteq \\frac{i}{N}$. Fit the points of the plane given by coordinates ${\\ (\\log(\\mu_i), -\\log(1 - F^{\\text{emp}}(\\mu_i))) \\ | \\ i = 1, \\ldots, N\\ }$ with a straight line passing through the origin. Even if the results above are derived in the case of a uniform distribution of points there is no dependence on the density $\\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.\nEstimating Intrinsic Dimension of a Dataset by Moment\u0026rsquo;s Method from skdim.id import TwoNN Properties of Isotropy Mean Agnosticism:\nIsotropy is a property solely of the covariance matrix of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.\nInvariance to Scalar Multiples of the Covariance Matrix:\nIsotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\\Sigma=Cov(I)$ where $I$ is the identity matrix and $\\lambda \u0026gt;0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the shape of the distribution rather than the overall size of the data spread. Thus, for an isotropic distribution, $Cov(\\lambda)=Cov(\\lambda I)$.\nVariance Distribution Across Dimensions:\nFor a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.\nRotation Invariance:\nAn ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.\nUtilization of Dimensions:\nThere is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.\nGlobal Stability:\nAn isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.\n","permalink":"https://ldomenichelli.github.io/posts/post1/","summary":"\u003cp\u003eUnderstanding the \u003cstrong\u003ecurse of dimensionality\u003c/strong\u003e requires more than just examining the representational aspect of data. A key insight lies in the concept of \u003cstrong\u003eintrinsic dimensionality (ID)\u003c/strong\u003e‚Äîthe number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\u003c/p\u003e","title":"Space uniformity"},{"content":"Here are the slides and notes on the course hold by Prof. Chiaromonte at Sant\u0026rsquo;Anna University, Pisa. ","permalink":"https://ldomenichelli.github.io/posts/post8/","summary":"\u003cp\u003eHere are the slides and notes on the course hold by Prof. Chiaromonte at Sant\u0026rsquo;Anna University, Pisa.\n\u003cembed src=\"/SLLD_new.pdf\" width=\"100%\" height=\"800px\" type=\"application/pdf\"\u003e\u003c/p\u003e","title":"Statistical Learning and Large Data"},{"content":"Predictive Models for Time Series Analysis Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.\n1. Introduction to Time Series Analysis A time series is typically defined as: $$ T = { x_1, x_2, \\dots, x_m } $$ where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.\nUnivariate Time Series: A single variable measured over time. For instance, daily temperature readings. Multivariate Time Series: Multiple variables (channels) measured simultaneously, e.g., temperature, pressure, and humidity recorded at the same timestamps. Applications Forecasting: Predict future values based on historical patterns (e.g., stock prices, energy consumption). Classification / Regression: Predict class labels (e.g., device failure vs. normal) or numeric values (e.g., how many products will be sold). Clustering: Group time series exhibiting similar behavior or patterns (e.g., grouping customers by purchasing trends). Anomaly Detection: Detect unusual events or patterns in time (e.g., sudden sensor spikes). Pattern Mining: Identify repeated motifs (recurring subsequences) or rare discords. 2. Time Series Analytics Tasks Classification\nExample: Identify whether an ECG signal indicates normal heart activity or arrhythmia. Often uses distance-based approaches (DTW), shapelets, or feature-based transformations. Regression\nExample: Predict the amount of rainfall based on historical climate data. Can be framed as forecasting (single-step ahead) or a standard regression if the target is derived from the same temporal data. Forecasting\nExample: Project sales for the next quarter. Usually requires modeling temporal dependence (ARIMA, exponential smoothing, deep learning, etc.). Clustering\nExample: Group power-consumption patterns from different households to find typical usage profiles. May use DTW-based distance or other specialized measures. Anomaly Detection\nExample: Spot sudden temperature spikes in a production line sensor. Often involves statistical thresholding, machine learning models, or reconstruction-based methods (e.g., autoencoders). Pattern Mining\nExample: Detect repeating patterns (motifs) or unusual subsequences (discords) in ECG signals. 3. Time Series Visualization Common goals when plotting a time series:\nTrend: Does the data consistently move up, down, or show a long-term drift? Periodicity: Are there regular cycles, such as daily or monthly fluctuations? Seasonality: A special case of periodicity, often tied to known phenomena (e.g., yearly temperature cycles). Heteroskedasticity: Variance that changes over time (e.g., volatility clusters in financial data). Outliers: Points or subsequences that deviate significantly from the majority. Visual techniques might include standard line charts, rolling averages, or advanced dashboards that allow zooming and panning.\n4. Handling Missing Values Time series often have missing data due to sensor outages, data corruption, or irregular sampling.\nFilling with a constant value:\nForward fill (pad): Use the last known observation until a new one appears. Backward fill: Use the next known observation for previous gaps. Mean/median: Replace missing with overall mean or median. Nearest known value: Often used when sampling frequency is high. Linear Interpolation\nConnect neighboring known values with a straight line and fill in the gap. Forecasting-based Interpolation\nTrain a model on the known data and predict the missing points. Helps when the data exhibits predictable trends or seasonality. Random Imputation\nImpute from the distribution of known data. Sometimes used for simulation or bootstrapping. Careful selection of an imputation strategy is crucial‚Äîincorrect handling can introduce bias or distort subsequent analyses.\n5. Time Series Anomalies (Outliers) Outliers are observations that deviate substantially from the general behavior of the data. In time series, outliers might be due to measurement errors, system malfunctions, or truly significant (and possibly critical) events.\nTypes of Outliers Point Outlier: A single data point that is unusually large or small. Subsequence Outlier: A contiguous segment showing atypical behavior. Instance Outlier: An entire time series that differs markedly from others in a dataset. Common Outlier Detection Methods Histogram/Boxplot\nQuick visual approach; points beyond typical whiskers or outside certain standard deviations. IQR Filter\nLower bound:\n$$ Q1 - 1.5 \\times \\mathrm{IQR} $$ Upper bound:\n$$ Q3 + 1.5 \\times \\mathrm{IQR} $$ Data beyond these bounds may be considered outliers. Hampel Filter\nUses median and Median Absolute Deviation (MAD): $$ I = [\\text{median} - 3 \\times \\text{MAD},; \\text{median} + 3 \\times \\text{MAD}] $$ Grubbs‚Äô Test\nIteratively detects one outlier at a time, assuming normality of data. Sometimes, domain knowledge is key to deciding whether to remove outliers or treat them as important signals.\n6. Normalizations Due to varying scales, offsets, or trends in time series data, normalization or transformation steps can be essential:\nOffset Translation\nMean Removal: Subtract the global mean from all values. Min‚ÄìMax Normalization: Scale data to a ([0,1]) or ([-1,1]) range. Amplitude Scaling\nZ-Score Normalization: ( (x - \\mu) / \\sigma ). Helps unify variance. Linear Trend Removal\nDetrending: Fit and subtract a linear or polynomial trend. Mean Smoothing\nMoving Average: Helps reduce short-term volatility and highlight trends. Log Transformations\nUseful for data with exponential growth or multiplicative seasonality. Differencing\nReplace each value ( x_t ) with ( x_t - x_{t-1} ). Stabilizes mean if a strong linear trend is present. 7. Time Series Components A time series can often be decomposed into:\nLevel (baseline or average) Trend (long-term increase or decrease) Seasonality (repetitive, cyclical patterns) Noise (random, unexplained fluctuations) Additive Model $$ Y_t = \\text{Level} + \\text{Trend} + \\text{Seasonality} + \\text{Noise} $$\nMultiplicative Model $$ Y_t = \\text{Level} \\times \\text{Trend} \\times \\text{Seasonality} \\times \\text{Noise} $$\nFor instance, monthly sales data might have a rising trend, a strong seasonal pattern (e.g., holiday peaks), and some residual noise.\n8. Stationarity A time series is stationary if its statistical properties (mean, variance, autocovariance) remain constant over time. Many classic time series models (e.g., ARIMA) assume stationarity.\nCriteria:\nConstant Mean Constant Variance Autocovariance depends only on lag (i.e.,\n$$ \\mathrm{Cov}(x_t, x_{t+h}) \\text{ is independent of } t $$ ). Making a Series Stationary Detrending (subtracting a fitted linear or polynomial trend) Log Transform (reduces multiplicative effects) Differencing (subtract consecutive observations to remove trends) Seasonal Decomposition (removing seasonal components) Stationarity Test:\nAugmented Dickey‚ÄìFuller (ADF): If p-value is below a chosen threshold (e.g., 0.05), likely the series is stationary. 9. Time Series Similarities (Distances) Shape-based Similarity Euclidean Distance: Simple, but sensitive to misalignments or variable speeds. Dynamic Time Warping (DTW): Allows stretching/compressing in time, aligning sequences that are similar but out of phase. Sakoe‚ÄìChiba Band or Itakura Parallelogram can constrain warping paths, reducing computation. Structural-based Similarity Focuses on comparing broader patterns (e.g., overall shape, location of peaks/valleys). 10. Time Series Approximations \u0026amp; Dimensionality Reduction For very long or high-frequency time series, approximation methods reduce storage/computational demands:\nPAA (Piecewise Aggregate Approximation)\nDivide series into fixed-size segments; each segment is represented by its mean.\nSAX (Symbolic Aggregate Approximation)\nPAA + discretization into symbols from a finite alphabet.\nDFT (Discrete Fourier Transform)\nDecompose series into sums of sinusoidal components.\nSFA (Symbolic Fourier Approximation)\nDFT + discretization.\nSVD / PCA\nDimensionality reduction capturing principal variations. Often used across a collection of time series (e.g., multiple sensors). 11. Classification \u0026amp; Regression Instance-based (Memory-based) k-NN uses a distance measure (e.g., DTW). May store training data in memory and compare new time series to nearest neighbors. Linear / Logistic Models Often require stationarity or specific feature engineering to handle temporal correlations. Tree-based Approaches Decision Trees, Random Forests, Gradient Boosted Trees, etc. Work well with tabular features extracted from time windows (though must consider autocorrelation). Ensemble Methods Bagging, Boosting Proximity Forest, Time Series Forest (specialized ensemble methods). Interval-based Methods Time Series Forest Random Interval Spectral Ensemble (RISE) Supervised Time Series Forest\nThese extract features (mean, variance, slope, etc.) from various intervals of a time series. 12. Dictionary-based and Shapelet-based Models Dictionary-based Approaches Convert time series into ‚Äúdocuments‚Äù of discrete symbols and then analyze them with bag-of-words or similar text mining techniques:\nBag of Patterns (BOP) Bag of SFA Symbols (BOSS) WEASEL (Word ExtrAction for time SEries cLassification) Shapelet-based Models A shapelet is a small subsequence that is highly representative or discriminative of a specific class.\nExtraction: Identify the most discriminative subsequences. Transformation: Convert each full time series to a vector of distances to shapelets. Classification: Train any standard classifier (e.g., SVM, Random Forest) on shapelet-distance features. 13. Multivariate Time Series Multiple channels measured simultaneously:\nIndependent Assumption: Model each channel separately if they don‚Äôt interact strongly. Concatenation: Flatten all channels into one univariate series (loses some cross-channel info). Advanced Methods: MUSE (extension of WEASEL) integrates multiple channels. Neural networks (e.g., LSTM) that handle multiple input features. 14. Deep Learning Methods CNNs\nExploit convolution + pooling layers to automatically learn local features from raw data.\nRNNs / LSTMs\nModel long-term dependencies, capturing temporal context across many time steps.\nInception Networks\nUse multi-scale filters in parallel (adapted from computer vision).\n![[Pasted image 20250409113111.png]]\nTapNet, Multivariate LSTM-FCN, etc.\nMerge RNNs/CNNs with fully-connected layers for robust feature extraction.\nKernel-based Models ROCKET (RandOm Convolutional KErnel Transform) MiniRocket, MultiRocket Hydra, MultiROCKET-Hydra They transform time series via numerous random convolutional kernels, then feed into a linear model. Offers strong performance with high efficiency.\n15. Hybrid Models HIVE-COTE (Hierarchical Vote Collective of Transformation-based Ensembles)\nCombines diverse transformation modules (e.g., shapelets, dictionary methods, intervals). TS-CHIEF (Time Series Combination of Heterogeneous and Integrated Embeddings Forest)\nRandomized decision trees using multiple embedded approaches. These ensembles often achieve state-of-the-art classification accuracy on benchmark datasets by combining complementary representations.\n16. Explainable AI With complex models (deep networks, large ensembles), interpretability can be challenging:\nFeature Attribution: Methods like Grad-CAM, integrated gradients, or saliency maps adapted to time series. Shapelet-based: Provides subsequences that are inherently interpretable. Surrogate Models: Train a simpler, interpretable model (e.g., decision tree) to mimic the predictions of a black-box. Rule Extraction: Derive approximate rules or patterns from complex models. 17. Time Series Forecasting Forecasting means predicting future observations from historical data. Errors are typically measured by:\nMAE (Mean Absolute Error) RMSE (Root Mean Squared Error) MAPE (Mean Absolute Percentage Error) Simple Methods Average Method: Forecast is the mean of all past data. Na√Øve Method: Forecast is just the last observed value. Drift Method: Linear extrapolation from the first to the last observed point. These serve as baselines. More sophisticated models can outperform them, but these are often used as references.\n18. Exponential Smoothing Family SES (Simple Exponential Smoothing)\nSuitable for data with no trend or seasonality. Recent observations get higher weights.\nHolt‚Äôs Method\nExtends SES with a trend component.\nHolt‚ÄìWinters Method\nIncludes seasonality (additive or multiplicative).\n19. ARIMA-based Models ARIMA (AutoRegressive Integrated Moving Average) captures autocorrelation in time series.\nAR(p) Model $$ y_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + e_t $$\nMA(q) Model $$ y_t = c + e_t + \\theta_1 e_{t-1} + \\dots + \\theta_q e_{t-q} $$\nARIMA(p,d,q) Incorporates differencing of order (d) to handle non-stationarity.\nSARIMA Incorporates seasonal terms:\nAutoARIMA can automatically select (p, d, q) (and seasonal orders). Prophet (by Facebook/Meta) is another robust tool that handles multiple seasonalities, holidays, regressors, etc. 20. Forecasting via Reduced Regression This ‚Äúreduction‚Äù approach converts forecasting into a supervised learning problem:\nChoose a window size ( w ). E.g., use the last ( w ) observations as features. Predict the next value (or multiple future values). Use standard regression methods: Random forests, linear regression, XGBoost, etc. For multi-step forecasting, one can repeat this approach or predict multiple future time steps at once.\n21. Forecasting via Deep Learning RNN/LSTM\nCapture long-term sequential dependencies. Variants (GRU, Bi-LSTM) may improve performance. Sequence-to-Sequence (Seq2Seq) Networks\nOriginally used for machine translation, can handle multi-step forecasting. Temporal Fusion Transformers\nCombine attention mechanisms with recurrent networks to handle complex time series with covariates. Neural networks excel with large datasets and can learn nonlinear patterns that simpler models might miss.\nAdditional Considerations Hyperparameter Tuning: Time series models often have multiple hyperparameters (e.g., ARIMA orders, number of hidden units in an LSTM). Automated searches like grid search or Bayesian optimization can help. Model Validation: Standard cross-validation splits might not apply directly because of the temporal order. Techniques like rolling forecasting origin or walk-forward validation preserve the time structure. Performance Metrics: Selecting metrics that align with business goals or practical considerations (e.g., if small absolute errors or relative errors matter more). Domain Knowledge: Often crucial in deciding how to handle missing data, outliers, or interpret model outputs. Summary Time series analysis and forecasting encompass a broad spectrum of methods, from simple baselines (Na√Øve, Average) and classic models (ARIMA, exponential smoothing) to advanced machine learning techniques (deep learning, shapelets, kernel-based ensembles). The choice of method depends on:\nData Characteristics: Stationarity, seasonality, presence of trends, magnitude of noise. Task Requirements: Single-step forecasting, anomaly detection, classification, etc. Computational Constraints: For large-scale or high-frequency data, efficient approximation or specialized algorithms may be needed. Explainability: Simpler models or shapelet-based approaches may provide clearer insights, while deep models might yield higher accuracy but be harder to interpret. Overall, success in time series analysis hinges on proper preprocessing (missing-value handling, outlier management, normalization, feature engineering) and a careful choice of models and validation strategies.\n","permalink":"https://ldomenichelli.github.io/posts/post9/","summary":"\u003ch1 id=\"predictive-models-for-time-series-analysis\"\u003ePredictive Models for Time Series Analysis\u003c/h1\u003e\n\u003cp\u003eTime series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-introduction-to-time-series-analysis\"\u003e1. Introduction to Time Series Analysis\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003etime series\u003c/strong\u003e is typically defined as:\n$$\nT = { x_1, x_2, \\dots, x_m }\n$$\nwhere each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.\u003c/p\u003e","title":"Time Series"},{"content":"Here are my notes from the lectures on \u0026ldquo;Topological Data Analysis\u0026rdquo; that was held from Prof. Patrizio Frosini at UniPi in Jan 2025.\n","permalink":"https://ldomenichelli.github.io/posts/post4/","summary":"\u003cp\u003eHere are my notes from the lectures on \u0026ldquo;Topological Data Analysis\u0026rdquo; that was held from Prof. Patrizio Frosini at UniPi in Jan 2025.\u003c/p\u003e\n\u003cembed src=\"/tdafull.pdf\" width=\"100%\" height=\"800px\" type=\"application/pdf\"\u003e","title":"Topological Data Analysis"}]