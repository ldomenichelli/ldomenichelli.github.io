[{"content":"üóì Day¬†1 ‚Äî Monday, 10‚ÄØFeb‚ÄØ2025 üåç What Is Common‚ÄØCrawl? Common‚ÄØCrawl is a huge, free snapshot of the public web.\nA non‚Äëprofit updates it every month, storing:\nBillions of HTML pages Their cleaned‚Äëup text content Extra metadata (links, timestamps, MIME types, ‚Ä¶) Why It Matters Track language change ‚Äì see how words, memes, and topics shift over time. Map the web‚Äôs link network ‚Äì study which sites connect and why. Train big ML models ‚Äì use real‚Äëworld data instead of tiny toy datasets. Because each release includes both the raw HTML and a parsed text layer, you can analyze:\nLayer What you can study Raw HTML structure Link graphs, page layout, site categories Clean text content Sentiment, topic trends, new buzzwords Perks for Researchers No crawler needed ‚Äì skip the cost and hassle of scraping the web yourself. Open licence ‚Äì anyone can share code, replicate results, and build on your work. Regular updates ‚Äì monthly snapshots reveal sudden spikes (e.g., when a new tech goes viral). All of this makes Common‚ÄØCrawl a go‚Äëto resource for tasks like:\nNamed‚Äëentity recognition Topic classification Question answering By pooling efforts around one massive, open dataset, researchers push the limits of NLP faster than they could alone.\nüìç Common Crawl üìç Factuality¬†\u0026amp;¬†Hallucinations¬†in¬†LLMs Speaker: Anna‚ÄØRogers (University‚ÄØof‚ÄØCopenhagen)\n‚ÄúLarge language models are fluent‚ÄØbullshit generators‚Äîthey sound right even when they‚Äôre wrong.‚Äù\n‚Äî‚ÄØA.‚ÄØRogers\nLLMs can drift from the truth, a problem known as hallucination. Rogers reviews two popular fixes and where they fall short:\nApproach How it works Main weakness RAG\n(Retrieval‚ÄëAugmented Generation) Looks up facts in a search index or database, then feeds the snippets to the model as it writes. Bad retrieval = bad answer; citations can be incorrect or missing. CoT\n(Chain‚Äëof‚ÄëThought prompting) Prompts the model to show step‚Äëby‚Äëstep reasoning before the final answer. ‚ÄúReasoning‚Äù may be invented; method can be abused to jailbreak the model. Impact on the Web Surge in AI‚Äëgenerated spam and click‚Äëbait Harder to tell real news from synthetic text New headaches for search engines and fact‚Äëcheckers Takeaway: RAG and CoT help, but they don‚Äôt eliminate hallucinations. Better evaluation metrics and stronger guardrails are still needed.\nRAG (Retrieval‚ÄëAugmented Generation) and CoT (Chain‚Äëof‚ÄëThought) both try to make LLM answers more trustworthy, but neither is a silver bullet.\nüîç RAG ‚Äî Look it up, then write How it works\nRetrieve‚ÄáFind facts in a search index or database. Generate‚ÄáFeed those facts to the model so it can weave them into its answer. Where it breaks\nIf the search misses the right passage, the answer is still wrong. Measuring ‚Äúquality‚Äù is tricky: you need to score retrieval hit‚Äërate, answer truthfulness, and source fidelity‚Äîall at once. Evaluations often rely on yet another LLM, which can add bias. Even with good sources, the model may paraphrase or misquote them. üìù CoT ‚Äî Show your thinking How it works\nGive the model examples that spell out step‚Äëby‚Äëstep reasoning. Ask it to copy that style: ‚ÄúFirst, think. Then, answer.‚Äù Where it breaks\nWorks great on some tasks, worse on others‚Äîespecially biased ones. The ‚Äúreasoning‚Äù it prints may be made up, not its true internal logic. Attackers can use CoT to slip past safety rules (‚Äújailbreaking‚Äù). üóì Day¬†2 ‚Äî Tuesday, 11‚ÄØFeb‚ÄØ2025 üìç FineWeb‚ÄØ2 ‚Äî Multilingual Web Data at Scale Speaker: Guilherme‚ÄØPenedo (Hugging‚ÄØFace)\nThe opening talk introduced FineWeb‚ÄØ2, a brand‚Äënew, multilingual web corpus for pre‚Äëtraining large language models.\nPenedo explained how the team is porting and tuning the English‚Äëcentric cleaning pipeline‚Äîdeduplication, language ID, toxicity filters, and more‚Äîso it works reliably across dozens of other languages.\nüìç Power‚ÄØLaws \u0026amp; Generalization Speakers: Jenia‚ÄØJitsev‚ÄØ\u0026amp;‚ÄØMarianna‚ÄØNezhurina\nThis session explored how power‚Äëlaw scaling shows up in deep‚Äëlearning curves‚Äîand why turning those neat mathematical fits into real‚Äëworld ‚Äúgeneralization scores‚Äù is harder than it looks. The speakers highlighted pitfalls such as noisy data, shifting task definitions, and compute limits that break the power‚Äëlaw trend once models get big enough.\n## üìç *Generalization* ","permalink":"https://ldomenichelli.github.io/posts/post5/","summary":"\u003ch1 id=\"-day1--monday-10feb2025\"\u003eüóì Day¬†1 ‚Äî Monday, 10‚ÄØFeb‚ÄØ2025\u003c/h1\u003e\n\u003ch3 id=\"-what-is-commoncrawl\"\u003eüåç What Is \u003cstrong\u003eCommon‚ÄØCrawl\u003c/strong\u003e?\u003c/h3\u003e\n\u003cp\u003eCommon‚ÄØCrawl is a \u003cstrong\u003ehuge, free snapshot of the public web\u003c/strong\u003e.\u003cbr\u003e\nA non‚Äëprofit updates it every month, storing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBillions of HTML pages\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eTheir cleaned‚Äëup \u003cstrong\u003etext content\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eExtra \u003cstrong\u003emetadata\u003c/strong\u003e (links, timestamps, MIME types, ‚Ä¶)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"why-it-matters\"\u003eWhy It Matters\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTrack language change\u003c/strong\u003e ‚Äì see how words, memes, and topics shift over time.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMap the web‚Äôs link network\u003c/strong\u003e ‚Äì study which sites connect and why.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTrain big ML models\u003c/strong\u003e ‚Äì use real‚Äëworld data instead of tiny toy datasets.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBecause each release includes both the \u003cstrong\u003eraw HTML\u003c/strong\u003e and a parsed text layer, you can analyze:\u003c/p\u003e","title":"‚ùÑÔ∏è HPLT √ó NLPL Winter School"},{"content":"What is Abalone? Have you ever tried pushing your opponent off a cliff‚Ä¶ in a friendly way? If that sounds intriguing, let me introduce you to Abalone, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent L√©vi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\nThe game‚Äôs name comes from the abalone, a type of mollusk known for its \u0026ldquo;ear-shaped\u0026rdquo; shell. The Italian name for the game, aliotide, combines the Latin prefix \u0026ldquo;ab-\u0026rdquo; (meaning \u0026ldquo;from\u0026rdquo; or \u0026ldquo;away\u0026rdquo;) and the English word \u0026ldquo;alone,\u0026rdquo; hinting at the isolated struggle between two opponents.\nIn Abalone, two players compete on a hexagonal board with 14 marbles each: white for one player, black for the other. The aim is straightforward: be the first to push six of your opponent\u0026rsquo;s marbles off the board. But as you\u0026rsquo;ll soon discover, achieving this goal requires careful planning, tactical moves, and a deep understanding of positioning.\nGame Components and Setup The game is played on a hexagonal board with 61 circular positions arranged in rows:\nFigure 1: Board layout\nEach player starts with 14 marbles, and the initial setup (shown below) places these marbles in a specific formation, primed for strategic movement.\nFigure 2: Initial setup\nEach position on the board is labeled using a grid system with letters and numbers, allowing players to communicate moves easily.\nBasic Rules and Movements In Abalone, players take turns making a single move per turn. Here are the core rules:\nMoving a Marble: On your turn, you can move one marble to any adjacent, empty spot on the board. Line and Lateral Moves: You can also move a line of two or three marbles as long as they are aligned in the same direction. This can be done in two ways: In-Line Movement: Move all marbles in the line forward in the same direction. Lateral Movement: Shift all marbles in the line to the side without changing their orientation. Sumito (Pushing Marbles): If your marbles outnumber the adjacent marbles of your opponent in a line, you can push them. For instance, two marbles can push one, and three can push two. Pushing is only possible if there‚Äôs an empty spot behind the opposing marble(s) for them to move into. Special Positioning: \u0026ldquo;Pac\u0026rdquo; and Strategic Pushes The concept of pac is crucial for mastering Abalone. When white and black marbles are aligned in equal numbers, neither player can push the other (this creates a \u0026ldquo;pac\u0026rdquo; or standoff). This means that understanding numerical superiority and positioning is essential.\nFor example, in a scenario where three black marbles face three white marbles, no push is possible. However, a setup with three black marbles against two white marbles allows the black player to push forward. This rule creates opportunities to set up defenses and traps, making the game highly tactical.\nKey Strategies for Winning To succeed in Abalone, keep these strategies in mind:\nControl the Center: Marbles near the edges are at greater risk of being pushed out. Maintaining a central position allows flexibility and reduces the likelihood of getting cornered. Set Up Sumito Opportunities: Since pushing depends on having more marbles in a line, positioning your marbles strategically to outnumber opponents in key areas is vital. Avoid Isolation: Isolated marbles are easy targets for a Sumito. Keep your marbles grouped to maintain pushing power and defend against your opponent‚Äôs moves. Force a Pac: Sometimes, creating a standoff situation (pac) can disrupt your opponent‚Äôs plans, giving you time to reposition your marbles. Now, enjoy this 90\u0026rsquo;s commercial or play the game online while I\u0026rsquo;m writing the rest of the rules!\n","permalink":"https://ldomenichelli.github.io/games/abalone/","summary":"\u003ch2 id=\"what-is-abalone\"\u003eWhat is Abalone?\u003c/h2\u003e\n\u003cp\u003eHave you ever tried pushing your opponent off a cliff‚Ä¶ in a friendly way? If that sounds intriguing, let me introduce you to \u003cem\u003eAbalone\u003c/em\u003e, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent L√©vi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\u003c/p\u003e","title":"Abalone"},{"content":" me fr fr.\nWelcome to my study space! I\u0026rsquo;m Lucia, a first-year PhD student in AI at UniPi. My research interest in mostly about Representation Learning in NLP.\nOutside of research, my main hobbies are learning languages and bouldering üßó\nHere, I collect personal notes on various topics I‚Äôm learning. They‚Äôre written for me, but might be helpful to others, too. Enjoy reading!\nNews Our paper ‚ÄúFrom Human Reading to NLM Understanding: Evaluating the Role of Eye‚ÄëTracking Data in Encoder‚ÄëBased Models‚Äù was accepted at CLiC-it¬†2025¬†ü¶© \u0026mdash; ItaliaNLP‚ÄØLab (@ItaliaNLP_Lab) July¬†22 2025 I presented a poster at the ‚ÄúLectures on Computational Linguistics‚Äù in Milan! \u0026mdash; ItaliaNLP‚ÄØLab (@ItaliaNLP_Lab) July¬†14‚ÄØ2025 üëâ Poster¬†(PDF) üéûÔ∏è Slides¬†(PDF) Our paper ‚ÄúFrom Human Reading to NLM Understanding: Evaluating the Role of Eye‚ÄëTracking Data in Encoder‚ÄëBased Models‚Äù was accepted at ACL¬†2025¬†üéâ \u0026mdash; ItaliaNLP‚ÄØLab (@ItaliaNLP_Lab) July¬†9‚ÄØ2025 üìÑ Link to the paper ¬∑ ","permalink":"https://ldomenichelli.github.io/about/","summary":"who am i?","title":"about this site and me"},{"content":"Achi: A Traditional African Game Achi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as Care), and others. It is classified among alignment games, which also include games like Tapatan, Tant Fant, Shisima, and Pong Hau K\u0026rsquo;i.\nObjective: The goal of Achi is to form an uninterrupted line of three pieces of the same color, placed on the same horizontal, vertical, or diagonal line. Once a player achieves this, the game ends immediately, and that player wins.\nEquipment: The game consists of:\nA board with 9 intersections (as shown in the diagram). 8 pieces: 4 white and 4 black. Pieces are placed on the intersections, not within the squares of the grid. Players: The game is played by two players, who take turns.\nRules: There are two main phases in the game: the Placement Phase and the Movement Phase.\n1. Placement Phase: During the Placement Phase, players take turns placing one piece at a time on any available intersection on the board. Pieces cannot be moved during this phase. This phase ends once all 8 pieces (4 for each player) have been placed on the board. After this phase, there will be only one empty intersection left on the board. The initial setup leaves the board empty, and either player can start. 2. Movement Phase: Once all pieces are placed, the game transitions into the Movement Phase. From the fifth move onwards, players can move their pieces. A piece may be moved to an adjacent intersection (orthogonally or diagonally), as long as the target intersection is empty. Players continue to move their pieces, aiming to align three of their pieces in a row (horizontally, vertically, or diagonally). The game ends as soon as a player forms a line of three pieces of their color, either during the Placement Phase or the Movement Phase. ","permalink":"https://ldomenichelli.github.io/games/achi/","summary":"\u003ch1 id=\"achi-a-traditional-african-game\"\u003eAchi: A Traditional African Game\u003c/h1\u003e\n\u003cp\u003eAchi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as \u003cem\u003eCare\u003c/em\u003e), and others. It is classified among alignment games, which also include games like \u003cem\u003eTapatan\u003c/em\u003e, \u003cem\u003eTant Fant\u003c/em\u003e, \u003cem\u003eShisima\u003c/em\u003e, and \u003cem\u003ePong Hau K\u0026rsquo;i\u003c/em\u003e.\u003c/p\u003e","title":"Achi"},{"content":"Adugo: The Game of the Jaguar and Dogs Adugo, also known as \u0026ldquo;Jaguar and Dogs,\u0026rdquo; is a traditional game played by the Bororo people in Brazil. This game simulates a hunting scenario and shares similarities with other strategic games from Southeast Asia.\nOverview of the Game Adugo is played on a board where the black piece represents the jaguar, and the white pieces represent the dogs. The game is an asymmetrical strategy challenge, where each side has distinct goals and abilities. How to Play Setup: The board consists of intersecting lines forming a grid. The jaguar starts on one side of the board, while the dogs are positioned on the opposite side. First Move: The jaguar always moves first. Movement Rules: Jaguar: Can move one intersection at a time, along orthogonal or diagonal lines. Dogs: Each dog moves one intersection at a time, along the same lines. Capturing: The jaguar can capture dogs by jumping over them, akin to the rules in checkers. A capture is only possible if the spot directly behind the dog is vacant. Winning the Game Jaguar\u0026rsquo;s Goal: Capture all the dogs. Dogs\u0026rsquo; Goal: Surround and trap the jaguar so it cannot move. CN6gs2sN 329681219498\n","permalink":"https://ldomenichelli.github.io/games/adugo/","summary":"\u003ch1 id=\"adugo-the-game-of-the-jaguar-and-dogs\"\u003eAdugo: The Game of the Jaguar and Dogs\u003c/h1\u003e\n\u003cp\u003eAdugo, also known as \u0026ldquo;Jaguar and Dogs,\u0026rdquo; is a traditional game played by the \u003ca href=\"https://pib.socioambiental.org/en/Povo:Bororo\"\u003eBororo\u003c/a\u003e people in Brazil. This game simulates a hunting scenario and shares similarities with other strategic games from Southeast Asia.\u003c/p\u003e\n\u003ch2 id=\"overview-of-the-game\"\u003eOverview of the Game\u003c/h2\u003e\n\u003cp\u003eAdugo is played on a board where the black piece represents the \u003cstrong\u003ejaguar\u003c/strong\u003e, and the white pieces represent the \u003cstrong\u003edogs\u003c/strong\u003e. The game is an asymmetrical strategy challenge, where each side has distinct goals and abilities.\n\n\n\n\n\n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-06b52\" hidden\u003e\n    \u003clabel for=\"zoomCheck-06b52\"\u003e\n        \u003cimg class=\"zoomCheck\" loading=\"lazy\" decoding=\"async\"\n            src=\"img/plancia.png\" alt=\"plancia\"\n             /\u003e\n    \u003c/label\u003e\n\n\u003c/p\u003e","title":"Adugo"},{"content":"Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)‚Äîthe number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\nThe Role of Dimensionality Reduction Dimensionality reduction has gained prominence in machine learning and data science for its ability to simplify complex datasets while preserving essential structure. Methods in this domain are broadly categorized into:\nLinear Techniques:\nPrincipal Component Analysis (PCA) and its variants (e.g., Bouveyron et al. 2011) focus on orthogonal transformations to project data onto a lower-dimensional space. Non-linear Techniques (Manifold Learning):\nThese methods include Isometric Mapping (Tenenbaum et al. 2000), Locally Linear Embedding (Roweis and Saul 2000), and Hessian Eigenmapping (Donoho and Grimes 2003). They aim to capture the underlying structure of data that lies on a non-linear manifold. While most methods require users to specify a target dimension, some techniques adaptively infer it based on the dataset\u0026rsquo;s intrinsic dimensionality. This adaptability underscores the significance of ID estimation. Models and Methods for Estimating Intrinsic Dimensionality Over the years, researchers have developed diverse models to estimate intrinsic dimensionality. These fall into several categories:\nTopological Approaches: Analyze the tangent space of the manifold using local samples (e.g., Fukunaga and Olsen 1971; Verveer and Duin 1995).\nFractal Measures: Use metrics like the Correlation Dimension (Faloutsos and Kamel 1994) to estimate ID based on space-filling properties of data.\nGraph-based Methods: Employ $k$-nearest neighbors and density metrics to infer ID (Costa and Hero 2004).\nParametric Models: Leverage statistical models to estimate ID, such as those by Levina and Bickel (2004).\nGlobal vs. Local Intrinsic Dimensionality Intrinsic dimensionality measures can be broadly classified into global and local approaches:\nGlobal Measures: Analyze the dataset as a whole, treating all objects uniformly. These measures are well-suited for homogeneous datasets with a single dominant manifold.\nLocal Measures: Focus on the $k$-nearest neighbors of a specific point. These methods are essential for heterogeneous datasets comprising multiple, overlapping manifolds. Notable local ID models include:\nExpansion Dimension (ED) (Karger and Ruhl 2002). Generalized Expansion Dimension (GED) (Houle et al. 2012). Local Intrinsic Dimensionality (LID) (Houle 2013). Local ID measures are particularly relevant in applications such as similarity search, where they can estimate query complexity or optimize search termination. They are also applied in outlier detection and density estimation.\nBalancing Local and Global Insights Machine learning techniques often face challenges like overfitting when relying heavily on local information. To mitigate this, methods such as Local Tangent Space Alignment (LTSA) (Zhang and Zha 2004) combine local and global perspectives by aligning neighborhoods of points while penalizing overfitting during optimization. This balance enables a more comprehensive understanding of the data structure.\nContinuous Intrinsic Dimension In this section, we explore Local Intrinsic Dimensionality (LID), a model that extends intrinsic dimensionality to continuous distributions of distances, as proposed by Houle (2013). LID quantifies the local intrinsic dimensionality (ID) of a feature space by focusing exclusively on the distribution of inter-point distances.\nDefining the Distribution of Distances Let $(\\mathbb{R}^m, \\text{dist})$ represent a domain equipped with a non-negative distance function dist. Consider the distribution of distances with respect to a fixed reference point. This distribution can be modeled as a random variable $\\mathbf{X}$, with support $[0, \\infty)$. The probability density function (PDF) of $\\mathbf{X}$ is denoted by $f_{\\mathbf{X}}$, where $f_{\\mathbf{X}}$ is a non-negative, Lebesgue-integrable function. For any $a, b \\in [0, \\infty)$ such that $a \\leq b$, the probability is given by:\n$$ \\operatorname{Pr}[a \\leq \\mathbf{X} \\leq b] = \\int_a^b f_{\\mathbf{X}}(x) , \\mathrm{d}x. $$\nThe corresponding cumulative density function (CDF) $F_{\\mathbf{X}}$ is defined as:\n$$ F_{\\mathbf{X}}(x) = \\operatorname{Pr}[\\mathbf{X} \\leq x] = \\int_0^x f_{\\mathbf{X}}(u) , \\mathrm{d}u. $$\nFor values where $\\mathbf{X}$ is absolutely continuous at $x$, the CDF $F_{\\mathbf{X}}$ is differentiable at $x$, and its first-order derivative is $f_{\\mathbf{X}}(x)$.\nThe Local Continuous Intrinsic Dimension The local intrinsic dimension at distance $x$ is defined as follows:\nDefinition 1 (Houle, 2013):\nGiven an absolutely continuous random distance variable $\\mathbf{X}$, for any distance threshold $x$ such that $F_{\\mathbf{X}}(x) \u0026gt; 0$, the local continuous intrinsic dimension $ \\mathrm{ID}_{\\mathbf{X}}(x)$ of $\\mathbf{X}$ at distance $x$ is:\n$$ \\mathrm{ID}{\\mathbf{X}}(x) \\triangleq \\lim{\\epsilon \\to 0^+} \\frac{\\ln F_{\\mathbf{X}}((1+\\epsilon) x) - \\ln F_{\\mathbf{X}}(x)}{\\ln (1+\\epsilon)} $$\nwherever the limit exists.\nRelation to the Generalized Expansion Dimension The LID definition builds upon the generalized expansion dimension (GED) proposed by Houle et al. (2012a). GED measures dimensionality by comparing neighborhood radii $x$ and $(1+\\epsilon)x$, replacing neighborhood cardinalities with the expected number of neighbors.\nIn essence, the LID formulation quantifies the discriminative power of a distance measure. Both LID and GED share the same closed-form representation, reflecting their foundational equivalence in characterizing local dimensionality.\nTheorem 1 (Houle 2013) Let X be an absolutely continuous random distance variable. If $F_X$ is both positive and differentiable at x, then:\n$$ \\text{ID}_X(x) = \\frac{x f_X(x)}{F_X(x)}. $$\nLocal intrinsic dimensionality (Local ID) has potential for wide application thanks to its very general treatment of distances as a continuous random variable. Direct estimation of $ID_X(x)$, however, requires the knowledge of the distribution of $X$. Extreme value theory, which we survey in the following section, allows the estimation of the limit of $x \\to 0$ without any explicit assumptions of the data distribution other than continuity.\n3. Extreme Value Theory Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes.\nDefinition 2 Let $\\mu \\in R$ and $\\sigma \u0026gt; 0$. The family of generalized extreme value distributions $F_{GEV}$ covers distributions whose cumulative distribution functions have the form:\n$$ F_{GEV} = \\exp \\left( - \\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-\\frac{1}{\\xi}} \\right) \u0026amp; \\text{if } \\xi \\neq 0, \\ \\exp \\left( -\\exp \\left( -\\frac{x - \\mu}{\\sigma} \\right) \\right) \u0026amp; \\text{if } \\xi = 0. $$\nA distribution $G \\in F_{GEV}$ has support:\n$$ \\text{supp}(G) = \\begin{cases} [\\mu - \\frac{\\sigma}{\\xi}, \\infty) \u0026amp; \\text{when } \\xi \u0026gt; 0, \\ (-\\infty, \\mu - \\frac{\\sigma}{\\xi}] \u0026amp; \\text{when } \\xi \u0026lt; 0, \\ (-\\infty, \\infty) \u0026amp; \\text{if } \\xi = 0. \\end{cases} $$\nIts best-known theorem, attributed in parts to Fisher and Tippett (1928), and Gnedenko (1943), states that the maximum of n independent identically-distributed random variables (after proper renormalization) converges in distribution to a generalized extreme value distribution as $n \\to \\infty$ .\nTheorem 2 (Fisher-Tippett-Gnedenko)\nLet $(X_i)_{i \\in N}$ be a sequence of independent identically-distributed random variables and let $M_n=max X_i$ If there exist a sequence of positive constants $a_n$, $n \\in N$ and a sequence of constants $b_n$ , $n \\in N$ , such that:\n$$ \\lim_{n \\to \\infty} P\\left( \\frac{M_n - b_n}{a_n} \\leq x \\right) = F(x), $$\nthen F(x) belongs to the generalized extreme value family.\nIsotropy A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportion al to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\nOther measures Avg cosine similarity : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution. Partition Isotropy score : Proposed by Arora et al. $$Z(c):= \\sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$ Intrinsic Dimensionality : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy. Linear dimensionality estimate: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions. *Non linear dimensionality estimate: - [MLE] \u0026ndash;\u0026gt; Levina2005\n- [Moment\u0026rsquo;s Method]\u0026ndash;\u0026gt;() - [Two_NN] Variance Explained ratio: measures how much total variance is explained by the first $k$ principal components of data. It requieres an a priori number of PC to examine. Estimating Intrinsic Dimension of a Dataset by MLE from skdim.id import MLE Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 \u0026hellip; X_n$ in $\\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\\mathbb{R}^m$ , with unknown $m \\leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\\mathbb{R}^m$ are mapped to close neighbors in the embedding.\nThe basic idea is to fix a point $x$ , assume $f(x) \\sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:\n$$ N(t, x) = \\sum_{i=1}^n \\mathbf{1} {X_i \\in S_x(t)} $$\nwhich counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\\lambda(t)$ of the process $N(t)$ as:\n$$ \\lambda(t) = f(x) V(m) m t^{m-1} $$\nThis follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \\frac{d}{dt} \\left[ V(m) t^m \\right]$ is the surface area of the sphere $S_x(t)$. Letting $\\theta = \\log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:\n$$ L(m, \\theta) = \\int_0^R \\log \\lambda(t) , d N(t) - \\int_0^R \\lambda(t) , dt $$\nThis is an exponential family for which MLEs exist with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$ and are unique. The MLEs must satisfy the likelihood equations\n$$ \\frac{\\partial L}{\\partial \\theta} = \\int_0^R d N(t) - \\int_0^R \\lambda(t) , dt = N(R) - e^\\theta V(m) R^m = 0, $$\n$$ \\frac{\\partial L}{\\partial m} = \\left( \\frac{1}{m} + \\frac{V^{\\prime}(m)}{V(m)} \\right) N(R) + \\int_0^R \\log t , d N(t)\ne^\\theta V(m) R^m \\left( \\log R + \\frac{V^{\\prime}(m)}{V(m)} \\right) = 0. $$ Substituting we get:\n$$ m_R(x) = \\left[ \\frac{1}{N(R, x)} \\sum_{j=1}^{N(R, x)} \\log \\frac{R}{T_j(x)} \\right]^{-1}. $$\nIn practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:\n$$ m_k(x) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\log \\frac{T_k(x)}{T_j(x)} \\right]^{-1} $$\nNote that we omit the last (zero) term in the sum. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),\nFor some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. We will assume that all the data points come from the same \u0026ldquo;manifold,\u0026rdquo; and therefore average over all observations.\nThe choice of $k$ clearly affects the estimate. It can be the case that a dataset has different intrinsic dimensions at different scales, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.\n$$ m_k = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}_k(X_i) $$\n$$ m = \\frac{1}{k_2 - k_1 + 1} \\sum_{k=k_1}^{k_2} \\hat{m}_k. $$\nThe only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.\nEstimating Intrinsic Dimension of a Dataset by NN Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \\ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.\nThe volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by $$ \\Delta v_l=\\omega_d\\left(r_l^d-r_{l-1}^d\\right), $$ where $d$ is the dimensionality of the space in which the points are embedded and $\\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\\rho$ : $$ P\\left(\\Delta v_l \\in[v, v+d v]\\right)=\\rho e^{-\\rho v} d v $$\nConsider two shells $\\Delta v_1$ and $\\Delta v_2$, and let $R$ be the quantity $\\frac{\\Delta v_i}{\\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :\n$$ P(R \\in[\\bar{R}, \\bar{R}+d \\bar{R}]) = \\int_0^{\\infty} d v_i \\int_0^{\\infty} d v_j \\rho^2 e^{-\\rho\\left(v_i+v_j\\right)} \\left{\\frac{v_j}{v_i} \\in[\\bar{R}, \\bar{R}+d \\bar{R}]\\right} = d \\bar{R} \\frac{1}{(1+\\bar{R})^2}. $$\nwhere 1 represents the indicator function. Dividing by $d \\bar{R}$ we obtain the pdf for $R$ : $$ g(R)=\\frac{1}{(1+R)^2} $$\nThe pdf does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\\mu \\doteq \\frac{r_2}{r_1} \\in[1,+\\infty) . R$ and $\\mu$ are related by equality: $$ R=\\mu^d-1 $$\nThis equation allows to find an explicit formula for the distribution of $\\mu$ : $$ f(\\mu)=d \\mu^{-d-1} 1_{[1,+\\infty]}(\\mu), $$ while the cumulative distribution (cdf) is obtained by integration: $$ F(\\mu)=\\left(1-\\mu^{-d}\\right) 1_{[1,+\\infty]}(\\mu) . $$\nFunctions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.\nThe derivation presented above leads to a simple observation. The value of the intrinsic dimension $d$ can be estimated through the following equation: $$ \\frac{\\log (1-F(\\mu))}{\\log (\\mu)}=d $$\nRemarkably the density $\\rho$ does not appear in this equation, since the $\\operatorname{cdf} F$ is independent of $\\rho$. If we consider the set $S \\subset \\mathbb{R}^2, S \\doteq{(\\log (\\mu),-\\log (1-F(\\mu)))}$, $S$ is contained in a straight line $l \\doteq{(x, y) \\mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:\nCompute the pairwise distances for each point in the dataset $i=1, \\ldots, N$. For each point i find the two shortest distances $r_1$ and $r_2$. For each point i compute $\\mu_i=\\frac{r_2}{r_1}$. Compute the empirical cumulate $F^{e m p}(\\mu)$ by sorting the values of $\\mu$ in an ascending order through a permutation $\\sigma$, then define $F^{e m p}\\left(\\mu_{\\sigma(i)}\\right) \\doteq \\frac{i}{N}$. Fit the points of the plane given by coordinates ${\\ (\\log(\\mu_i), -\\log(1 - F^{\\text{emp}}(\\mu_i))) \\ | \\ i = 1, \\ldots, N\\ }$ with a straight line passing through the origin. Even if the results above are derived in the case of a uniform distribution of points there is no dependence on the density $\\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.\nEstimating Intrinsic Dimension of a Dataset by Moment\u0026rsquo;s Method from skdim.id import TwoNN Properties of Isotropy Mean Agnosticism:\nIsotropy is a property solely of the covariance matrix of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.\nInvariance to Scalar Multiples of the Covariance Matrix:\nIsotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\\Sigma=Cov(I)$ where $I$ is the identity matrix and $\\lambda \u0026gt;0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the shape of the distribution rather than the overall size of the data spread. Thus, for an isotropic distribution, $Cov(\\lambda)=Cov(\\lambda I)$.\nVariance Distribution Across Dimensions:\nFor a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.\nRotation Invariance:\nAn ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.\nUtilization of Dimensions:\nThere is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.\nGlobal Stability:\nAn isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.\n","permalink":"https://ldomenichelli.github.io/posts/post1/","summary":"\u003cp\u003eUnderstanding the \u003cstrong\u003ecurse of dimensionality\u003c/strong\u003e requires more than just examining the representational aspect of data. A key insight lies in the concept of \u003cstrong\u003eintrinsic dimensionality (ID)\u003c/strong\u003e‚Äîthe number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\u003c/p\u003e","title":"Embeddings space ñ¶π◊Ç ‚ÇäÀö‚äπ‚ãÜ"},{"content":"üèóÔ∏è‚ÄØRepresentation‚ÄØLearning for NLP All neural‚Äënetwork (NN) architectures create vector‚ÄØrepresentations‚Äîalso called embeddings‚Äîof the input.\nThese vectors pack statistical and semantic cues that let the model classify, translate, or generate text.\nThe network learns better representations through feedback from a loss function.\nTransformers build features for each word with an attention mechanism that asks:\n‚ÄúHow important is every other word in the sentence to this word?‚Äù\nüîó‚ÄØGNNs‚ÄîRepresenting Graphs Graph Neural Networks (GNNs) or Graph Convolutional Networks (GCNs) embed nodes and edges.\nThey rely on neighbourhood aggregation‚ÄØ/‚ÄØmessage passing:\nGNN update the hidden feature h of node i at layer l via a non-linear transformation of the node\u0026rsquo;s own feature $h_i^l$ added to the aggregation of feature $h_j^l$ from each neighboring node $j \\in N(i)$\n$$h_{i}^{\\ell+1} = \\sigma \\Big( U^{\\ell} h_{i}^{\\ell} + \\sum_{j \\in \\mathcal{N}(i)} \\left( V^{\\ell} h_{j}^{\\ell} \\right) \\Big)$$ where $U^l$ and $V^l$ are learnable weight matrices of the GNN layer and $\\sigma$ is a non-linear function such as ReLU.\nSymbol Meaning $(h_i^\\ell$) feature of node i at layer‚ÄØ($\\ell$) $(\\mathcal{N}(i))$ neighbours of i $(U^\\ell, V^\\ell)$ learnable weights ($\\sigma)$ non‚Äëlinearity (e.g., ReLU) Stacking layers lets information flow across the whole graph.\nüß©‚ÄØWhere Transformers Meet GNNs Replace the plain sum with a weighted sum via attention ‚Üí you get a Graph Attention Network (GAT).\nAdd layer‚Äënorm and an MLP, and voil√†‚ÄØ‚Äî‚ÄØa Graph‚ÄØTransformer!\nWhat we have with transformers instead, is this, for an hidden feature $h$:\n$$h_{i}^{\\ell+1} = \\text{Attention} \\left( Q^{\\ell} h_{i}^{\\ell} \\ , K^{\\ell} h_{j}^{\\ell} \\ , V^{\\ell} h_{j}^{\\ell} \\right)$$ with: $$\\ h_{i}^{\\ell+1} = \\sum_{j \\in \\mathcal{S}} w_{ij} \\left( V^{\\ell} h_{j}^{\\ell} \\right)$$ $$ \\text{where} \\ w_{ij} = \\text{softmax}_j \\left ( Q^{\\ell} h^{\\ell}_i \\cdot K^{\\ell} h^{\\ell}_j \\right)$$\nüìù‚ÄØSentences as Graphs‚ÄîBut with Caveats Think of a sentence as a fully‚Äëconnected graph where every word links to every other.\nTransformers = GNNs with multi‚Äëhead attention acting as the aggregation rule.\nYet fully connected graphs mean quadratic growth in edges, which makes learning very long‚Äërange word relations hard.\nü§î‚ÄØAre Transformers Learning Neural Syntax? Studies suggest attention heads latch onto task‚Äëspecific syntax:\nAttention can surface the most relevant word pairs in a sentence. Different heads specialise in different syntactic cues. Graph‚Äëtheoretic view: can GNNs on full graphs reveal which edges matter most by inspecting the aggregation weights? This might expose the hidden structure driving model accuracy.\nüìö‚ÄØReferences Chaitanya‚ÄØK.‚ÄØJoshi, ‚ÄúTransformers are Graph Neural Networks,‚Äù The‚ÄØGradient‚ÄØ(2020). üé•‚ÄØYouTube‚ÄØTalk ","permalink":"https://ldomenichelli.github.io/posts/post7/","summary":"\u003ch3 id=\"representationlearning-for-nlp\"\u003eüèóÔ∏è‚ÄØRepresentation‚ÄØLearning for NLP\u003c/h3\u003e\n\u003cp\u003eAll neural‚Äënetwork (NN) architectures create \u003cstrong\u003evector‚ÄØrepresentations\u003c/strong\u003e‚Äîalso called \u003cem\u003eembeddings\u003c/em\u003e‚Äîof the input.\u003cbr\u003e\nThese vectors pack statistical \u003cem\u003eand\u003c/em\u003e semantic cues that let the model classify, translate, or generate text.\u003c/p\u003e\n\u003cp\u003eThe network \u003cem\u003elearns\u003c/em\u003e better representations through feedback from a \u003cstrong\u003eloss function\u003c/strong\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eTransformers\u003c/strong\u003e build features for each word with an \u003cstrong\u003eattention mechanism\u003c/strong\u003e that asks:\u003cbr\u003e\n\u003cem\u003e‚ÄúHow important is every other word in the sentence to this word?‚Äù\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch4 id=\"gnnsrepresenting-graphs\"\u003eüîó‚ÄØGNNs‚ÄîRepresenting Graphs\u003c/h4\u003e\n\u003cp\u003eGraph Neural Networks (GNNs) or Graph Convolutional Networks (GCNs) embed \u003cstrong\u003enodes and edges\u003c/strong\u003e.\u003cbr\u003e\nThey rely on \u003cstrong\u003eneighbourhood aggregation‚ÄØ/‚ÄØmessage passing\u003c/strong\u003e:\u003c/p\u003e","title":"Geometric Deep Learning"},{"content":"To qualify as a distance, a measure must satisfy the following properties:\nSymmetry: $ d(P, Q) = d(Q, P) )$ Triangle inequality: $ d(P, Q) + d(Q, R) \\geq d(P, R) $ However, in practice, we often deal with weaker notions of distances, commonly referred to as divergences. Example: KL Divergence The Kullback-Leibler (KL) divergence is defined as: $$ D_{\\text{KL}}(P || Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx $$\nProperties of KL Divergence Not Symmetric: $$ D_{\\text{KL}}(P || Q) \\neq D_{\\text{KL}}(Q || P) $$ Infinite for Different Supports: $$ D_{\\text{KL}}(P || Q) \\to \\infty \\quad \\text{if } P \\text{ and } Q \\text{ have different supports.} $$\nAddressing These Challenges Solution 1: Smoothing Distributionsüí° To avoid the issue of different supports, one solution is to smooth the distributions to match their supports.\nSolution 2: Use a Different Divergenceüí° An alternative approach is to use a divergence that naturally handles different supports and adheres to desirable distance properties. One such measure is the Wasserstein Distance.\nWasserstein Distance The Wasserstein distance, rooted in optimal transport theory, addresses the shortcomings of KL divergence by offering:\nSymmetry Triangle inequality A meaningful geometry of the space of distributions. Intuition Behind Optimal Transport The Wasserstein distance can be understood as the minimum \u0026ldquo;cost\u0026rdquo; to transform one distribution into another. Imagine redistributing the \u0026ldquo;mass\u0026rdquo; of one distribution $P$ to match another distribution $Q$. Each unit of mass has a transportation cost proportional to the distance it is moved.\nFormally, the $p$-Wasserstein distance is defined as:\n$$ W_p(P, Q) = \\left( \\inf_{\\gamma \\in \\Pi(P, Q)} \\int | x - y |^p d\\gamma(x, y) \\right)^{1/p} $$ Here:\n$ \\Pi(P, Q) $: The set of all couplings (joint distributions) with marginals $P$ and $Q$. $ | x - y | $: The cost of moving mass from $x$ to $y$. Properties of Wasserstein Distance Captures the geometric relationship between distributions. Finite even for distributions with disjoint supports. Offers meaningful insights in contexts like generative modeling and comparing empirical distributions. Optimal transport and the Wasserstein distance are widely used in fields such as:\nMachine Learning: Generative models (e.g., GANs with Wasserstein loss). Economics: Resource allocation problems. Physics: Modeling fluid dynamics. Image Processing: Comparing distributions of pixel intensities. By leveraging the principles of optimal transport, we gain a robust and versatile framework for comparing and transforming probability distributions.\nMore in written notes: ","permalink":"https://ldomenichelli.github.io/posts/post2/","summary":"\u003cp\u003eTo qualify as a \u003cstrong\u003edistance\u003c/strong\u003e, a measure must satisfy the following properties:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eSymmetry\u003c/strong\u003e: $ d(P, Q) = d(Q, P) )$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTriangle inequality\u003c/strong\u003e: $ d(P, Q) + d(Q, R) \\geq d(P, R) $\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"however-in-practice-we-often-deal-with-weaker-notions-of-distances-commonly-referred-to-as-divergences\"\u003eHowever, in practice, we often deal with \u003cstrong\u003eweaker notions of distances\u003c/strong\u003e, commonly referred to as \u003cstrong\u003edivergences\u003c/strong\u003e.\u003c/h2\u003e\n\u003ch3 id=\"example-kl-divergence\"\u003eExample: KL Divergence\u003c/h3\u003e\n\u003cp\u003eThe Kullback-Leibler (KL) divergence is defined as:\n$$\nD_{\\text{KL}}(P || Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx\n$$\u003c/p\u003e\n\u003ch4 id=\"properties-of-kl-divergence\"\u003eProperties of KL Divergence\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eNot Symmetric\u003c/strong\u003e:\n$$\nD_{\\text{KL}}(P || Q) \\neq D_{\\text{KL}}(Q || P)\n$$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInfinite for Different Supports\u003c/strong\u003e:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e$$\nD_{\\text{KL}}(P || Q) \\to \\infty \\quad \\text{if } P \\text{ and } Q \\text{ have different supports.}\n$$\u003c/p\u003e","title":"Optimal Transport üï∑Ô∏è and Wasserstein distance"},{"content":"L‚Äôuomo beve il T√® perch√© lo angoscia l‚Äôuomo. Il T√® beve l‚Äôuomo, l‚Äôerba pi√π amara.\n-Guido Ceronetti\nTea transcends being just a beverage‚Äîit\u0026rsquo;s a ritual, a tradition, and a bridge between nature and culture. With over 3,000 varieties, tea offers an incredible diversity of flavors, aromas, and benefits. Despite its vastness, every true tea originates from one remarkable plant: Camellia sinensis. The wide variety arises from how the leaves are processed, combined, or infused. This guide delves into the unique categories of true teas, mixed teas, and herbal tisanes, as well as the distinctions between Japanese and Chinese tea traditions. Steps to produce tea (stolen from infograph)\nTrue Teas: the Classics True teas are made solely from the Camellia sinensis plant. What sets each type apart is the processing technique, which influences the flavor, color, and aroma. Below is a detailed comparison of the six main types of true teas:\nTea Type Processing Highlights Flavor Profile Health Benefits Popular Varieties White Tea Minimally processed; only withering and drying. Light, floral, and mildly sweet. High in antioxidants, supports skin and immunity. Silver Needle, Bai Mudan. Yellow Tea Gently heated, slow oxidation wrapped in cloth. Smooth, sweet, with honey-like notes. Rare; aids digestion, boosts focus. Huo Shan Huang Ya. Green Tea Oxidation halted early using steaming or pan-firing. Grassy, fresh, occasionally nutty. Boosts metabolism, brain health, and heart health. Sencha, Matcha, Dragon Well. Oolong Tea Partially oxidized; rolled repeatedly for complexity. Floral, fruity, and nutty. Reduces cholesterol, aids digestion. Tieguanyin, Da Hong Pao. Black Tea Fully oxidized for a robust flavor and dark color. Strong, malty, and bold. Improves energy, supports heart health. Assam, Darjeeling, Keemun. Pu-erh Tea Fermented and aged, often for years. Earthy, rich, and woody. Aids digestion, lowers cholesterol, boosts gut health. Sheng Pu-erh, Shou Pu-erh. Green Tea ü´ñ Green tea is celebrated for its vibrant flavor, delicate processing, and scientifically-backed health benefits. Unlike black or oolong teas, green tea undergoes minimal oxidation. Processing typically involves steaming (common in Japanese teas) or pan-firing (typical in Chinese teas) to halt oxidation and preserve natural antioxidants like catechins. This careful crafting ensures green tea retains its signature grassy, vegetal, or nutty character.\nGreen Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Sencha Japan Steamed; preserves chlorophyll and freshness Fresh, mildly sweet, slightly grassy Boosts metabolism, enhances energy, and supports cardiovascular health. Matcha Japan Shade-grown; ground into a fine powder Creamy, vegetal, umami-rich High in antioxidants; promotes focus, detoxification, and relaxation. Dragon Well China Pan-fired; leaves flattened into iconic shape Nutty, smooth, subtly sweet Encourages relaxation, supports heart health, and aids digestion. Gunpowder Green China Leaves rolled into small, tight pellets Bold, slightly smoky, robust Improves energy, promotes digestion, and has antioxidative effects. Jasmine Green China Scented with jasmine blossoms Floral, sweet, aromatic Calms the mind, reduces stress, and supports skin health with antioxidant-rich polyphenols. Oolong Tea üåø Oolong tea occupies a unique position between green and black teas, combining the fresh, floral notes of the former with the robust, complex flavors of the latter. This partially oxidized tea is prized for its intricate processing, which involves repeated rolling, shaping, and drying of the leaves. The oxidation level of oolong tea can range from 10% to 80%, creating a diverse spectrum of flavors and aromas.\nThe careful crafting of oolong emphasizes its layered profile. Rolling and firing the leaves multiple times during production intensifies the complexity, giving oolong its characteristic floral, fruity, and nutty notes. Different regions and methods yield distinct types, offering a broad range of sensory experiences.\nOolong Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Tieguanyin China (Anxi, Fujian) Lightly oxidized; tightly rolled, jade-green leaves Floral, creamy, sweet Boosts skin health, supports digestion, and reduces stress. Da Hong Pao China (Wuyi Mountains) Heavily oxidized; roasted for depth Roasted, woody, slightly mineral Promotes heart health, improves energy, and lowers cholesterol. Oriental Beauty Taiwan Naturally oxidized by leafhoppers; less rolled Fruity, honey-like, mellow Rich in antioxidants; supports metabolism and enhances relaxation. Milk Oolong Taiwan Lightly oxidized; steamed for creaminess Buttery, smooth, subtly floral Aids in hydration, improves focus, and provides a soothing experience. Phoenix Dan Cong China (Guangdong) Medium oxidized; leaves twisted into long shapes Fruity, floral, and aromatic Supports gut health, aids weight management, and calms the nervous system. Oolong tea\u0026rsquo;s allure lies in its balance‚Äîa harmony between freshness and depth, floral lightness and roasted warmth. It invites tea enthusiasts to explore its range, from the creamy smoothness of Milk Oolong to the bold richness of Da Hong Pao. Whether sipped for relaxation or paired with food, oolong tea is a testament to the artistry and science of tea-making.\nBlack Tea Black tea is the most oxidized of all true teas, resulting in its signature dark color and robust flavor. During production, the leaves are fully oxidized after being withered and rolled, a process that intensifies their malty, brisk, and sometimes sweet notes. This oxidation also enhances the development of theaflavins and thearubigins, compounds responsible for black tea\u0026rsquo;s characteristic taste and many of its health benefits.\nWith its bold profile and high caffeine content compared to green or white tea, black tea has become a staple in cultures worldwide, whether as a standalone beverage or as a base for blends like chai or Earl Grey.\nBlack Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Assam India (Assam Valley) Fully oxidized; rolled for even processing Strong, malty, brisk Boosts energy, supports cardiovascular health, and improves focus. Darjeeling India (Darjeeling) Lightly oxidized compared to other black teas Floral, muscatel, slightly astringent Rich in antioxidants; aids digestion and supports immune health. Keemun China (Anhui Province) Slowly oxidized; carefully dried Smooth, smoky, slightly sweet Reduces stress, promotes relaxation, and enhances heart health. Ceylon Sri Lanka Fully oxidized; grown at varying altitudes Bold, citrusy, and brisk Improves digestion, boosts energy, and supports metabolism. Lapsang Souchong China (Fujian Province) Smoked over pinewood fires Smoky, rich, and earthy Provides warmth, reduces inflammation, and promotes relaxation. Black tea represents strength, both in flavor and character. Its ability to harmonize with other ingredients while standing strong on its own makes it a versatile and enduring favorite. From the malty richness of Assam to the smoky intrigue of Lapsang Souchong, black tea offers a bold sensory experience steeped in tradition and global significance.\nWhite Tea: The Purest Brew White tea is the least processed of all true teas, known for its delicate flavor and light, airy characteristics. Harvested primarily as young buds and leaves, it undergoes minimal oxidation, with processing typically limited to gentle withering and drying. This careful handling allows white tea to retain a high concentration of polyphenols, particularly catechins, and its characteristic light, floral aroma.\nOften regarded as the most natural tea, white tea is celebrated for its subtlety and nuanced sweetness. It embodies simplicity, offering a refreshing and soothing experience that has been cherished for centuries.\nWhite Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Silver Needle (Bai Hao Yinzhen) China (Fujian Province) Handpicked young buds; minimally processed Light, sweet, floral High in antioxidants; supports skin health and reduces oxidative stress. White Peony (Bai Mudan) China (Fujian Province) Young buds with some leaves; sun-dried Fruity, floral, slightly robust Aids in relaxation, supports immune function, and boosts heart health. Shou Mei China (Fujian or Guangxi) Older leaves; naturally withered and dried Earthy, nutty, and full-bodied Promotes digestion, supports metabolism, and improves hydration. Darjeeling White India (Darjeeling) Lightly processed from young Darjeeling leaves Delicate, floral, with muscatel notes Enhances focus, reduces inflammation, and provides gentle energy. White tea is a testament to the beauty of simplicity. Its light, soothing nature makes it a perfect choice for moments of calm and introspection. Whether you savor the delicate sweetness of Silver Needle or the slightly robust notes of White Peony, white tea offers an unparalleled experience that bridges tradition and wellness.\nMixed Teas Mixed teas combine the foundation of true teas with additional ingredients, resulting in endless flavor possibilities. For example, Earl Grey is a black tea infused with bergamot oil, creating a citrusy aroma that has become a British staple. Meanwhile, Masala Chai, a spiced blend of black tea with cinnamon, cloves, and ginger, offers a warming, aromatic treat deeply rooted in Indian culture.\nMixed Tea Base Unique Ingredients Flavor Notes Earl Grey Black tea Bergamot oil Citrusy and floral. Masala Chai Black tea Spices: cinnamon, cardamom, cloves, ginger, milk Spicy, rich, and warming. Jasmine Tea Green or black Jasmine blossoms Lightly floral and sweet. Thai Iced Tea Black tea Sweetened condensed milk Sweet, creamy, and refreshing. Mint Tea Green tea Fresh mint leaves Cooling and refreshing. Lychee Tea Black tea Lychee fruit essence Tropical, fruity, and sweet. These blends highlight how tea can be endlessly customized, whether for cultural rituals or personal enjoyment.\nHerbal Tisanes: Beyond Camellia Sinensis Herbal tisanes are caffeine-free infusions made from flowers, fruits, herbs, or spices. Though not technically \u0026ldquo;tea,\u0026rdquo; they provide a world of flavors and wellness benefits. For example, Chamomile is renowned for its calming properties, making it a popular bedtime drink. Similarly, Hibiscus offers a tart, cranberry-like taste packed with vitamin C.\nHerbal Tisane Main Ingredient Flavor Profile Health Benefits Mate Yerba mate leaves Smoky and earthy. Boosts energy and focus naturally. Rooibos Rooibos plant (South Africa) Sweet and nutty. Rich in antioxidants, aids relaxation. Chamomile Chamomile flowers Light and floral. Promotes sleep, reduces anxiety. Hibiscus Hibiscus petals Tart and cranberry-like. Supports heart health, boosts immunity. Lemongrass Lemongrass stalks Citrusy and refreshing. Aids digestion, reduces inflammation. Japanese vs. Chinese Tea üë≤üèº While Japan and China share a long history of tea cultivation, their approaches highlight distinct cultural philosophies.\nTea traditions in Japan and China are deeply intertwined with their histories and cultural values, but their approaches to tea production and consumption reflect vastly different philosophies. While both nations share a reverence for tea, their practices diverge in ways that make each tradition distinct and uniquely beautiful.\nIn Japan, tea culture revolves almost entirely around green tea, celebrated for its fresh, grassy flavors. Japanese tea processing prioritizes a steaming method, which halts oxidation and preserves the vibrant green color of the leaves. This results in teas with clean, vegetal profiles and an umami richness. Matcha, a powdered green tea, stands at the heart of Japan\u0026rsquo;s iconic tea ceremony, where every gesture reflects mindfulness and harmony. Similarly, other green teas like sencha and gyokuro reflect Japan\u0026rsquo;s emphasis on simplicity and precision. Japanese tea farms are meticulously managed, often employing shading techniques that enhance sweetness and umami in the leaves. Modern Japan has also embraced convenience, with bottled green tea and matcha-flavored products widely available, ensuring tea remains part of everyday life.\nIn contrast, Chinese tea culture is vast and varied, encompassing green, white, oolong, black, and Pu-erh teas, each with its own regional specialties and processing techniques. Unlike Japan‚Äôs steaming process, Chinese teas are often pan-fried or baked, creating nutty, toasty, and floral flavors. For example, Dragon Well (Longjing) green tea has a smooth, roasted nuttiness, while oolong teas like Tieguanyin showcase intricate floral aromas. China\u0026rsquo;s tea production takes full advantage of its diverse geography, with each region contributing distinct flavors influenced by local soil and climate. Whether it‚Äôs the earthy complexity of Pu-erh from Yunnan or the refined elegance of Keemun black tea from Anhui, Chinese teas reflect the terroir of their origins.\nCulturally, Japanese tea is rooted in Zen philosophy, focusing on the meditative aspects of preparation and drinking. The Japanese tea ceremony, or chanoyu, emphasizes simplicity, quietness, and the spiritual connection between host and guest. In contrast, Chinese tea practices celebrate variety and experimentation. The gongfu tea ceremony, often performed with Yixing clay teapots or a gaiwan, focuses on extracting the perfect flavor through multiple infusions. Chinese tea culture encourages savoring the changing notes of the tea with each steeping, turning every session into a sensory exploration.\nEven the flavors differ fundamentally between the two traditions. Japanese teas tend to be grassy, umami-rich, and vegetal, with a focus on freshness. Matcha‚Äôs creamy, bittersweet profile embodies this characteristic perfectly, as does the clean, savory taste of gyokuro. On the other hand, Chinese teas span a broader spectrum, from the delicate sweetness of white teas to the smoky, earthy richness of Pu-erh. This diversity makes Chinese tea a journey of discovery, where each cup offers a new story.\nWhile Japan modernizes its tea industry with bottled teas and matcha lattes, China retains its traditional focus on loose-leaf teas and tea houses, where time slows down for the appreciation of aroma, texture, and flavor. Both cultures, however, uphold tea as a reflection of nature, craftsmanship, and human connection, reminding us that tea is far more than a drink‚Äîit is an experience.\nUltimately, Japanese and Chinese teas reflect their respective cultures‚Äô approaches to life: Japan‚Äôs emphasis on precision and purity contrasts beautifully with China‚Äôs celebration of diversity and depth. Whether you prefer the grassy umami of Japanese green tea or the complex, evolving flavors of Chinese oolong, both traditions invite you to explore the art of tea in your own way.\nAspect Japanese Tea Chinese Tea Primary Type Green tea (e.g., Matcha, Sencha) Green, white, oolong, black, Pu-erh Processing Steaming (preserves grassy notes) Pan-frying or baking (toasty, nutty notes) Cultural Focus Zen-inspired simplicity, mindfulness Variety and exploration of flavors Popular Ceremony Matcha-based tea ceremony (chanoyu) Gongfu ceremony (multiple infusions) Flavor Notes Grassy, umami-rich, fresh Broad range: floral, earthy, fruity ","permalink":"https://ldomenichelli.github.io/random/tea/","summary":"\u003cp\u003e\u003cem\u003eL‚Äôuomo beve il T√® perch√© lo angoscia l‚Äôuomo.\nIl T√® beve l‚Äôuomo, l‚Äôerba pi√π amara.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e-Guido Ceronetti\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eTea transcends being just a beverage‚Äîit\u0026rsquo;s a ritual, a tradition, and a bridge between nature and culture. With over \u003cstrong\u003e3,000 varieties\u003c/strong\u003e, tea offers an incredible diversity of flavors, aromas, and benefits. Despite its vastness, every true tea originates from one remarkable plant: \u003cstrong\u003eCamellia sinensis\u003c/strong\u003e. The wide variety arises from how the leaves are processed, combined, or infused. This guide delves into the unique categories of \u003cstrong\u003etrue teas\u003c/strong\u003e, \u003cstrong\u003emixed teas\u003c/strong\u003e, and \u003cstrong\u003eherbal tisanes\u003c/strong\u003e, as well as the distinctions between Japanese and Chinese tea traditions.\n\n\n\n\n\n    \n    \u003cinput type=\"checkbox\" id=\"zoomCheck-a3854\" hidden\u003e\n    \u003clabel for=\"zoomCheck-a3854\"\u003e\n        \u003cimg class=\"zoomCheck\" loading=\"lazy\" decoding=\"async\"\n            src=\"img/step.png\" alt=\"tea\"\n             /\u003e\n    \u003c/label\u003e\n\n\n\u003cem\u003eSteps to produce tea (stolen from infograph)\u003c/em\u003e\u003c/p\u003e","title":"Pensieri del t√® üçµ"},{"content":"Here are the slides and notes on the course hold by Prof. Chiaromonte at Sant\u0026rsquo;Anna University, Pisa. ","permalink":"https://ldomenichelli.github.io/posts/post8/","summary":"\u003cp\u003eHere are the slides and notes on the course hold by Prof. Chiaromonte at Sant\u0026rsquo;Anna University, Pisa.\n\u003cembed src=\"/SLLD_new.pdf\" width=\"100%\" height=\"800px\" type=\"application/pdf\"\u003e\u003c/p\u003e","title":"Statistical Learning and Large Data üìä"},{"content":"Predictive Models for Time Series Analysis Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.\n1. Introduction to Time Series Analysis A time series is typically defined as: $$ T = { x_1, x_2, \\dots, x_m } $$ where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.\nUnivariate Time Series: A single variable measured over time. For instance, daily temperature readings. Multivariate Time Series: Multiple variables (channels) measured simultaneously, e.g., temperature, pressure, and humidity recorded at the same timestamps. Applications Forecasting: Predict future values based on historical patterns (e.g., stock prices, energy consumption). Classification / Regression: Predict class labels (e.g., device failure vs. normal) or numeric values (e.g., how many products will be sold). Clustering: Group time series exhibiting similar behavior or patterns (e.g., grouping customers by purchasing trends). Anomaly Detection: Detect unusual events or patterns in time (e.g., sudden sensor spikes). Pattern Mining: Identify repeated motifs (recurring subsequences) or rare discords. 2. Time Series Analytics Tasks Classification\nExample: Identify whether an ECG signal indicates normal heart activity or arrhythmia. Often uses distance-based approaches (DTW), shapelets, or feature-based transformations. Regression\nExample: Predict the amount of rainfall based on historical climate data. Can be framed as forecasting (single-step ahead) or a standard regression if the target is derived from the same temporal data. Forecasting\nExample: Project sales for the next quarter. Usually requires modeling temporal dependence (ARIMA, exponential smoothing, deep learning, etc.). Clustering\nExample: Group power-consumption patterns from different households to find typical usage profiles. May use DTW-based distance or other specialized measures. Anomaly Detection\nExample: Spot sudden temperature spikes in a production line sensor. Often involves statistical thresholding, machine learning models, or reconstruction-based methods (e.g., autoencoders). Pattern Mining\nExample: Detect repeating patterns (motifs) or unusual subsequences (discords) in ECG signals. 3. Time Series Visualization Common goals when plotting a time series:\nTrend: Does the data consistently move up, down, or show a long-term drift? Periodicity: Are there regular cycles, such as daily or monthly fluctuations? Seasonality: A special case of periodicity, often tied to known phenomena (e.g., yearly temperature cycles). Heteroskedasticity: Variance that changes over time (e.g., volatility clusters in financial data). Outliers: Points or subsequences that deviate significantly from the majority. Visual techniques might include standard line charts, rolling averages, or advanced dashboards that allow zooming and panning.\n4. Handling Missing Values Time series often have missing data due to sensor outages, data corruption, or irregular sampling.\nFilling with a constant value:\nForward fill (pad): Use the last known observation until a new one appears. Backward fill: Use the next known observation for previous gaps. Mean/median: Replace missing with overall mean or median. Nearest known value: Often used when sampling frequency is high. Linear Interpolation\nConnect neighboring known values with a straight line and fill in the gap. Forecasting-based Interpolation\nTrain a model on the known data and predict the missing points. Helps when the data exhibits predictable trends or seasonality. Random Imputation\nImpute from the distribution of known data. Sometimes used for simulation or bootstrapping. Careful selection of an imputation strategy is crucial‚Äîincorrect handling can introduce bias or distort subsequent analyses.\n5. Time Series Anomalies (Outliers) Outliers are observations that deviate substantially from the general behavior of the data. In time series, outliers might be due to measurement errors, system malfunctions, or truly significant (and possibly critical) events.\nTypes of Outliers Point Outlier: A single data point that is unusually large or small. Subsequence Outlier: A contiguous segment showing atypical behavior. Instance Outlier: An entire time series that differs markedly from others in a dataset. Common Outlier Detection Methods Histogram/Boxplot\nQuick visual approach; points beyond typical whiskers or outside certain standard deviations. IQR Filter\nLower bound:\n$$ Q1 - 1.5 \\times \\mathrm{IQR} $$ Upper bound:\n$$ Q3 + 1.5 \\times \\mathrm{IQR} $$ Data beyond these bounds may be considered outliers. Hampel Filter\nUses median and Median Absolute Deviation (MAD): $$ I = [\\text{median} - 3 \\times \\text{MAD},; \\text{median} + 3 \\times \\text{MAD}] $$ Grubbs‚Äô Test\nIteratively detects one outlier at a time, assuming normality of data. Sometimes, domain knowledge is key to deciding whether to remove outliers or treat them as important signals.\n6. Normalizations Due to varying scales, offsets, or trends in time series data, normalization or transformation steps can be essential:\nOffset Translation\nMean Removal: Subtract the global mean from all values. Min‚ÄìMax Normalization: Scale data to a ([0,1]) or ([-1,1]) range. Amplitude Scaling\nZ-Score Normalization: ( (x - \\mu) / \\sigma ). Helps unify variance. Linear Trend Removal\nDetrending: Fit and subtract a linear or polynomial trend. Mean Smoothing\nMoving Average: Helps reduce short-term volatility and highlight trends. Log Transformations\nUseful for data with exponential growth or multiplicative seasonality. Differencing\nReplace each value ( x_t ) with ( x_t - x_{t-1} ). Stabilizes mean if a strong linear trend is present. 7. Time Series Components A time series can often be decomposed into:\nLevel (baseline or average) Trend (long-term increase or decrease) Seasonality (repetitive, cyclical patterns) Noise (random, unexplained fluctuations) Additive Model $$ Y_t = \\text{Level} + \\text{Trend} + \\text{Seasonality} + \\text{Noise} $$\nMultiplicative Model $$ Y_t = \\text{Level} \\times \\text{Trend} \\times \\text{Seasonality} \\times \\text{Noise} $$\nFor instance, monthly sales data might have a rising trend, a strong seasonal pattern (e.g., holiday peaks), and some residual noise.\n8. Stationarity A time series is stationary if its statistical properties (mean, variance, autocovariance) remain constant over time. Many classic time series models (e.g., ARIMA) assume stationarity.\nCriteria:\nConstant Mean Constant Variance Autocovariance depends only on lag (i.e.,\n$$ \\mathrm{Cov}(x_t, x_{t+h}) \\text{ is independent of } t $$ ). Making a Series Stationary Detrending (subtracting a fitted linear or polynomial trend) Log Transform (reduces multiplicative effects) Differencing (subtract consecutive observations to remove trends) Seasonal Decomposition (removing seasonal components) Stationarity Test:\nAugmented Dickey‚ÄìFuller (ADF): If p-value is below a chosen threshold (e.g., 0.05), likely the series is stationary. 9. Time Series Similarities (Distances) Shape-based Similarity Euclidean Distance: Simple, but sensitive to misalignments or variable speeds. Dynamic Time Warping (DTW): Allows stretching/compressing in time, aligning sequences that are similar but out of phase. Sakoe‚ÄìChiba Band or Itakura Parallelogram can constrain warping paths, reducing computation. Structural-based Similarity Focuses on comparing broader patterns (e.g., overall shape, location of peaks/valleys). 10. Time Series Approximations \u0026amp; Dimensionality Reduction For very long or high-frequency time series, approximation methods reduce storage/computational demands:\nPAA (Piecewise Aggregate Approximation)\nDivide series into fixed-size segments; each segment is represented by its mean.\nSAX (Symbolic Aggregate Approximation)\nPAA + discretization into symbols from a finite alphabet.\nDFT (Discrete Fourier Transform)\nDecompose series into sums of sinusoidal components.\nSFA (Symbolic Fourier Approximation)\nDFT + discretization.\nSVD / PCA\nDimensionality reduction capturing principal variations. Often used across a collection of time series (e.g., multiple sensors). 11. Classification \u0026amp; Regression Instance-based (Memory-based) k-NN uses a distance measure (e.g., DTW). May store training data in memory and compare new time series to nearest neighbors. Linear / Logistic Models Often require stationarity or specific feature engineering to handle temporal correlations. Tree-based Approaches Decision Trees, Random Forests, Gradient Boosted Trees, etc. Work well with tabular features extracted from time windows (though must consider autocorrelation). Ensemble Methods Bagging, Boosting Proximity Forest, Time Series Forest (specialized ensemble methods). Interval-based Methods Time Series Forest Random Interval Spectral Ensemble (RISE) Supervised Time Series Forest\nThese extract features (mean, variance, slope, etc.) from various intervals of a time series. 12. Dictionary-based and Shapelet-based Models Dictionary-based Approaches Convert time series into ‚Äúdocuments‚Äù of discrete symbols and then analyze them with bag-of-words or similar text mining techniques:\nBag of Patterns (BOP) Bag of SFA Symbols (BOSS) WEASEL (Word ExtrAction for time SEries cLassification) Shapelet-based Models A shapelet is a small subsequence that is highly representative or discriminative of a specific class.\nExtraction: Identify the most discriminative subsequences. Transformation: Convert each full time series to a vector of distances to shapelets. Classification: Train any standard classifier (e.g., SVM, Random Forest) on shapelet-distance features. 13. Multivariate Time Series Multiple channels measured simultaneously:\nIndependent Assumption: Model each channel separately if they don‚Äôt interact strongly. Concatenation: Flatten all channels into one univariate series (loses some cross-channel info). Advanced Methods: MUSE (extension of WEASEL) integrates multiple channels. Neural networks (e.g., LSTM) that handle multiple input features. 14. Deep Learning Methods CNNs\nExploit convolution + pooling layers to automatically learn local features from raw data.\nRNNs / LSTMs\nModel long-term dependencies, capturing temporal context across many time steps.\nInception Networks\nUse multi-scale filters in parallel (adapted from computer vision).\n![[Pasted image 20250409113111.png]]\nTapNet, Multivariate LSTM-FCN, etc.\nMerge RNNs/CNNs with fully-connected layers for robust feature extraction.\nKernel-based Models ROCKET (RandOm Convolutional KErnel Transform) MiniRocket, MultiRocket Hydra, MultiROCKET-Hydra They transform time series via numerous random convolutional kernels, then feed into a linear model. Offers strong performance with high efficiency.\n15. Hybrid Models HIVE-COTE (Hierarchical Vote Collective of Transformation-based Ensembles)\nCombines diverse transformation modules (e.g., shapelets, dictionary methods, intervals). TS-CHIEF (Time Series Combination of Heterogeneous and Integrated Embeddings Forest)\nRandomized decision trees using multiple embedded approaches. These ensembles often achieve state-of-the-art classification accuracy on benchmark datasets by combining complementary representations.\n16. Explainable AI With complex models (deep networks, large ensembles), interpretability can be challenging:\nFeature Attribution: Methods like Grad-CAM, integrated gradients, or saliency maps adapted to time series. Shapelet-based: Provides subsequences that are inherently interpretable. Surrogate Models: Train a simpler, interpretable model (e.g., decision tree) to mimic the predictions of a black-box. Rule Extraction: Derive approximate rules or patterns from complex models. 17. Time Series Forecasting Forecasting means predicting future observations from historical data. Errors are typically measured by:\nMAE (Mean Absolute Error) RMSE (Root Mean Squared Error) MAPE (Mean Absolute Percentage Error) Simple Methods Average Method: Forecast is the mean of all past data. Na√Øve Method: Forecast is just the last observed value. Drift Method: Linear extrapolation from the first to the last observed point. These serve as baselines. More sophisticated models can outperform them, but these are often used as references.\n18. Exponential Smoothing Family SES (Simple Exponential Smoothing)\nSuitable for data with no trend or seasonality. Recent observations get higher weights.\nHolt‚Äôs Method\nExtends SES with a trend component.\nHolt‚ÄìWinters Method\nIncludes seasonality (additive or multiplicative).\n19. ARIMA-based Models ARIMA (AutoRegressive Integrated Moving Average) captures autocorrelation in time series.\nAR(p) Model $$ y_t = c + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + e_t $$\nMA(q) Model $$ y_t = c + e_t + \\theta_1 e_{t-1} + \\dots + \\theta_q e_{t-q} $$\nARIMA(p,d,q) Incorporates differencing of order (d) to handle non-stationarity.\nSARIMA Incorporates seasonal terms:\nAutoARIMA can automatically select (p, d, q) (and seasonal orders). Prophet (by Facebook/Meta) is another robust tool that handles multiple seasonalities, holidays, regressors, etc. 20. Forecasting via Reduced Regression This ‚Äúreduction‚Äù approach converts forecasting into a supervised learning problem:\nChoose a window size ( w ). E.g., use the last ( w ) observations as features. Predict the next value (or multiple future values). Use standard regression methods: Random forests, linear regression, XGBoost, etc. For multi-step forecasting, one can repeat this approach or predict multiple future time steps at once.\n21. Forecasting via Deep Learning RNN/LSTM\nCapture long-term sequential dependencies. Variants (GRU, Bi-LSTM) may improve performance. Sequence-to-Sequence (Seq2Seq) Networks\nOriginally used for machine translation, can handle multi-step forecasting. Temporal Fusion Transformers\nCombine attention mechanisms with recurrent networks to handle complex time series with covariates. Neural networks excel with large datasets and can learn nonlinear patterns that simpler models might miss.\nAdditional Considerations Hyperparameter Tuning: Time series models often have multiple hyperparameters (e.g., ARIMA orders, number of hidden units in an LSTM). Automated searches like grid search or Bayesian optimization can help. Model Validation: Standard cross-validation splits might not apply directly because of the temporal order. Techniques like rolling forecasting origin or walk-forward validation preserve the time structure. Performance Metrics: Selecting metrics that align with business goals or practical considerations (e.g., if small absolute errors or relative errors matter more). Domain Knowledge: Often crucial in deciding how to handle missing data, outliers, or interpret model outputs. Summary Time series analysis and forecasting encompass a broad spectrum of methods, from simple baselines (Na√Øve, Average) and classic models (ARIMA, exponential smoothing) to advanced machine learning techniques (deep learning, shapelets, kernel-based ensembles). The choice of method depends on:\nData Characteristics: Stationarity, seasonality, presence of trends, magnitude of noise. Task Requirements: Single-step forecasting, anomaly detection, classification, etc. Computational Constraints: For large-scale or high-frequency data, efficient approximation or specialized algorithms may be needed. Explainability: Simpler models or shapelet-based approaches may provide clearer insights, while deep models might yield higher accuracy but be harder to interpret. Overall, success in time series analysis hinges on proper preprocessing (missing-value handling, outlier management, normalization, feature engineering) and a careful choice of models and validation strategies.\n","permalink":"https://ldomenichelli.github.io/posts/post9/","summary":"\u003ch1 id=\"predictive-models-for-time-series-analysis\"\u003ePredictive Models for Time Series Analysis\u003c/h1\u003e\n\u003cp\u003eTime series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"1-introduction-to-time-series-analysis\"\u003e1. Introduction to Time Series Analysis\u003c/h2\u003e\n\u003cp\u003eA \u003cstrong\u003etime series\u003c/strong\u003e is typically defined as:\n$$\nT = { x_1, x_2, \\dots, x_m }\n$$\nwhere each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.\u003c/p\u003e","title":"Time Series üìà"},{"content":"Here are my notes from the lectures on \u0026ldquo;Topological Data Analysis\u0026rdquo; that was held from Prof. Patrizio Frosini at UniPi in Jan 2025.\n","permalink":"https://ldomenichelli.github.io/posts/post4/","summary":"\u003cp\u003eHere are my notes from the lectures on \u0026ldquo;Topological Data Analysis\u0026rdquo; that was held from Prof. Patrizio Frosini at UniPi in Jan 2025.\u003c/p\u003e\n\u003cembed src=\"/tdafull.pdf\" width=\"100%\" height=\"800px\" type=\"application/pdf\"\u003e","title":"Topological Data Analysis üåê"}]