[{"content":"What is Abalone? Have you ever tried pushing your opponent off a cliff… in a friendly way? If that sounds intriguing, let me introduce you to Abalone, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent Lévi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\nThe game’s name comes from the abalone, a type of mollusk known for its \u0026ldquo;ear-shaped\u0026rdquo; shell. The Italian name for the game, aliotide, combines the Latin prefix \u0026ldquo;ab-\u0026rdquo; (meaning \u0026ldquo;from\u0026rdquo; or \u0026ldquo;away\u0026rdquo;) and the English word \u0026ldquo;alone,\u0026rdquo; hinting at the isolated struggle between two opponents.\nIn Abalone, two players compete on a hexagonal board with 14 marbles each: white for one player, black for the other. The aim is straightforward: be the first to push six of your opponent\u0026rsquo;s marbles off the board. But as you\u0026rsquo;ll soon discover, achieving this goal requires careful planning, tactical moves, and a deep understanding of positioning.\nGame Components and Setup The game is played on a hexagonal board with 61 circular positions arranged in rows:\nFigure 1: Board layout\nEach player starts with 14 marbles, and the initial setup (shown below) places these marbles in a specific formation, primed for strategic movement.\nFigure 2: Initial setup\nEach position on the board is labeled using a grid system with letters and numbers, allowing players to communicate moves easily.\nBasic Rules and Movements In Abalone, players take turns making a single move per turn. Here are the core rules:\nMoving a Marble: On your turn, you can move one marble to any adjacent, empty spot on the board. Line and Lateral Moves: You can also move a line of two or three marbles as long as they are aligned in the same direction. This can be done in two ways: In-Line Movement: Move all marbles in the line forward in the same direction. Lateral Movement: Shift all marbles in the line to the side without changing their orientation. Sumito (Pushing Marbles): If your marbles outnumber the adjacent marbles of your opponent in a line, you can push them. For instance, two marbles can push one, and three can push two. Pushing is only possible if there’s an empty spot behind the opposing marble(s) for them to move into. Special Positioning: \u0026ldquo;Pac\u0026rdquo; and Strategic Pushes The concept of pac is crucial for mastering Abalone. When white and black marbles are aligned in equal numbers, neither player can push the other (this creates a \u0026ldquo;pac\u0026rdquo; or standoff). This means that understanding numerical superiority and positioning is essential.\nFor example, in a scenario where three black marbles face three white marbles, no push is possible. However, a setup with three black marbles against two white marbles allows the black player to push forward. This rule creates opportunities to set up defenses and traps, making the game highly tactical.\nKey Strategies for Winning To succeed in Abalone, keep these strategies in mind:\nControl the Center: Marbles near the edges are at greater risk of being pushed out. Maintaining a central position allows flexibility and reduces the likelihood of getting cornered. Set Up Sumito Opportunities: Since pushing depends on having more marbles in a line, positioning your marbles strategically to outnumber opponents in key areas is vital. Avoid Isolation: Isolated marbles are easy targets for a Sumito. Keep your marbles grouped to maintain pushing power and defend against your opponent’s moves. Force a Pac: Sometimes, creating a standoff situation (pac) can disrupt your opponent’s plans, giving you time to reposition your marbles. Now, enjoy this 90\u0026rsquo;s commercial or play the game online while I\u0026rsquo;m writing the rest of the rules!\n","permalink":"https://ldomenichelli.github.io/games/abalone/","summary":"\u003ch2 id=\"what-is-abalone\"\u003eWhat is Abalone?\u003c/h2\u003e\n\u003cp\u003eHave you ever tried pushing your opponent off a cliff… in a friendly way? If that sounds intriguing, let me introduce you to \u003cem\u003eAbalone\u003c/em\u003e, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent Lévi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\u003c/p\u003e","title":"Abalone"},{"content":" Me fr fr.\nWelcome to my study space! I\u0026rsquo;m Lucia, a first-year PhD student in AI at UniPi.\nHere, I collect personal notes on various topics I’m learning. They’re written for me, but might be helpful to others, too. Enjoy reading!\n","permalink":"https://ldomenichelli.github.io/about/","summary":"Information about me.","title":"About this site"},{"content":"Achi: A Traditional African Game Achi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as Care), and others. It is classified among alignment games, which also include games like Tapatan, Tant Fant, Shisima, and Pong Hau K\u0026rsquo;i.\nObjective: The goal of Achi is to form an uninterrupted line of three pieces of the same color, placed on the same horizontal, vertical, or diagonal line. Once a player achieves this, the game ends immediately, and that player wins.\nEquipment: The game consists of:\nA board with 9 intersections (as shown in the diagram). 8 pieces: 4 white and 4 black. Pieces are placed on the intersections, not within the squares of the grid. Players: The game is played by two players, who take turns.\nRules: There are two main phases in the game: the Placement Phase and the Movement Phase.\n1. Placement Phase: During the Placement Phase, players take turns placing one piece at a time on any available intersection on the board. Pieces cannot be moved during this phase. This phase ends once all 8 pieces (4 for each player) have been placed on the board. After this phase, there will be only one empty intersection left on the board. The initial setup leaves the board empty, and either player can start. 2. Movement Phase: Once all pieces are placed, the game transitions into the Movement Phase. From the fifth move onwards, players can move their pieces. A piece may be moved to an adjacent intersection (orthogonally or diagonally), as long as the target intersection is empty. Players continue to move their pieces, aiming to align three of their pieces in a row (horizontally, vertically, or diagonally). The game ends as soon as a player forms a line of three pieces of their color, either during the Placement Phase or the Movement Phase. ","permalink":"https://ldomenichelli.github.io/games/achi/","summary":"\u003ch1 id=\"achi-a-traditional-african-game\"\u003eAchi: A Traditional African Game\u003c/h1\u003e\n\u003cp\u003eAchi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as \u003cem\u003eCare\u003c/em\u003e), and others. It is classified among alignment games, which also include games like \u003cem\u003eTapatan\u003c/em\u003e, \u003cem\u003eTant Fant\u003c/em\u003e, \u003cem\u003eShisima\u003c/em\u003e, and \u003cem\u003ePong Hau K\u0026rsquo;i\u003c/em\u003e.\u003c/p\u003e","title":"Achi"},{"content":"L’uomo beve il Tè perché lo angoscia l’uomo. Il Tè beve l’uomo, l’erba più amara.\n-Guido Ceronetti\nTea transcends being just a beverage—it\u0026rsquo;s a ritual, a tradition, and a bridge between nature and culture. With over 3,000 varieties, tea offers an incredible diversity of flavors, aromas, and benefits. Despite its vastness, every true tea originates from one remarkable plant: Camellia sinensis. The wide variety arises from how the leaves are processed, combined, or infused. This guide delves into the unique categories of true teas, mixed teas, and herbal tisanes, as well as the distinctions between Japanese and Chinese tea traditions.\nTrue Teas: the Classics True teas are made solely from the Camellia sinensis plant. What sets each type apart is the processing technique, which influences the flavor, color, and aroma. Below is a detailed comparison of the six main types of true teas:\nTea Type Processing Highlights Flavor Profile Health Benefits Popular Varieties White Tea Minimally processed; only withering and drying. Light, floral, and mildly sweet. High in antioxidants, supports skin and immunity. Silver Needle, Bai Mudan. Yellow Tea Gently heated, slow oxidation wrapped in cloth. Smooth, sweet, with honey-like notes. Rare; aids digestion, boosts focus. Huo Shan Huang Ya. Green Tea Oxidation halted early using steaming or pan-firing. Grassy, fresh, occasionally nutty. Boosts metabolism, brain health, and heart health. Sencha, Matcha, Dragon Well. Oolong Tea Partially oxidized; rolled repeatedly for complexity. Floral, fruity, and nutty. Reduces cholesterol, aids digestion. Tieguanyin, Da Hong Pao. Black Tea Fully oxidized for a robust flavor and dark color. Strong, malty, and bold. Improves energy, supports heart health. Assam, Darjeeling, Keemun. Pu-erh Tea Fermented and aged, often for years. Earthy, rich, and woody. Aids digestion, lowers cholesterol, boosts gut health. Sheng Pu-erh, Shou Pu-erh. Green Tea 🫖 Green tea is celebrated for its vibrant flavor, delicate processing, and scientifically-backed health benefits. Unlike black or oolong teas, green tea undergoes minimal oxidation. Processing typically involves steaming (common in Japanese teas) or pan-firing (typical in Chinese teas) to halt oxidation and preserve natural antioxidants like catechins. This careful crafting ensures green tea retains its signature grassy, vegetal, or nutty character.\nGreen Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Sencha Japan Steamed; preserves chlorophyll and freshness Fresh, mildly sweet, slightly grassy Boosts metabolism, enhances energy, and supports cardiovascular health. Matcha Japan Shade-grown; ground into a fine powder Creamy, vegetal, umami-rich High in antioxidants; promotes focus, detoxification, and relaxation. Dragon Well China Pan-fired; leaves flattened into iconic shape Nutty, smooth, subtly sweet Encourages relaxation, supports heart health, and aids digestion. Gunpowder Green China Leaves rolled into small, tight pellets Bold, slightly smoky, robust Improves energy, promotes digestion, and has antioxidative effects. Jasmine Green China Scented with jasmine blossoms Floral, sweet, aromatic Calms the mind, reduces stress, and supports skin health with antioxidant-rich polyphenols. Oolong Tea 🌿 Oolong tea occupies a unique position between green and black teas, combining the fresh, floral notes of the former with the robust, complex flavors of the latter. This partially oxidized tea is prized for its intricate processing, which involves repeated rolling, shaping, and drying of the leaves. The oxidation level of oolong tea can range from 10% to 80%, creating a diverse spectrum of flavors and aromas.\nThe careful crafting of oolong emphasizes its layered profile. Rolling and firing the leaves multiple times during production intensifies the complexity, giving oolong its characteristic floral, fruity, and nutty notes. Different regions and methods yield distinct types, offering a broad range of sensory experiences.\nOolong Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Tieguanyin China (Anxi, Fujian) Lightly oxidized; tightly rolled, jade-green leaves Floral, creamy, sweet Boosts skin health, supports digestion, and reduces stress. Da Hong Pao China (Wuyi Mountains) Heavily oxidized; roasted for depth Roasted, woody, slightly mineral Promotes heart health, improves energy, and lowers cholesterol. Oriental Beauty Taiwan Naturally oxidized by leafhoppers; less rolled Fruity, honey-like, mellow Rich in antioxidants; supports metabolism and enhances relaxation. Milk Oolong Taiwan Lightly oxidized; steamed for creaminess Buttery, smooth, subtly floral Aids in hydration, improves focus, and provides a soothing experience. Phoenix Dan Cong China (Guangdong) Medium oxidized; leaves twisted into long shapes Fruity, floral, and aromatic Supports gut health, aids weight management, and calms the nervous system. Oolong tea\u0026rsquo;s allure lies in its balance—a harmony between freshness and depth, floral lightness and roasted warmth. It invites tea enthusiasts to explore its range, from the creamy smoothness of Milk Oolong to the bold richness of Da Hong Pao. Whether sipped for relaxation or paired with food, oolong tea is a testament to the artistry and science of tea-making.\nBlack Tea Black tea is the most oxidized of all true teas, resulting in its signature dark color and robust flavor. During production, the leaves are fully oxidized after being withered and rolled, a process that intensifies their malty, brisk, and sometimes sweet notes. This oxidation also enhances the development of theaflavins and thearubigins, compounds responsible for black tea\u0026rsquo;s characteristic taste and many of its health benefits.\nWith its bold profile and high caffeine content compared to green or white tea, black tea has become a staple in cultures worldwide, whether as a standalone beverage or as a base for blends like chai or Earl Grey.\nBlack Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Assam India (Assam Valley) Fully oxidized; rolled for even processing Strong, malty, brisk Boosts energy, supports cardiovascular health, and improves focus. Darjeeling India (Darjeeling) Lightly oxidized compared to other black teas Floral, muscatel, slightly astringent Rich in antioxidants; aids digestion and supports immune health. Keemun China (Anhui Province) Slowly oxidized; carefully dried Smooth, smoky, slightly sweet Reduces stress, promotes relaxation, and enhances heart health. Ceylon Sri Lanka Fully oxidized; grown at varying altitudes Bold, citrusy, and brisk Improves digestion, boosts energy, and supports metabolism. Lapsang Souchong China (Fujian Province) Smoked over pinewood fires Smoky, rich, and earthy Provides warmth, reduces inflammation, and promotes relaxation. Black tea represents strength, both in flavor and character. Its ability to harmonize with other ingredients while standing strong on its own makes it a versatile and enduring favorite. From the malty richness of Assam to the smoky intrigue of Lapsang Souchong, black tea offers a bold sensory experience steeped in tradition and global significance.\nWhite Tea: The Purest Brew White tea is the least processed of all true teas, known for its delicate flavor and light, airy characteristics. Harvested primarily as young buds and leaves, it undergoes minimal oxidation, with processing typically limited to gentle withering and drying. This careful handling allows white tea to retain a high concentration of polyphenols, particularly catechins, and its characteristic light, floral aroma.\nOften regarded as the most natural tea, white tea is celebrated for its subtlety and nuanced sweetness. It embodies simplicity, offering a refreshing and soothing experience that has been cherished for centuries.\nWhite Tea Varieties Variety Origin Processing Technique Flavor Profile Health Benefits Silver Needle (Bai Hao Yinzhen) China (Fujian Province) Handpicked young buds; minimally processed Light, sweet, floral High in antioxidants; supports skin health and reduces oxidative stress. White Peony (Bai Mudan) China (Fujian Province) Young buds with some leaves; sun-dried Fruity, floral, slightly robust Aids in relaxation, supports immune function, and boosts heart health. Shou Mei China (Fujian or Guangxi) Older leaves; naturally withered and dried Earthy, nutty, and full-bodied Promotes digestion, supports metabolism, and improves hydration. Darjeeling White India (Darjeeling) Lightly processed from young Darjeeling leaves Delicate, floral, with muscatel notes Enhances focus, reduces inflammation, and provides gentle energy. White tea is a testament to the beauty of simplicity. Its light, soothing nature makes it a perfect choice for moments of calm and introspection. Whether you savor the delicate sweetness of Silver Needle or the slightly robust notes of White Peony, white tea offers an unparalleled experience that bridges tradition and wellness.\nMixed Teas Mixed teas combine the foundation of true teas with additional ingredients, resulting in endless flavor possibilities. For example, Earl Grey is a black tea infused with bergamot oil, creating a citrusy aroma that has become a British staple. Meanwhile, Masala Chai, a spiced blend of black tea with cinnamon, cloves, and ginger, offers a warming, aromatic treat deeply rooted in Indian culture.\nMixed Tea Base Unique Ingredients Flavor Notes Earl Grey Black tea Bergamot oil Citrusy and floral. Masala Chai Black tea Spices: cinnamon, cardamom, cloves, ginger, milk Spicy, rich, and warming. Jasmine Tea Green or black Jasmine blossoms Lightly floral and sweet. Thai Iced Tea Black tea Sweetened condensed milk Sweet, creamy, and refreshing. Mint Tea Green tea Fresh mint leaves Cooling and refreshing. Lychee Tea Black tea Lychee fruit essence Tropical, fruity, and sweet. These blends highlight how tea can be endlessly customized, whether for cultural rituals or personal enjoyment.\nHerbal Tisanes: Beyond Camellia Sinensis Herbal tisanes are caffeine-free infusions made from flowers, fruits, herbs, or spices. Though not technically \u0026ldquo;tea,\u0026rdquo; they provide a world of flavors and wellness benefits. For example, Chamomile is renowned for its calming properties, making it a popular bedtime drink. Similarly, Hibiscus offers a tart, cranberry-like taste packed with vitamin C.\nHerbal Tisane Main Ingredient Flavor Profile Health Benefits Mate Yerba mate leaves Smoky and earthy. Boosts energy and focus naturally. Rooibos Rooibos plant (South Africa) Sweet and nutty. Rich in antioxidants, aids relaxation. Chamomile Chamomile flowers Light and floral. Promotes sleep, reduces anxiety. Hibiscus Hibiscus petals Tart and cranberry-like. Supports heart health, boosts immunity. Lemongrass Lemongrass stalks Citrusy and refreshing. Aids digestion, reduces inflammation. Japanese vs. Chinese Tea 👲🏼 While Japan and China share a long history of tea cultivation, their approaches highlight distinct cultural philosophies.\nTea traditions in Japan and China are deeply intertwined with their histories and cultural values, but their approaches to tea production and consumption reflect vastly different philosophies. While both nations share a reverence for tea, their practices diverge in ways that make each tradition distinct and uniquely beautiful.\nIn Japan, tea culture revolves almost entirely around green tea, celebrated for its fresh, grassy flavors. Japanese tea processing prioritizes a steaming method, which halts oxidation and preserves the vibrant green color of the leaves. This results in teas with clean, vegetal profiles and an umami richness. Matcha, a powdered green tea, stands at the heart of Japan\u0026rsquo;s iconic tea ceremony, where every gesture reflects mindfulness and harmony. Similarly, other green teas like sencha and gyokuro reflect Japan\u0026rsquo;s emphasis on simplicity and precision. Japanese tea farms are meticulously managed, often employing shading techniques that enhance sweetness and umami in the leaves. Modern Japan has also embraced convenience, with bottled green tea and matcha-flavored products widely available, ensuring tea remains part of everyday life.\nIn contrast, Chinese tea culture is vast and varied, encompassing green, white, oolong, black, and Pu-erh teas, each with its own regional specialties and processing techniques. Unlike Japan’s steaming process, Chinese teas are often pan-fried or baked, creating nutty, toasty, and floral flavors. For example, Dragon Well (Longjing) green tea has a smooth, roasted nuttiness, while oolong teas like Tieguanyin showcase intricate floral aromas. China\u0026rsquo;s tea production takes full advantage of its diverse geography, with each region contributing distinct flavors influenced by local soil and climate. Whether it’s the earthy complexity of Pu-erh from Yunnan or the refined elegance of Keemun black tea from Anhui, Chinese teas reflect the terroir of their origins.\nCulturally, Japanese tea is rooted in Zen philosophy, focusing on the meditative aspects of preparation and drinking. The Japanese tea ceremony, or chanoyu, emphasizes simplicity, quietness, and the spiritual connection between host and guest. In contrast, Chinese tea practices celebrate variety and experimentation. The gongfu tea ceremony, often performed with Yixing clay teapots or a gaiwan, focuses on extracting the perfect flavor through multiple infusions. Chinese tea culture encourages savoring the changing notes of the tea with each steeping, turning every session into a sensory exploration.\nEven the flavors differ fundamentally between the two traditions. Japanese teas tend to be grassy, umami-rich, and vegetal, with a focus on freshness. Matcha’s creamy, bittersweet profile embodies this characteristic perfectly, as does the clean, savory taste of gyokuro. On the other hand, Chinese teas span a broader spectrum, from the delicate sweetness of white teas to the smoky, earthy richness of Pu-erh. This diversity makes Chinese tea a journey of discovery, where each cup offers a new story.\nWhile Japan modernizes its tea industry with bottled teas and matcha lattes, China retains its traditional focus on loose-leaf teas and tea houses, where time slows down for the appreciation of aroma, texture, and flavor. Both cultures, however, uphold tea as a reflection of nature, craftsmanship, and human connection, reminding us that tea is far more than a drink—it is an experience.\nUltimately, Japanese and Chinese teas reflect their respective cultures’ approaches to life: Japan’s emphasis on precision and purity contrasts beautifully with China’s celebration of diversity and depth. Whether you prefer the grassy umami of Japanese green tea or the complex, evolving flavors of Chinese oolong, both traditions invite you to explore the art of tea in your own way.\nAspect Japanese Tea Chinese Tea Primary Type Green tea (e.g., Matcha, Sencha) Green, white, oolong, black, Pu-erh Processing Steaming (preserves grassy notes) Pan-frying or baking (toasty, nutty notes) Cultural Focus Zen-inspired simplicity, mindfulness Variety and exploration of flavors Popular Ceremony Matcha-based tea ceremony (chanoyu) Gongfu ceremony (multiple infusions) Flavor Notes Grassy, umami-rich, fresh Broad range: floral, earthy, fruity ","permalink":"https://ldomenichelli.github.io/random/tea/","summary":"\u003cp\u003e\u003cem\u003eL’uomo beve il Tè perché lo angoscia l’uomo.\nIl Tè beve l’uomo, l’erba più amara.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e-Guido Ceronetti\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eTea transcends being just a beverage—it\u0026rsquo;s a ritual, a tradition, and a bridge between nature and culture. With over \u003cstrong\u003e3,000 varieties\u003c/strong\u003e, tea offers an incredible diversity of flavors, aromas, and benefits. Despite its vastness, every true tea originates from one remarkable plant: \u003cstrong\u003eCamellia sinensis\u003c/strong\u003e. The wide variety arises from how the leaves are processed, combined, or infused. This guide delves into the unique categories of \u003cstrong\u003etrue teas\u003c/strong\u003e, \u003cstrong\u003emixed teas\u003c/strong\u003e, and \u003cstrong\u003eherbal tisanes\u003c/strong\u003e, as well as the distinctions between Japanese and Chinese tea traditions.\u003c/p\u003e","title":"Pensieri del Tè 🍵"},{"content":"Quantum Computing - Intro Quantum computing leverages the principles of quantum mechanics to solve problems that are difficult or impossible for classical computers. In Python, quantum computing can be explored through various frameworks and libraries, such as IBM\u0026rsquo;s Qiskit and Google\u0026rsquo;s Cirq.\nImport essential components from Qiskit, for handling quantum circuits and visualization.\nfrom qiskit import QuantumCircuit, Aer, execute from qiskit.visualization import plot_bloch_vector In quantum computing, a qubit is the fundamental unit of quantum information. You can create a quantum circuit with a single qubit like this:\nqc = QuantumCircuit(1) Quantum gates manipulate qubits, changing their states. Quantum gates are represented as unitary matrices that transform qubit states in vector form.\n$$ U^\\dagger U = U U^\\dagger = I $$\nWhere:\n$ U^\\dagger $ is the conjugate transpose (Hermitian adjoint) of the matrix $U$. $I$ is the identity matrix of the same dimension as $U$. Unitary matrices preserve the length (norm) of quantum states, which is essential for ensuring that quantum computations are physically meaningful.\nBasic Quantum Gates Pauli-X Gate The X-gate, akin to the classical NOT gate, flips a qubit’s state.\nMatrix Representation:\n$$ X = \\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{pmatrix} $$ Action: Transforms$|0\\rangle$ to $|1\\rangle$ and vice versa. Pauli-Y and Pauli-Z Gates The Y and Z gates provide further qubit manipulation along different axes.\nY-Gate Matrix:\n$$ Y = \\begin{pmatrix} 0 \u0026amp; -i \\ i \u0026amp; 0 \\end{pmatrix} $$ Z-Gate Matrix:\n$$ Z = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix} $$\nEffect: The Y-gate performs a phase rotation, while the Z-gate flips the phase of the ($|1\\rangle$ state. Hadamard Gate (H) The Hadamard gate creates a superposition from a basis state, equalizing the probabilities of measuring $|0\\rangle$ or $|1\\rangle$.\nMatrix Representation:\n$$ H = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; -1 \\end{pmatrix} $$ Effect: Transforms$|0\\rangle$ into\n$$ \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}} $$ a superposition state. Phase Gate (S and T) The Phase Gate (S) and T-Gate add specific phase rotations, essential in quantum Fourier transforms.\nS-Gate Matrix:\n$$ S = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; i \\end{pmatrix} $$ T-Gate Matrix:\n$$ T = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; e^{i\\pi/4} \\end{pmatrix} $$\nPurpose: These gates modify the phase of qubits, supporting interference and entanglement. 3. Multi-Qubit Gates CNOT Gate The Controlled NOT (CNOT) gate acts on two qubits, flipping the second qubit (target) if the first qubit (control) is $|1\\rangle$.\nMatrix Representation:\n$$ CNOT = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} $$\nFunction: Creates entanglement, foundational for quantum cryptography and algorithms like Grover’s and Shor’s. Toffoli and SWAP Gates These gates expand control capabilities in multi-qubit systems.\nToffoli (CCNOT): Three-qubit gate; flips the third qubit if the first two are (|1\\rangle). SWAP Gate: Exchanges the states of two qubits. ","permalink":"https://ldomenichelli.github.io/random/quantum/","summary":"\u003ch2 id=\"quantum-computing---intro\"\u003eQuantum Computing - Intro\u003c/h2\u003e\n\u003cp\u003eQuantum computing leverages the principles of quantum mechanics to solve problems that are difficult or impossible for classical computers. In Python, quantum computing can be explored through various frameworks and libraries, such as IBM\u0026rsquo;s Qiskit and Google\u0026rsquo;s Cirq.\u003c/p\u003e\n\u003cp\u003eImport essential components from Qiskit, for handling quantum circuits and visualization.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom qiskit import QuantumCircuit, Aer, execute\nfrom qiskit.visualization import plot_bloch_vector\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn quantum computing, a qubit is the fundamental unit of quantum information. You can create a quantum circuit with a single qubit like this:\u003c/p\u003e","title":"Quantum Computing - Intro "},{"content":"Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\nThe Role of Dimensionality Reduction Dimensionality reduction has gained prominence in machine learning and data science for its ability to simplify complex datasets while preserving essential structure. Methods in this domain are broadly categorized into:\nLinear Techniques:\nPrincipal Component Analysis (PCA) and its variants (e.g., Bouveyron et al. 2011) focus on orthogonal transformations to project data onto a lower-dimensional space. Non-linear Techniques (Manifold Learning):\nThese methods include Isometric Mapping (Tenenbaum et al. 2000), Locally Linear Embedding (Roweis and Saul 2000), and Hessian Eigenmapping (Donoho and Grimes 2003). They aim to capture the underlying structure of data that lies on a non-linear manifold. While most methods require users to specify a target dimension, some techniques adaptively infer it based on the dataset\u0026rsquo;s intrinsic dimensionality. This adaptability underscores the significance of ID estimation. Models and Methods for Estimating Intrinsic Dimensionality Over the years, researchers have developed diverse models to estimate intrinsic dimensionality. These fall into several categories:\nTopological Approaches: Analyze the tangent space of the manifold using local samples (e.g., Fukunaga and Olsen 1971; Verveer and Duin 1995).\nFractal Measures: Use metrics like the Correlation Dimension (Faloutsos and Kamel 1994) to estimate ID based on space-filling properties of data.\nGraph-based Methods: Employ $k$-nearest neighbors and density metrics to infer ID (Costa and Hero 2004).\nParametric Models: Leverage statistical models to estimate ID, such as those by Levina and Bickel (2004).\nGlobal vs. Local Intrinsic Dimensionality Intrinsic dimensionality measures can be broadly classified into global and local approaches:\nGlobal Measures: Analyze the dataset as a whole, treating all objects uniformly. These measures are well-suited for homogeneous datasets with a single dominant manifold.\nLocal Measures: Focus on the $k$-nearest neighbors of a specific point. These methods are essential for heterogeneous datasets comprising multiple, overlapping manifolds. Notable local ID models include:\nExpansion Dimension (ED) (Karger and Ruhl 2002). Generalized Expansion Dimension (GED) (Houle et al. 2012). Local Intrinsic Dimensionality (LID) (Houle 2013). Local ID measures are particularly relevant in applications such as similarity search, where they can estimate query complexity or optimize search termination. They are also applied in outlier detection and density estimation.\nBalancing Local and Global Insights Machine learning techniques often face challenges like overfitting when relying heavily on local information. To mitigate this, methods such as Local Tangent Space Alignment (LTSA) (Zhang and Zha 2004) combine local and global perspectives by aligning neighborhoods of points while penalizing overfitting during optimization. This balance enables a more comprehensive understanding of the data structure.\nContinuous Intrinsic Dimension In this section, we explore Local Intrinsic Dimensionality (LID), a model that extends intrinsic dimensionality to continuous distributions of distances, as proposed by Houle (2013). LID quantifies the local intrinsic dimensionality (ID) of a feature space by focusing exclusively on the distribution of inter-point distances.\nDefining the Distribution of Distances Let $(\\mathbb{R}^m, \\text{dist})$ represent a domain equipped with a non-negative distance function dist. Consider the distribution of distances with respect to a fixed reference point. This distribution can be modeled as a random variable $\\mathbf{X}$, with support $[0, \\infty)$. The probability density function (PDF) of $\\mathbf{X}$ is denoted by $f_{\\mathbf{X}}$, where $f_{\\mathbf{X}}$ is a non-negative, Lebesgue-integrable function. For any $a, b \\in [0, \\infty)$ such that $a \\leq b$, the probability is given by:\n$$ \\operatorname{Pr}[a \\leq \\mathbf{X} \\leq b] = \\int_a^b f_{\\mathbf{X}}(x) , \\mathrm{d}x. $$\nThe corresponding cumulative density function (CDF) $F_{\\mathbf{X}}$ is defined as:\n$$ F_{\\mathbf{X}}(x) = \\operatorname{Pr}[\\mathbf{X} \\leq x] = \\int_0^x f_{\\mathbf{X}}(u) , \\mathrm{d}u. $$\nFor values where $\\mathbf{X}$ is absolutely continuous at $x$, the CDF $F_{\\mathbf{X}}$ is differentiable at $x$, and its first-order derivative is $f_{\\mathbf{X}}(x)$.\nThe Local Continuous Intrinsic Dimension The local intrinsic dimension at distance $x$ is defined as follows:\nDefinition 1 (Houle, 2013):\nGiven an absolutely continuous random distance variable $\\mathbf{X}$, for any distance threshold $x$ such that $F_{\\mathbf{X}}(x) \u0026gt; 0$, the local continuous intrinsic dimension $\\mathrm{ID}_{\\mathbf{X}}(x)$ of $\\mathbf{X}$ at distance $x$ is:\n$$ \\mathrm{ID}{\\mathbf{X}}(x) \\triangleq \\lim{\\epsilon \\to 0^+} \\frac{\\ln F_{\\mathbf{X}}((1+\\epsilon) x) - \\ln F_{\\mathbf{X}}(x)}{\\ln (1+\\epsilon)} $$\nwherever the limit exists.\nRelation to the Generalized Expansion Dimension The LID definition builds upon the generalized expansion dimension (GED) proposed by Houle et al. (2012a). GED measures dimensionality by comparing neighborhood radii $x$ and $(1+\\epsilon)x$, replacing neighborhood cardinalities with the expected number of neighbors.\nIn essence, the LID formulation quantifies the discriminative power of a distance measure. Both LID and GED share the same closed-form representation, reflecting their foundational equivalence in characterizing local dimensionality.\nTheorem 1 (Houle 2013) Let X be an absolutely continuous random distance variable. If $F_X$ is both positive and differentiable at x, then:\n$$ \\text{ID}_X(x) = \\frac{x f_X(x)}{F_X(x)}. $$\nLocal intrinsic dimensionality (Local ID) has potential for wide application thanks to its very general treatment of distances as a continuous random variable. Direct estimation of $ID_X(x)$, however, requires the knowledge of the distribution of $X$. Extreme value theory, which we survey in the following section, allows the estimation of the limit of $x \\to 0$ without any explicit assumptions of the data distribution other than continuity.\n3. Extreme Value Theory Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes.\nDefinition 2 Let $\\mu \\in R$ and $\\sigma \u0026gt; 0$. The family of generalized extreme value distributions $F_{GEV}$ covers distributions whose cumulative distribution functions have the form:\n$$ F_{GEV} = \\exp \\left( - \\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-\\frac{1}{\\xi}} \\right) \u0026amp; \\text{if } \\xi \\neq 0, \\ \\exp \\left( -\\exp \\left( -\\frac{x - \\mu}{\\sigma} \\right) \\right) \u0026amp; \\text{if } \\xi = 0. $$\nA distribution $G \\in F_{GEV}$ has support:\n$$ \\text{supp}(G) = \\begin{cases} [\\mu - \\frac{\\sigma}{\\xi}, \\infty) \u0026amp; \\text{when } \\xi \u0026gt; 0, \\ (-\\infty, \\mu - \\frac{\\sigma}{\\xi}] \u0026amp; \\text{when } \\xi \u0026lt; 0, \\ (-\\infty, \\infty) \u0026amp; \\text{if } \\xi = 0. \\end{cases} $$\nIts best-known theorem, attributed in parts to Fisher and Tippett (1928), and Gnedenko (1943), states that the maximum of n independent identically-distributed random variables (after proper renormalization) converges in distribution to a generalized extreme value distribution as $n \\to \\infty$ .\nTheorem 2 (Fisher-Tippett-Gnedenko)\nLet $(X_i)_{i \\in N}$ be a sequence of independent identically-distributed random variables and let $M_n=max X_i$ If there exist a sequence of positive constants $a_n$, $n \\in N$ and a sequence of constants $b_n$ , $n \\in N$ , such that:\n$$ \\lim_{n \\to \\infty} P\\left( \\frac{M_n - b_n}{a_n} \\leq x \\right) = F(x), $$\nthen F(x) belongs to the generalized extreme value family.\nIsotropy A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportion al to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\nOther measures Avg cosine similarity : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution. Partition Isotropy score : Proposed by Arora et al. $$Z(c):= \\sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$ Intrinsic Dimensionality : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy. Linear dimensionality estimate: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions. *Non linear dimensionality estimate: - [MLE] \u0026ndash;\u0026gt; Levina2005\n- [Moment\u0026rsquo;s Method]\u0026ndash;\u0026gt;() - [Two_NN] Variance Explained ratio: measures how much total variance is explained by the first $k$ principal components of data. It requieres an a priori number of PC to examine. Estimating Intrinsic Dimension of a Dataset by MLE from skdim.id import MLE Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 \u0026hellip; X_n$ in $\\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\\mathbb{R}^m$ , with unknown $m \\leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\\mathbb{R}^m$ are mapped to close neighbors in the embedding.\nThe basic idea is to fix a point $x$ , assume $f(x) \\sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:\n$$ N(t, x) = \\sum_{i=1}^n \\mathbf{1} {X_i \\in S_x(t)} $$\nwhich counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\\lambda(t)$ of the process $N(t)$ as:\n$$ \\lambda(t) = f(x) V(m) m t^{m-1} $$\nThis follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \\frac{d}{dt} \\left[ V(m) t^m \\right]$ is the surface area of the sphere $S_x(t)$. Letting $\\theta = \\log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:\n$$ L(m, \\theta) = \\int_0^R \\log \\lambda(t) , d N(t) - \\int_0^R \\lambda(t) , dt $$\nThis is an exponential family for which MLEs exist with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$ and are unique. The MLEs must satisfy the likelihood equations\n$$ \\frac{\\partial L}{\\partial \\theta} = \\int_0^R d N(t) - \\int_0^R \\lambda(t) , dt = N(R) - e^\\theta V(m) R^m = 0, $$\n$$ \\frac{\\partial L}{\\partial m} = \\left( \\frac{1}{m} + \\frac{V^{\\prime}(m)}{V(m)} \\right) N(R) + \\int_0^R \\log t , d N(t)\ne^\\theta V(m) R^m \\left( \\log R + \\frac{V^{\\prime}(m)}{V(m)} \\right) = 0. $$ Substituting we get:\n$$ m_R(x) = \\left[ \\frac{1}{N(R, x)} \\sum_{j=1}^{N(R, x)} \\log \\frac{R}{T_j(x)} \\right]^{-1}. $$\nIn practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:\n$$ m_k(x) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\log \\frac{T_k(x)}{T_j(x)} \\right]^{-1} $$\nNote that we omit the last (zero) term in the sum. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),\nFor some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. We will assume that all the data points come from the same \u0026ldquo;manifold,\u0026rdquo; and therefore average over all observations.\nThe choice of $k$ clearly affects the estimate. It can be the case that a dataset has different intrinsic dimensions at different scales, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.\n$$ m_k = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}_k(X_i) $$\n$$ m = \\frac{1}{k_2 - k_1 + 1} \\sum_{k=k_1}^{k_2} \\hat{m}_k. $$\nThe only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.\nEstimating Intrinsic Dimension of a Dataset by NN Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \\ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.\nThe volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by $$ \\Delta v_l=\\omega_d\\left(r_l^d-r_{l-1}^d\\right), $$ where $d$ is the dimensionality of the space in which the points are embedded and $\\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\\rho$ : $$ P\\left(\\Delta v_l \\in[v, v+d v]\\right)=\\rho e^{-\\rho v} d v $$\nConsider two shells $\\Delta v_1$ and $\\Delta v_2$, and let $R$ be the quantity $\\frac{\\Delta v_i}{\\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :\n$$ P(R \\in[\\bar{R}, \\bar{R}+d \\bar{R}]) = \\int_0^{\\infty} d v_i \\int_0^{\\infty} d v_j \\rho^2 e^{-\\rho\\left(v_i+v_j\\right)} \\left{\\frac{v_j}{v_i} \\in[\\bar{R}, \\bar{R}+d \\bar{R}]\\right} = d \\bar{R} \\frac{1}{(1+\\bar{R})^2}. $$\nwhere 1 represents the indicator function. Dividing by $d \\bar{R}$ we obtain the pdf for $R$ : $$ g(R)=\\frac{1}{(1+R)^2} $$\nThe pdf does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\\mu \\doteq \\frac{r_2}{r_1} \\in[1,+\\infty) . R$ and $\\mu$ are related by equality: $$ R=\\mu^d-1 $$\nThis equation allows to find an explicit formula for the distribution of $\\mu$ : $$ f(\\mu)=d \\mu^{-d-1} 1_{[1,+\\infty]}(\\mu), $$ while the cumulative distribution (cdf) is obtained by integration: $$ F(\\mu)=\\left(1-\\mu^{-d}\\right) 1_{[1,+\\infty]}(\\mu) . $$\nFunctions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.\nThe derivation presented above leads to a simple observation. The value of the intrinsic dimension $d$ can be estimated through the following equation: $$ \\frac{\\log (1-F(\\mu))}{\\log (\\mu)}=d $$\nRemarkably the density $\\rho$ does not appear in this equation, since the $\\operatorname{cdf} F$ is independent of $\\rho$. If we consider the set $S \\subset \\mathbb{R}^2, S \\doteq{(\\log (\\mu),-\\log (1-F(\\mu)))}$, $S$ is contained in a straight line $l \\doteq{(x, y) \\mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:\nCompute the pairwise distances for each point in the dataset $i=1, \\ldots, N$. For each point i find the two shortest distances $r_1$ and $r_2$. For each point i compute $\\mu_i=\\frac{r_2}{r_1}$. Compute the empirical cumulate $F^{e m p}(\\mu)$ by sorting the values of $\\mu$ in an ascending order through a permutation $\\sigma$, then define $F^{e m p}\\left(\\mu_{\\sigma(i)}\\right) \\doteq \\frac{i}{N}$. Fit the points of the plane given by coordinates ${\\ (\\log(\\mu_i), -\\log(1 - F^{\\text{emp}}(\\mu_i))) \\ | \\ i = 1, \\ldots, N\\ }$ with a straight line passing through the origin. Even if the results above are derived in the case of a uniform distribution of points there is no dependence on the density $\\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.\nEstimating Intrinsic Dimension of a Dataset by Moment\u0026rsquo;s Method from skdim.id import TwoNN Properties of Isotropy Mean Agnosticism:\nIsotropy is a property solely of the covariance matrix of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.\nInvariance to Scalar Multiples of the Covariance Matrix:\nIsotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\\Sigma=Cov(I)$ where $I$ is the identity matrix and $\\lambda \u0026gt;0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the shape of the distribution rather than the overall size of the data spread. Thus, for an isotropic distribution, $Cov(\\lambda)=Cov(\\lambda I)$.\nVariance Distribution Across Dimensions:\nFor a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.\nRotation Invariance:\nAn ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.\nUtilization of Dimensions:\nThere is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.\nGlobal Stability:\nAn isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.\n","permalink":"https://ldomenichelli.github.io/posts/post1/","summary":"\u003cp\u003eUnderstanding the \u003cstrong\u003ecurse of dimensionality\u003c/strong\u003e requires more than just examining the representational aspect of data. A key insight lies in the concept of \u003cstrong\u003eintrinsic dimensionality (ID)\u003c/strong\u003e—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\u003c/p\u003e","title":"Space uniformity"}]