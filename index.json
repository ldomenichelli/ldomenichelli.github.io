[{"content":"What is Abalone? Have you ever tried pushing your opponent off a cliff… in a friendly way? If that sounds intriguing, let me introduce you to Abalone, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent Lévi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\nThe game’s name comes from the abalone, a type of mollusk known for its \u0026ldquo;ear-shaped\u0026rdquo; shell. The Italian name for the game, aliotide, combines the Latin prefix \u0026ldquo;ab-\u0026rdquo; (meaning \u0026ldquo;from\u0026rdquo; or \u0026ldquo;away\u0026rdquo;) and the English word \u0026ldquo;alone,\u0026rdquo; hinting at the isolated struggle between two opponents.\nIn Abalone, two players compete on a hexagonal board with 14 marbles each: white for one player, black for the other. The aim is straightforward: be the first to push six of your opponent\u0026rsquo;s marbles off the board. But as you\u0026rsquo;ll soon discover, achieving this goal requires careful planning, tactical moves, and a deep understanding of positioning.\nGame Components and Setup The game is played on a hexagonal board with 61 circular positions arranged in rows:\nFigure 1: Board layout\nEach player starts with 14 marbles, and the initial setup (shown below) places these marbles in a specific formation, primed for strategic movement.\nFigure 2: Initial setup\nEach position on the board is labeled using a grid system with letters and numbers, allowing players to communicate moves easily.\nBasic Rules and Movements In Abalone, players take turns making a single move per turn. Here are the core rules:\nMoving a Marble: On your turn, you can move one marble to any adjacent, empty spot on the board. Line and Lateral Moves: You can also move a line of two or three marbles as long as they are aligned in the same direction. This can be done in two ways: In-Line Movement: Move all marbles in the line forward in the same direction. Lateral Movement: Shift all marbles in the line to the side without changing their orientation. Sumito (Pushing Marbles): If your marbles outnumber the adjacent marbles of your opponent in a line, you can push them. For instance, two marbles can push one, and three can push two. Pushing is only possible if there’s an empty spot behind the opposing marble(s) for them to move into. Special Positioning: \u0026ldquo;Pac\u0026rdquo; and Strategic Pushes The concept of pac is crucial for mastering Abalone. When white and black marbles are aligned in equal numbers, neither player can push the other (this creates a \u0026ldquo;pac\u0026rdquo; or standoff). This means that understanding numerical superiority and positioning is essential.\nFor example, in a scenario where three black marbles face three white marbles, no push is possible. However, a setup with three black marbles against two white marbles allows the black player to push forward. This rule creates opportunities to set up defenses and traps, making the game highly tactical.\nKey Strategies for Winning To succeed in Abalone, keep these strategies in mind:\nControl the Center: Marbles near the edges are at greater risk of being pushed out. Maintaining a central position allows flexibility and reduces the likelihood of getting cornered. Set Up Sumito Opportunities: Since pushing depends on having more marbles in a line, positioning your marbles strategically to outnumber opponents in key areas is vital. Avoid Isolation: Isolated marbles are easy targets for a Sumito. Keep your marbles grouped to maintain pushing power and defend against your opponent’s moves. Force a Pac: Sometimes, creating a standoff situation (pac) can disrupt your opponent’s plans, giving you time to reposition your marbles. Now, enjoy this 90\u0026rsquo;s commercial or play the game online while I\u0026rsquo;m writing the rest of the rules!\n","permalink":"https://ldomenichelli.github.io/games/abalone/","summary":"\u003ch2 id=\"what-is-abalone\"\u003eWhat is Abalone?\u003c/h2\u003e\n\u003cp\u003eHave you ever tried pushing your opponent off a cliff… in a friendly way? If that sounds intriguing, let me introduce you to \u003cem\u003eAbalone\u003c/em\u003e, a strategy board game where you literally push your way to victory! Created in 1987 by Michel Lalet and Laurent Lévi, this two-player game is a unique blend of simplicity and depth that has earned it a spot in the hearts of board game lovers worldwide.\u003c/p\u003e","title":"Abalone"},{"content":" Me fr fr.\nWelcome to my study space! I\u0026rsquo;m Lucia, a first-year PhD student in AI at UniPi.\nHere, I collect personal notes on various topics I’m learning. They’re written for me, but might be helpful to others, too. Enjoy reading!\n","permalink":"https://ldomenichelli.github.io/about/","summary":"Information about me.","title":"About this site"},{"content":"Achi: A Traditional African Game Achi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as Care), and others. It is classified among alignment games, which also include games like Tapatan, Tant Fant, Shisima, and Pong Hau K\u0026rsquo;i.\nObjective: The goal of Achi is to form an uninterrupted line of three pieces of the same color, placed on the same horizontal, vertical, or diagonal line. Once a player achieves this, the game ends immediately, and that player wins.\nEquipment: The game consists of:\nA board with 9 intersections (as shown in the diagram). 8 pieces: 4 white and 4 black. Pieces are placed on the intersections, not within the squares of the grid. Players: The game is played by two players, who take turns.\nRules: There are two main phases in the game: the Placement Phase and the Movement Phase.\n1. Placement Phase: During the Placement Phase, players take turns placing one piece at a time on any available intersection on the board. Pieces cannot be moved during this phase. This phase ends once all 8 pieces (4 for each player) have been placed on the board. After this phase, there will be only one empty intersection left on the board. The initial setup leaves the board empty, and either player can start. 2. Movement Phase: Once all pieces are placed, the game transitions into the Movement Phase. From the fifth move onwards, players can move their pieces. A piece may be moved to an adjacent intersection (orthogonally or diagonally), as long as the target intersection is empty. Players continue to move their pieces, aiming to align three of their pieces in a row (horizontally, vertically, or diagonally). The game ends as soon as a player forms a line of three pieces of their color, either during the Placement Phase or the Movement Phase. ","permalink":"https://ldomenichelli.github.io/games/achi/","summary":"\u003ch1 id=\"achi-a-traditional-african-game\"\u003eAchi: A Traditional African Game\u003c/h1\u003e\n\u003cp\u003eAchi is a traditional board game originating from Ghana, and is similar to the game of \u0026ldquo;Nine Men\u0026rsquo;s Morris\u0026rdquo; (or \u0026ldquo;Mill\u0026rdquo;), but it has a unique structure and rules. The game is widely played in several African countries, including Nigeria, Senegal (where it is known as \u003cem\u003eCare\u003c/em\u003e), and others. It is classified among alignment games, which also include games like \u003cem\u003eTapatan\u003c/em\u003e, \u003cem\u003eTant Fant\u003c/em\u003e, \u003cem\u003eShisima\u003c/em\u003e, and \u003cem\u003ePong Hau K\u0026rsquo;i\u003c/em\u003e.\u003c/p\u003e","title":"Achi"},{"content":"Quantum Computing - Intro Quantum computing leverages the principles of quantum mechanics to solve problems that are difficult or impossible for classical computers. In Python, quantum computing can be explored through various frameworks and libraries, such as IBM\u0026rsquo;s Qiskit and Google\u0026rsquo;s Cirq.\nImport essential components from Qiskit, for handling quantum circuits and visualization.\nfrom qiskit import QuantumCircuit, Aer, execute from qiskit.visualization import plot_bloch_vector In quantum computing, a qubit is the fundamental unit of quantum information. You can create a quantum circuit with a single qubit like this:\nqc = QuantumCircuit(1) Quantum gates manipulate qubits, changing their states. Quantum gates are represented as unitary matrices that transform qubit states in vector form.\n$$ U^\\dagger U = U U^\\dagger = I $$\nWhere:\n$ U^\\dagger $ is the conjugate transpose (Hermitian adjoint) of the matrix $U$. $I$ is the identity matrix of the same dimension as $U$. Unitary matrices preserve the length (norm) of quantum states, which is essential for ensuring that quantum computations are physically meaningful.\nBasic Quantum Gates Pauli-X Gate The X-gate, akin to the classical NOT gate, flips a qubit’s state.\nMatrix Representation:\n$$ X = \\begin{pmatrix} 0 \u0026amp; 1 \\ 1 \u0026amp; 0 \\end{pmatrix} $$ Action: Transforms$|0\\rangle$ to $|1\\rangle$ and vice versa. Pauli-Y and Pauli-Z Gates The Y and Z gates provide further qubit manipulation along different axes.\nY-Gate Matrix:\n$$ Y = \\begin{pmatrix} 0 \u0026amp; -i \\ i \u0026amp; 0 \\end{pmatrix} $$ Z-Gate Matrix:\n$$ Z = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; -1 \\end{pmatrix} $$\nEffect: The Y-gate performs a phase rotation, while the Z-gate flips the phase of the ($|1\\rangle$ state. Hadamard Gate (H) The Hadamard gate creates a superposition from a basis state, equalizing the probabilities of measuring $|0\\rangle$ or $|1\\rangle$.\nMatrix Representation:\n$$ H = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \u0026amp; 1 \\ 1 \u0026amp; -1 \\end{pmatrix} $$ Effect: Transforms$|0\\rangle$ into\n$$ \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}} $$ a superposition state. Phase Gate (S and T) The Phase Gate (S) and T-Gate add specific phase rotations, essential in quantum Fourier transforms.\nS-Gate Matrix:\n$$ S = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; i \\end{pmatrix} $$ T-Gate Matrix:\n$$ T = \\begin{pmatrix} 1 \u0026amp; 0 \\ 0 \u0026amp; e^{i\\pi/4} \\end{pmatrix} $$\nPurpose: These gates modify the phase of qubits, supporting interference and entanglement. 3. Multi-Qubit Gates CNOT Gate The Controlled NOT (CNOT) gate acts on two qubits, flipping the second qubit (target) if the first qubit (control) is $|1\\rangle$.\nMatrix Representation:\n$$ CNOT = \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{pmatrix} $$\nFunction: Creates entanglement, foundational for quantum cryptography and algorithms like Grover’s and Shor’s. Toffoli and SWAP Gates These gates expand control capabilities in multi-qubit systems.\nToffoli (CCNOT): Three-qubit gate; flips the third qubit if the first two are (|1\\rangle). SWAP Gate: Exchanges the states of two qubits. ","permalink":"https://ldomenichelli.github.io/random/quantum/","summary":"\u003ch2 id=\"quantum-computing---intro\"\u003eQuantum Computing - Intro\u003c/h2\u003e\n\u003cp\u003eQuantum computing leverages the principles of quantum mechanics to solve problems that are difficult or impossible for classical computers. In Python, quantum computing can be explored through various frameworks and libraries, such as IBM\u0026rsquo;s Qiskit and Google\u0026rsquo;s Cirq.\u003c/p\u003e\n\u003cp\u003eImport essential components from Qiskit, for handling quantum circuits and visualization.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom qiskit import QuantumCircuit, Aer, execute\nfrom qiskit.visualization import plot_bloch_vector\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn quantum computing, a qubit is the fundamental unit of quantum information. You can create a quantum circuit with a single qubit like this:\u003c/p\u003e","title":"Quantum Computing - Intro "},{"content":"Definition A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\nOther measures Average cosine similarity : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution. Partition Isotropy score : Proposed by Arora et al. $$Z(c):= \\sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$ Intrinsic Dimensionality : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy. Linear dimensionality estimate: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions. Non linear dimensionality estimate: - [MLE] \u0026ndash;\u0026gt; Levina2005\n- [Two_NN] Variance Explained ratio: measures how much total variance is explained by the first $k$ principal components of data. It requieres an a priori number of PC to examine. Estimating Intrinsic Dimension of a Dataset by MLE from skdim.id import MLE Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 \u0026hellip; X_n$ in $\\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\\mathbb{R}^m$ , with unknown $m \\leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\\mathbb{R}^m$ are mapped to close neighbors in the embedding.\nThe basic idea is to fix a point $x$ , assume $f(x) \\sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:\n$$ N(t, x) = \\sum_{i=1}^n \\mathbf{1} {X_i \\in S_x(t)} $$\nwhich counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\\lambda(t)$ of the process $N(t)$ as:\n$$ \\lambda(t) = f(x) V(m) m t^{m-1} $$\nThis follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \\frac{d}{dt} \\left[ V(m) t^m \\right]$ is the surface area of the sphere $S_x(t)$. Letting $\\theta = \\log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:\n$$ L(m, \\theta) = \\int_0^R \\log \\lambda(t) , d N(t) - \\int_0^R \\lambda(t) , dt $$\nThis is an exponential family for which MLEs exist with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$ and are unique. The MLEs must satisfy the likelihood equations\n$$ \\frac{\\partial L}{\\partial \\theta} = \\int_0^R d N(t) - \\int_0^R \\lambda(t) , dt = N(R) - e^\\theta V(m) R^m = 0, $$\n$$ \\frac{\\partial L}{\\partial m} = \\left( \\frac{1}{m} + \\frac{V^{\\prime}(m)}{V(m)} \\right) N(R) + \\int_0^R \\log t \\ d N(t) - e^\\theta V(m) R^m \\left( \\log R + \\frac{V^{\\prime}(m)}{V(m)} \\right) = 0. $$\nSubstituting we get:\n$$ m_R(x) = \\left[ \\frac{1}{N(R, x)} \\sum_{j=1}^{N(R, x)} \\log \\frac{R}{T_j(x)} \\right]^{-1}. $$\nIn practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:\n$$ m_k(x) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\log \\frac{T_k(x)}{T_j(x)} \\right]^{-1} $$\nNote that we omit the last (zero) term in the sum. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),\nFor some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. We will assume that all the data points come from the same \u0026ldquo;manifold,\u0026rdquo; and therefore average over all observations.\nThe choice of $k$ clearly affects the estimate. It can be the case that a dataset has different intrinsic dimensions at different scales, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.\n$$ m_k = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}_k(X_i) $$\n$$ m = \\frac{1}{k_2 - k_1 + 1} \\sum_{k=k_1}^{k_2} \\hat{m}_k. $$\nThe only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.\nEstimating Intrinsic Dimension of a Dataset by NN Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \\ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.\nThe volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by $$ \\Delta v_l=\\omega_d\\left(r_l^d-r_{l-1}^d\\right), $$ where $d$ is the dimensionality of the space in which the points are embedded and $\\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\\rho$ : $$ P\\left(\\Delta v_l \\in[v, v+d v]\\right)=\\rho e^{-\\rho v} d v $$\nConsider two shells $\\Delta v_1$ and $\\Delta v_2$, and let $R$ be the quantity $\\frac{\\Delta v_i}{\\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :\n$P(R \\in[\\bar{R}, \\bar{R}+d \\bar{R}]) = \\int_0^{\\infty} d v_i \\int_0^{\\infty} d v_j \\rho^2 e^{-\\rho(v_i+v_j)} {\\frac{v_j}{v_i} \\in[\\bar{R}, \\bar{R}+d \\bar{R}]} = d \\bar{R} \\frac{1}{(1+\\bar{R})^2}$\nwhere 1 represents the indicator function. Dividing by $d \\bar{R}$ we obtain the pdf for $R$ : $$ g(R)=\\frac{1}{(1+R)^2} $$\nThe pdf does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\\mu \\doteq \\frac{r_2}{r_1} \\in[1,+\\infty) . R$ and $\\mu$ are related by equality: $$ R=\\mu^d-1 $$\nThis equation allows to find an explicit formula for the distribution of $\\mu$ : $$ f(\\mu)=d \\mu^{-d-1} 1_{[1,+\\infty]}(\\mu), $$ while the cumulative distribution (cdf) is obtained by integration: $$ F(\\mu)=\\left(1-\\mu^{-d}\\right) 1_{[1,+\\infty]}(\\mu) . $$\nFunctions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.\nThe derivation presented above leads to a simple observation. The value of the intrinsic dimension $d$ can be estimated through the following equation: $$ \\frac{\\log (1-F(\\mu))}{\\log (\\mu)}=d $$\nRemarkably the density $\\rho$ does not appear in this equation, since the $\\operatorname{cdf} F$ is independent of $\\rho$. If we consider the set $S \\subset \\mathbb{R}^2, S \\doteq{(\\log (\\mu),-\\log (1-F(\\mu)))}$, $S$ is contained in a straight line $l \\doteq{(x, y) \\mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:\nCompute the pairwise distances for each point in the dataset $i=1, \\ldots, N$. For each point i find the two shortest distances $r_1$ and $r_2$. For each point i compute $\\mu_i=\\frac{r_2}{r_1}$. Compute the empirical cumulate $F^{e m p}(\\mu)$ by sorting the values of $\\mu$ in an ascending order through a permutation $\\sigma$, then define $F^{e m p}\\left(\\mu_{\\sigma(i)}\\right) \\doteq \\frac{i}{N}$. Fit the points of the plane given by coordinates ${\\ (\\log(\\mu_i), -\\log(1 - F^{\\text{emp}}(\\mu_i))) \\ | \\ i = 1, \\ldots, N\\ }$ with a straight line passing through the origin. Even if the results above are derived in the case of a uniform distribution of points in equations (5) and (7) there is no dependence on the density $\\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.\nProperties of Isotropy Mean Agnosticism:\nIsotropy is a property solely of the covariance matrix of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.\nInvariance to Scalar Multiples of the Covariance Matrix:\nIsotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\\Sigma=Cov(I)$ where $I$ is the identity matrix and $\\lambda \u0026gt;0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the shape of the distribution rather than the overall size of the data spread. Thus, for an isotropic distribution, $Cov(\\lambda)=Cov(\\lambda I)$.\nVariance Distribution Across Dimensions:\nFor a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.\nRotation Invariance:\nAn ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.\nUtilization of Dimensions:\nThere is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.\nGlobal Stability:\nAn isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.\n","permalink":"https://ldomenichelli.github.io/posts/post1/","summary":"\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eA distribution is isotropic if its variance is uniformly distributed across all dimensions.\u003c/em\u003e Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in \u003cem\u003en-dimensional vector space\u003c/em\u003e  is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\u003c/p\u003e","title":"Space uniformity"}]