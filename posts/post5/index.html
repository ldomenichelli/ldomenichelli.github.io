<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>❄️ HPLT × NLPL Winter School | lucia's notes</title>
<meta name=keywords content="winter‑school,pretraining,factuality,LLMs,datasets"><meta name=description content="Short notes and slide links from the Winter School in Skeikampen, Norway (Feb 10‑14, 2025)."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post5/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="❄️ HPLT × NLPL Winter School"><meta property="og:description" content="Short notes and slide links from the Winter School in Skeikampen, Norway (Feb 10‑14, 2025)."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post5/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="❄️ HPLT × NLPL Winter School"><meta name=twitter:description content="Short notes and slide links from the Winter School in Skeikampen, Norway (Feb 10‑14, 2025)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"❄️ HPLT × NLPL Winter School","item":"https://ldomenichelli.github.io/posts/post5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"❄️ HPLT × NLPL Winter School","name":"❄️ HPLT × NLPL Winter School","description":"Short notes and slide links from the Winter School in Skeikampen, Norway (Feb 10‑14, 2025).","keywords":["winter‑school","pretraining","factuality","LLMs","datasets"],"articleBody":"🗓 Day 1 — Monday, 10 Feb 2025 🌍 What Is Common Crawl? Common Crawl is a huge, free snapshot of the public web.\nA non‑profit updates it every month, storing:\nBillions of HTML pages Their cleaned‑up text content Extra metadata (links, timestamps, MIME types, …) Why It Matters Track language change – see how words, memes, and topics shift over time. Map the web’s link network – study which sites connect and why. Train big ML models – use real‑world data instead of tiny toy datasets. Because each release includes both the raw HTML and a parsed text layer, you can analyze:\nLayer What you can study Raw HTML structure Link graphs, page layout, site categories Clean text content Sentiment, topic trends, new buzzwords Perks for Researchers No crawler needed – skip the cost and hassle of scraping the web yourself. Open licence – anyone can share code, replicate results, and build on your work. Regular updates – monthly snapshots reveal sudden spikes (e.g., when a new tech goes viral). All of this makes Common Crawl a go‑to resource for tasks like:\nNamed‑entity recognition Topic classification Question answering By pooling efforts around one massive, open dataset, researchers push the limits of NLP faster than they could alone.\n📍 Common Crawl 📍 Factuality \u0026 Hallucinations in LLMs Speaker: Anna Rogers (University of Copenhagen)\n“Large language models are fluent bullshit generators—they sound right even when they’re wrong.”\n— A. Rogers\nLLMs can drift from the truth, a problem known as hallucination. Rogers reviews two popular fixes and where they fall short:\nApproach How it works Main weakness RAG\n(Retrieval‑Augmented Generation) Looks up facts in a search index or database, then feeds the snippets to the model as it writes. Bad retrieval = bad answer; citations can be incorrect or missing. CoT\n(Chain‑of‑Thought prompting) Prompts the model to show step‑by‑step reasoning before the final answer. “Reasoning” may be invented; method can be abused to jailbreak the model. Impact on the Web Surge in AI‑generated spam and click‑bait Harder to tell real news from synthetic text New headaches for search engines and fact‑checkers Takeaway: RAG and CoT help, but they don’t eliminate hallucinations. Better evaluation metrics and stronger guardrails are still needed.\nRAG (Retrieval‑Augmented Generation) and CoT (Chain‑of‑Thought) both try to make LLM answers more trustworthy, but neither is a silver bullet.\n🔍 RAG — Look it up, then write How it works\nRetrieve Find facts in a search index or database. Generate Feed those facts to the model so it can weave them into its answer. Where it breaks\nIf the search misses the right passage, the answer is still wrong. Measuring “quality” is tricky: you need to score retrieval hit‑rate, answer truthfulness, and source fidelity—all at once. Evaluations often rely on yet another LLM, which can add bias. Even with good sources, the model may paraphrase or misquote them. 📝 CoT — Show your thinking How it works\nGive the model examples that spell out step‑by‑step reasoning. Ask it to copy that style: “First, think. Then, answer.” Where it breaks\nWorks great on some tasks, worse on others—especially biased ones. The “reasoning” it prints may be made up, not its true internal logic. Attackers can use CoT to slip past safety rules (“jailbreaking”). 🗓 Day 2 — Tuesday, 11 Feb 2025 📍 FineWeb 2 — Multilingual Web Data at Scale Speaker: Guilherme Penedo (Hugging Face)\nThe opening talk introduced FineWeb 2, a brand‑new, multilingual web corpus for pre‑training large language models.\nPenedo explained how the team is porting and tuning the English‑centric cleaning pipeline—deduplication, language ID, toxicity filters, and more—so it works reliably across dozens of other languages.\n📍 Power Laws \u0026 Generalization Speakers: Jenia Jitsev \u0026 Marianna Nezhurina\nThis session explored how power‑law scaling shows up in deep‑learning curves—and why turning those neat mathematical fits into real‑world “generalization scores” is harder than it looks. The speakers highlighted pitfalls such as noisy data, shifting task definitions, and compute limits that break the power‑law trend once models get big enough.\n## 📍 *Generalization* ","wordCount":"671","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post5/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>❄️ HPLT × NLPL Winter School</h1><div class=post-description>Short notes and slide links from the Winter School in Skeikampen, Norway (Feb 10‑14, 2025).</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#-what-is-commoncrawl>🌍 What Is <strong>Common Crawl</strong>?</a><ul><li><a href=#why-it-matters>Why It Matters</a></li><li><a href=#perks-for-researchers>Perks for Researchers</a></li></ul></li></ul></li><li><a href=#-common-crawl>📍 <em>Common Crawl</em></a></li><li><a href=#-factualityhallucinationsinllms>📍 Factuality & Hallucinations in LLMs</a><ul><li><ul><li><a href=#impact-on-the-web>Impact on the Web</a></li></ul></li><li><a href=#-rag--look-it-up-then-write>🔍 RAG — Look it up, then write</a></li><li><a href=#-cot--show-your-thinking>📝 CoT — Show your thinking</a></li></ul></li></ul><ul><li><a href=#-fineweb2--multilingual-web-data-at-scale>📍 FineWeb 2 — Multilingual Web Data at Scale</a></li><li><a href=#-powerlaws--generalization>📍 Power Laws & Generalization</a></li></ul></nav></div></details></div><div class=post-content><h1 id=-day1--monday-10feb2025>🗓 Day 1 — Monday, 10 Feb 2025<a hidden class=anchor aria-hidden=true href=#-day1--monday-10feb2025>#</a></h1><h3 id=-what-is-commoncrawl>🌍 What Is <strong>Common Crawl</strong>?<a hidden class=anchor aria-hidden=true href=#-what-is-commoncrawl>#</a></h3><p>Common Crawl is a <strong>huge, free snapshot of the public web</strong>.<br>A non‑profit updates it every month, storing:</p><ul><li><strong>Billions of HTML pages</strong></li><li>Their cleaned‑up <strong>text content</strong></li><li>Extra <strong>metadata</strong> (links, timestamps, MIME types, …)</li></ul><h4 id=why-it-matters>Why It Matters<a hidden class=anchor aria-hidden=true href=#why-it-matters>#</a></h4><ul><li><strong>Track language change</strong> – see how words, memes, and topics shift over time.</li><li><strong>Map the web’s link network</strong> – study which sites connect and why.</li><li><strong>Train big ML models</strong> – use real‑world data instead of tiny toy datasets.</li></ul><p>Because each release includes both the <strong>raw HTML</strong> and a parsed text layer, you can analyze:</p><table><thead><tr><th>Layer</th><th>What you can study</th></tr></thead><tbody><tr><td>Raw HTML structure</td><td>Link graphs, page layout, site categories</td></tr><tr><td>Clean text content</td><td>Sentiment, topic trends, new buzzwords</td></tr></tbody></table><hr><h4 id=perks-for-researchers>Perks for Researchers<a hidden class=anchor aria-hidden=true href=#perks-for-researchers>#</a></h4><ol><li><strong>No crawler needed</strong> – skip the cost and hassle of scraping the web yourself.</li><li><strong>Open licence</strong> – anyone can share code, replicate results, and build on your work.</li><li><strong>Regular updates</strong> – monthly snapshots reveal sudden spikes (e.g., when a new tech goes viral).</li></ol><p>All of this makes Common Crawl a go‑to resource for tasks like:</p><ul><li>Named‑entity recognition</li><li>Topic classification</li><li>Question answering</li></ul><p>By pooling efforts around one massive, open dataset, researchers push the limits of NLP faster than they could alone.</p><h2 id=-common-crawl>📍 <em>Common Crawl</em><a hidden class=anchor aria-hidden=true href=#-common-crawl>#</a></h2><embed src=/CommonCrawl.pdf width=100% height=800px type=application/pdf><h2 id=-factualityhallucinationsinllms>📍 Factuality & Hallucinations in LLMs<a hidden class=anchor aria-hidden=true href=#-factualityhallucinationsinllms>#</a></h2><p><strong>Speaker:</strong> Anna Rogers (University of Copenhagen)</p><blockquote><p>“Large language models are <strong>fluent bullshit generators</strong>—they sound right even when they’re wrong.”<br><small>— A. Rogers</small></p></blockquote><p>LLMs can drift from the truth, a problem known as <strong>hallucination</strong>. Rogers reviews two popular fixes and where they fall short:</p><table><thead><tr><th>Approach</th><th>How it works</th><th>Main weakness</th></tr></thead><tbody><tr><td><strong>RAG</strong><br>(Retrieval‑Augmented Generation)</td><td>Looks up facts in a search index or database, then feeds the snippets to the model as it writes.</td><td>Bad retrieval = bad answer; citations can be incorrect or missing.</td></tr><tr><td><strong>CoT</strong><br>(Chain‑of‑Thought prompting)</td><td>Prompts the model to show step‑by‑step reasoning before the final answer.</td><td>“Reasoning” may be invented; method can be abused to jailbreak the model.</td></tr></tbody></table><h4 id=impact-on-the-web>Impact on the Web<a hidden class=anchor aria-hidden=true href=#impact-on-the-web>#</a></h4><ul><li>Surge in AI‑generated spam and click‑bait</li><li>Harder to tell real news from synthetic text</li><li>New headaches for search engines and fact‑checkers</li></ul><p><strong>Takeaway:</strong> RAG and CoT help, but they don’t eliminate hallucinations. Better evaluation metrics and stronger guardrails are still needed.</p><embed src=/fact.pdf width=100% height=800px type=application/pdf><p><strong>RAG (Retrieval‑Augmented Generation)</strong> and <strong>CoT (Chain‑of‑Thought)</strong> both try to make LLM answers more trustworthy, but neither is a silver bullet.</p><hr><h3 id=-rag--look-it-up-then-write>🔍 RAG — Look it up, then write<a hidden class=anchor aria-hidden=true href=#-rag--look-it-up-then-write>#</a></h3><p><strong>How it works</strong></p><ol><li><strong>Retrieve</strong> Find facts in a search index or database.</li><li><strong>Generate</strong> Feed those facts to the model so it can weave them into its answer.</li></ol><p><strong>Where it breaks</strong></p><ul><li>If the search misses the right passage, the answer is still wrong.</li><li>Measuring “quality” is tricky: you need to score retrieval hit‑rate, answer truthfulness, and source fidelity—all at once.</li><li>Evaluations often rely on yet another LLM, which can add bias.</li><li>Even with good sources, the model may paraphrase or misquote them.</li></ul><hr><h3 id=-cot--show-your-thinking>📝 CoT — Show your thinking<a hidden class=anchor aria-hidden=true href=#-cot--show-your-thinking>#</a></h3><p><strong>How it works</strong></p><ol><li>Give the model examples that spell out step‑by‑step reasoning.</li><li>Ask it to copy that style: <em>“First, think. Then, answer.”</em></li></ol><p><strong>Where it breaks</strong></p><ul><li>Works great on some tasks, worse on others—especially biased ones.</li><li>The “reasoning” it prints may be made up, not its true internal logic.</li><li>Attackers can use CoT to slip past safety rules (“jailbreaking”).</li></ul><h1 id=-day2--tuesday-11feb2025>🗓 Day 2 — Tuesday, 11 Feb 2025<a hidden class=anchor aria-hidden=true href=#-day2--tuesday-11feb2025>#</a></h1><h2 id=-fineweb2--multilingual-web-data-at-scale>📍 FineWeb 2 — Multilingual Web Data at Scale<a hidden class=anchor aria-hidden=true href=#-fineweb2--multilingual-web-data-at-scale>#</a></h2><p><strong>Speaker:</strong> Guilherme Penedo (Hugging Face)</p><p>The opening talk introduced <strong>FineWeb 2</strong>, a brand‑new, multilingual web corpus for pre‑training large language models.<br>Penedo explained how the team is <strong>porting and tuning the English‑centric cleaning pipeline</strong>—deduplication, language ID, toxicity filters, and more—so it works reliably across dozens of other languages.</p><embed src=/fine.pdf width=100% height=800px type=application/pdf><h2 id=-powerlaws--generalization>📍 Power Laws & Generalization<a hidden class=anchor aria-hidden=true href=#-powerlaws--generalization>#</a></h2><p><strong>Speakers:</strong> Jenia Jitsev & Marianna Nezhurina</p><p>This session explored how <strong>power‑law scaling</strong> shows up in deep‑learning curves—and why turning those neat mathematical fits into real‑world “generalization scores” is harder than it looks. The speakers highlighted pitfalls such as noisy data, shifting task definitions, and compute limits that break the power‑law trend once models get big enough.</p><embed src=/jenia.pdf width=100% height=800px type=application/pdf>## 📍 *Generalization*
<embed src=/maria.pdf width=100% height=800px type=application/pdf></div><footer class=post-footer><ul class=post-tags><li><a href=https://ldomenichelli.github.io/tags/winterschool/>Winter‑school</a></li><li><a href=https://ldomenichelli.github.io/tags/pretraining/>Pretraining</a></li><li><a href=https://ldomenichelli.github.io/tags/factuality/>Factuality</a></li><li><a href=https://ldomenichelli.github.io/tags/llms/>LLMs</a></li><li><a href=https://ldomenichelli.github.io/tags/datasets/>Datasets</a></li></ul><nav class=paginav><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next »</span><br><span>Embeddings space 𖦹ׂ ₊˚⊹⋆</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>