<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ACL Vienna 2025 | lucia's notes</title>
<meta name=keywords content="NLP,ACL,computational linguistics"><meta name=description content="How ACL went..."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post3/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="ACL Vienna 2025"><meta property="og:description" content="How ACL went..."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post3/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="ACL Vienna 2025"><meta name=twitter:description content="How ACL went..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ACL Vienna 2025","item":"https://ldomenichelli.github.io/posts/post3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ACL Vienna 2025","name":"ACL Vienna 2025","description":"How ACL went...","keywords":["NLP","ACL","computational linguistics"],"articleBody":"Eye-tracking tutorial I attended the “Eye Tracking and NLP” tutorial in Vienna (Jul 27, 2025). If you work on cognitively‑informed models, evaluation beyond static labels, or human‑centered NLP, this one’s for you!!\nWhy gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (PubMed) Data is maturing: Multilingual, multi‑lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made high‑quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv) Models \u0026 evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned. Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology) Eye‑tracking 101 (super fast) 🧠👁️ Fixations \u0026 saccades. Reading is a hop‑and‑pause routine: brief saccades (tens of ms) between ~200–250 ms fixations; perception occurs mostly during fixations, not saccades. The classic eye‑mind assumption: minimal lag between what’s fixated and what’s processed. (andrewd.ces.clemson.edu, PubMed)\nPerceptual span. High‑acuity foveal vision is cone‑rich, while parafoveal vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (NCBI, PubMed Central, Frontiers)\nReading measures you’ll see in papers: skip rate, first‑fixation duration, gaze duration, regression rate, go‑past duration, total fixation time. These map fixations to Areas‑of‑Interest (AoIs) at token/region level. Hardware \u0026 sampling. For reading studies, stationary trackers with head stabilization and ≥200 Hz sampling are typical to get character‑level precision and reliable on/offsets. Pipelines \u0026 data structure. Raw samples → fixation detection → map to AoIs → compute measures per reader×word. Remember: data is not i.i.d. (nested readers/texts), which affects stats and ML splits. Low‑tech alternatives. When eye‑tracking isn’t feasible: Self‑Paced Reading (SPR), Maze, mouse‑tracking can capture useful online signals—with different trade‑offs. (PubMed Central, SpringerLink, SpringerLink)\nDatasets \u0026 tools I’m bookmarking 📚🧰 MECO (Multilingual Eye‑movement Corpus): large, coordinated, cross‑linguistic reading data; Wave 2 keeps expanding. (meco-read.com, PubMed Central) MultiplEYE (COST Action): enabling multilingual eye‑tracking‑while‑reading at scale; infrastructure, protocols, and community. (multipleye.eu, Radboud Universiteit) OneStop Eye Movements: 360 native readers, 2.6M tokens; great for comprehension‑linked analyses. (lacclab.github.io) Provo, ZuCo, Dundee, CELER… Useful complements for different tasks and populations. (PubMed) pymovements: open‑source package to download datasets and preprocess gaze (event detection, angles/velocities, etc.). (arXiv, pymovements.readthedocs.io) Using gaze in NLP models 🔧🤝 Word‑level alignment \u0026 embeddings. Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gaze‑motivated positional encodings and human attention signals. Synthetic scanpaths help scale. Since human gaze is scarce, Eyettention‑style scanpath generators and follow‑ups inject synthetic gaze to fine‑tune LMs, improving GLUE tasks (especially low‑resource). (arXiv, ACL Anthology, ACL Anthology)\nTask‑specific multitask learning. Training to predict reading measures jointly with downstream tasks (e.g., QA with question preview vs. ordinary reading) can induce more human‑like attention. (ACL Anthology)\nWhat to expect. Reported gains are real but often modest without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial. Examples \u0026 pointers: NER with gaze, compression/paraphrase, readability, parsing—plus general “gaze‑augmented PLMs.” (ACL Anthology, ACL Anthology, arXiv)\nUsing gaze to evaluate NLP 📏🧪 Behavioral evaluation uses online human signals—complementing labels or preferences. We saw applications to MT (reading effort), summarization (human vs. model saliency), and readability (reading‑ease metrics). (SpringerLink, ACL Anthology)\nCase study — Automatic Readability Assessment (ARA). A new eye‑tracking‑based benchmark correlates model scores with reading speed, skip rate, regressions, and total fixation time, revealing weak spots of classic readability formulas. Promising direction for cognitive evaluation. (hundred.org)\nPsycholinguistics ↔️ NLP: what’s being tested? 🧪🔁 Surprisal \u0026 RTs. Foundational results show a strong relation between LM surprisal and reading times; this holds across languages and for many modern LMs—with nuances. (lexplore.com, lexplore.com) Classics to know: Surprisal theory, Dependency Locality Theory, Uniform Information Density, Cue‑based retrieval/ACT‑R—usually operationalized via parsers/LMs. (eyetechds.com) Controlled tests. Agreement phenomena with GPT‑2 surprisal; embeddings as cognitive features for memory retrieval. (Appsource – Business Apps, eyetechds.com) Are LLMs aligned with human reading? 🤖🧍‍♀️ It’s complicated (and active in 2023–2025):\nBigger isn’t always better: Larger Transformers can fit worse to human RTs than smaller ones (surprisal‑RT link weakens with size). (ACL Anthology) …but layer matters: Intermediate layers may reverse that trend. (dfki.de) Individual differences: Surprisal better predicts first‑pass RTs for lower verbal IQ readers; entropy better fits those with higher working memory. (PubMed) Text \u0026 decoding matter: PP varies across generation strategies and reading measures; evaluating produced texts against human reading is informative. (ACL Anthology, ACL Anthology) Add cognitive bias: Injecting recency biases (e.g., ALiBi) improves LM fit to reading times. (ACL Anthology, ACL Anthology) Modeling eye movements themselves 🛠️ Cognitive models (fewer, interpretable parameters): E‑Z Reader, SWIFT, SEAM, OB1‑Reader. ML/NLP models (data‑hungry, high‑capacity): Eyettention, ScanDL 2.0, SP‑EyeGAN. The recent trend is to combine strengths (e.g., self‑supervised frameworks grounded in cognitive constraints). (PubMed, ScienceDirect, arXiv, ACM Digital Library, Zora, ACM Digital Library)\nHuman‑centered applications 🌍 Language assessment (L2): Eye movements carry proficiency signals (e.g., EyeScore‑style similarity to L1 prototypes). Reading impairment screening/monitoring: Commercial tools (e.g., Lexplore) and research platforms point to scalable screening and longitudinal tracking. (eyetechusa.com) Reading comprehension modeling: Predicting comprehension from gaze during QA is an emerging task on OneStop. (arXiv) How to get started (my checklist) ✅ Pick a dataset that matches your question (MECO/OneStop/Provo/etc.). (meco-read.com, lacclab.github.io, PubMed) Mind the structure (reader/text effects) and choose proper splits/stats. Use a pipeline (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (arXiv) Decide your integration: (a) features/embeddings, (b) auxiliary losses (multitask), or (c) synthetic gaze + LM fine‑tuning. (ACL Anthology, ACL Anthology) Evaluate cognitively: add behavioral metrics (e.g., ARA with eye‑tracking) alongside standard accuracy. (hundred.org) References \u0026 links 🔗 Tutorial slides: Eye Tracking and NLP (ACL 2025) — many figures and examples here are adapted from the tutorial. Foundations: Rayner’s classic review on eye movements \u0026 cognition; eye‑mind assumption background. (andrewd.ces.clemson.edu, PubMed) Perceptual span \u0026 physiology: asymmetries and fovea/cone density. (Frontiers, NCBI) Datasets/initiatives: MECO, MultiplEYE, OneStop, Provo; toolkit pymovements. (meco-read.com, multipleye.eu, lacclab.github.io, PubMed, arXiv) Gaze for modeling: NER with gaze; synthetic scanpaths + GLUE; multitask QA. (ACL Anthology, ACL Anthology, ACL Anthology, ACL Anthology) Behavioral eval: MT (eye‑tracking), summarization with eye‑gaze, readability via eye‑tracking. (SpringerLink, ACL Anthology, hundred.org) Psycholinguistic links: Smith \u0026 Levy; Demberg \u0026 Keller; Shain et al.; Wilcox et al.; Ryu \u0026 Lewis; Smith \u0026 Vasishth. (lexplore.com, eyetechds.com, lexplore.com, Appsource – Business Apps, eyetechds.com) Alignment \u0026 recency bias: Oh \u0026 Schuler (TACL 2023); Kuribayashi et al. (2025); Haller et al. (2024); Bolliger et al. (2024); de Varda \u0026 Marelli (2024); Clark et al. (COLING 2025). (ACL Anthology, dfki.de, PubMed, ACL Anthology, ACL Anthology, ACL Anthology) Scanpath modeling: Eyettention; ScanDL 2.0; SP‑EyeGAN; SEAM; OB1‑Reader. (arXiv, Zora, ACM Digital Library, arXiv, PubMed) ","wordCount":"1134","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post3/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>ACL Vienna 2025</h1><div class=post-description>How ACL went...</div><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#eyetracking-101-super-fast->Eye‑tracking 101 (super fast) 🧠👁️</a></li><li><a href=#datasets--tools-im-bookmarking->Datasets & tools I’m bookmarking 📚🧰</a></li><li><a href=#using-gaze-in-nlp-models->Using gaze <strong>in</strong> NLP models 🔧🤝</a></li><li><a href=#using-gaze-to-evaluate-nlp->Using gaze to <strong>evaluate</strong> NLP 📏🧪</a></li><li><a href=#psycholinguistics--nlp-whats-being-tested->Psycholinguistics ↔️ NLP: what’s being tested? 🧪🔁</a></li><li><a href=#are-llms-aligned-with-human-reading->Are LLMs aligned with human reading? 🤖🧍‍♀️</a></li><li><a href=#modeling-eye-movements-themselves->Modeling eye movements themselves 🛠️</a></li><li><a href=#humancentered-applications->Human‑centered applications 🌍</a></li><li><a href=#how-to-get-started-my-checklist->How to get started (my checklist) ✅</a></li><li><a href=#references--links->References & links 🔗</a></li></ul></nav></div></details></div><div class=post-content><h1 id=eye-tracking-tutorial>Eye-tracking tutorial<a hidden class=anchor aria-hidden=true href=#eye-tracking-tutorial>#</a></h1><blockquote><p>I attended the “Eye Tracking and NLP” tutorial in Vienna (Jul 27, 2025). If you work on cognitively‑informed models, evaluation beyond static labels, or human‑centered NLP, this one’s for you!!</p></blockquote><ul><li><strong>Why gaze?</strong> Eye movements reflect <em>online</em> processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (<a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Data is maturing:</strong> Multilingual, multi‑lab efforts (e.g., <strong>MECO</strong>, <strong>MultiplEYE</strong>) + tooling (e.g., <strong>pymovements</strong>) have made high‑quality datasets and pipelines more accessible. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE – Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Models & evals:</strong> Gaze can <strong>improve</strong> certain NLP tasks and also <strong>evaluate</strong> systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned.</li><li><strong>Open debates:</strong> How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding <strong>recency biases</strong> can help fit human behavior. (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h2 id=eyetracking-101-super-fast->Eye‑tracking 101 (super fast) 🧠👁️<a hidden class=anchor aria-hidden=true href=#eyetracking-101-super-fast->#</a></h2><p><strong>Fixations & saccades.</strong> Reading is a hop‑and‑pause routine: brief saccades (tens of ms) between ~200–250 ms fixations; perception occurs mostly during fixations, not saccades. The classic <strong>eye‑mind assumption</strong>: minimal lag between what’s fixated and what’s processed. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</p><p><strong>Perceptual span.</strong> High‑acuity <strong>foveal</strong> vision is cone‑rich, while <strong>parafoveal</strong> vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (<a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3075059/?utm_source=chatgpt.com" title="Eye movements, the perceptual span, and reading speed - PMC">PubMed Central</a>, <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>)</p><p><strong>Reading measures you’ll see in papers:</strong>
<em>skip rate, first‑fixation duration, gaze duration, regression rate, go‑past duration, total fixation time</em>. These map fixations to Areas‑of‑Interest (AoIs) at token/region level.</p><p><strong>Hardware & sampling.</strong> For reading studies, stationary trackers with head stabilization and <strong>≥200 Hz</strong> sampling are typical to get character‑level precision and reliable on/offsets.</p><p><strong>Pipelines & data structure.</strong> Raw samples → fixation detection → map to AoIs → compute measures per reader×word. Remember: <strong>data is not i.i.d.</strong> (nested readers/texts), which affects stats and ML splits.</p><p><strong>Low‑tech alternatives.</strong> When eye‑tracking isn’t feasible: <strong>Self‑Paced Reading (SPR)</strong>, <strong>Maze</strong>, <strong>mouse‑tracking</strong> can capture useful online signals—with different trade‑offs. (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12058810/?utm_source=chatgpt.com" title="A multiverse analysis of cleaning and analyzing procedures of eye ...">PubMed Central</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02536-8?utm_source=chatgpt.com" title="PoTeC: A German naturalistic eye-tracking-while-reading corpus">SpringerLink</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02517-x?utm_source=chatgpt.com" title="A Persian sentence reading corpus of eye movements">SpringerLink</a>)</p><hr><h2 id=datasets--tools-im-bookmarking->Datasets & tools I’m bookmarking 📚🧰<a hidden class=anchor aria-hidden=true href=#datasets--tools-im-bookmarking->#</a></h2><ul><li><strong>MECO</strong> (Multilingual Eye‑movement Corpus): large, coordinated, cross‑linguistic reading data; Wave 2 keeps expanding. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12246088/?utm_source=chatgpt.com" title="Wave 2 of the Multilingual Eye-Movement Corpus (MECO): New text ...">PubMed Central</a>)</li><li><strong>MultiplEYE (COST Action)</strong>: enabling multilingual eye‑tracking‑while‑reading at scale; infrastructure, protocols, and community. (<a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE – Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://www.ru.nl/en/research/research-projects/multipleye?utm_source=chatgpt.com" title="MultiplEYE | Radboud University">Radboud Universiteit</a>)</li><li><strong>OneStop Eye Movements</strong>: 360 native readers, 2.6M tokens; great for comprehension‑linked analyses. (<a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>)</li><li><strong>Provo, ZuCo, Dundee, CELER…</strong> Useful complements for different tasks and populations. (<a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>pymovements</strong>: open‑source package to <strong>download</strong> datasets and <strong>preprocess</strong> gaze (event detection, angles/velocities, etc.). (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>, <a href="https://pymovements.readthedocs.io/en/v0.17.0/tutorials/preprocessing-raw-data.html?utm_source=chatgpt.com" title="Preprocessing Raw Gaze Data - the pymovements documentation!">pymovements.readthedocs.io</a>)</li></ul><hr><h2 id=using-gaze-in-nlp-models->Using gaze <strong>in</strong> NLP models 🔧🤝<a hidden class=anchor aria-hidden=true href=#using-gaze-in-nlp-models->#</a></h2><p><strong>Word‑level alignment & embeddings.</strong> Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gaze‑motivated positional encodings and human attention signals.</p><p><strong>Synthetic scanpaths help scale.</strong> Since human gaze is scarce, <strong>Eyettention</strong>‑style scanpath generators and follow‑ups inject <em>synthetic</em> gaze to fine‑tune LMs, improving <strong>GLUE</strong> tasks (especially low‑resource). (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>)</p><p><strong>Task‑specific multitask learning.</strong> Training to predict reading measures jointly with downstream tasks (e.g., <strong>QA</strong> with question preview vs. ordinary reading) can induce more human‑like attention. (<a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)</p><p><strong>What to expect.</strong> Reported gains are real but often <strong>modest</strong> without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial.</p><p><strong>Examples & pointers:</strong> NER with gaze, compression/paraphrase, readability, parsing—plus general “gaze‑augmented PLMs.” (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.conll-1.8.pdf?utm_source=chatgpt.com" title="[PDF] A Comparative Study on Textual Saliency of Styles from Eye ...">ACL Anthology</a>, <a href="https://arxiv.org/pdf/2310.14676?utm_source=chatgpt.com" title="[PDF] arXiv:2310.14676v1 [cs.CL] 23 Oct 2023">arXiv</a>)</p><hr><h2 id=using-gaze-to-evaluate-nlp->Using gaze to <strong>evaluate</strong> NLP 📏🧪<a hidden class=anchor aria-hidden=true href=#using-gaze-to-evaluate-nlp->#</a></h2><p><strong>Behavioral evaluation</strong> uses <em>online</em> human signals—complementing labels or preferences. We saw applications to <strong>MT</strong> (reading effort), <strong>summarization</strong> (human vs. model saliency), and <strong>readability</strong> (reading‑ease metrics). (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>)</p><p><strong>Case study — Automatic Readability Assessment (ARA).</strong> A new eye‑tracking‑based benchmark correlates model scores with <strong>reading speed</strong>, <strong>skip rate</strong>, <strong>regressions</strong>, and <strong>total fixation time</strong>, revealing weak spots of classic readability formulas. Promising direction for <strong>cognitive evaluation</strong>. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</p><hr><h2 id=psycholinguistics--nlp-whats-being-tested->Psycholinguistics ↔️ NLP: what’s being tested? 🧪🔁<a hidden class=anchor aria-hidden=true href=#psycholinguistics--nlp-whats-being-tested->#</a></h2><ul><li><strong>Surprisal & RTs.</strong> Foundational results show a strong relation between <strong>LM surprisal</strong> and <strong>reading times</strong>; this holds across languages and for many modern LMs—with nuances. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>)</li><li><strong>Classics to know:</strong> <em>Surprisal theory</em>, <em>Dependency Locality Theory</em>, <em>Uniform Information Density</em>, <em>Cue‑based retrieval/ACT‑R</em>—usually operationalized via parsers/LMs. (<a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>)</li><li><strong>Controlled tests.</strong> Agreement phenomena with GPT‑2 surprisal; embeddings as cognitive features for memory retrieval. (<a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource – Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li></ul><hr><h2 id=are-llms-aligned-with-human-reading->Are LLMs aligned with human reading? 🤖🧍‍♀️<a hidden class=anchor aria-hidden=true href=#are-llms-aligned-with-human-reading->#</a></h2><p>It’s <strong>complicated</strong> (and active in 2023–2025):</p><ul><li><strong>Bigger isn’t always better:</strong> Larger Transformers can fit <em>worse</em> to human RTs than smaller ones (surprisal‑RT link weakens with size). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>)</li><li><strong>…but layer matters:</strong> <strong>Intermediate layers</strong> may reverse that trend. (<a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>)</li><li><strong>Individual differences:</strong> <strong>Surprisal</strong> better predicts first‑pass RTs for <strong>lower verbal IQ</strong> readers; <strong>entropy</strong> better fits those with <strong>higher working memory</strong>. (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li><li><strong>Text & decoding matter:</strong> PP varies across <strong>generation strategies</strong> and <strong>reading measures</strong>; evaluating <em>produced</em> texts against human reading is informative. (<a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14/?utm_source=chatgpt.com" title="On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>)</li><li><strong>Add cognitive bias:</strong> Injecting <strong>recency biases</strong> (e.g., <strong>ALiBi</strong>) improves LM fit to reading times. (<a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517.pdf?utm_source=chatgpt.com" title="[PDF] Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h2 id=modeling-eye-movements-themselves->Modeling eye movements themselves 🛠️<a hidden class=anchor aria-hidden=true href=#modeling-eye-movements-themselves->#</a></h2><p><strong>Cognitive models</strong> (fewer, interpretable parameters): <strong>E‑Z Reader</strong>, <strong>SWIFT</strong>, <strong>SEAM</strong>, <strong>OB1‑Reader</strong>. <strong>ML/NLP models</strong> (data‑hungry, high‑capacity): <strong>Eyettention</strong>, <strong>ScanDL 2.0</strong>, <strong>SP‑EyeGAN</strong>. The recent trend is to combine strengths (e.g., self‑supervised frameworks grounded in cognitive constraints). (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0749596X23000955?utm_source=chatgpt.com" title="SEAM: An integrated activation-coupled model of sentence ...">ScienceDirect</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://dl.acm.org/doi/10.1145/3591131?utm_source=chatgpt.com" title="An Attention-based Dual-Sequence Model for Predicting Human ...">ACM Digital Library</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>)</p><hr><h2 id=humancentered-applications->Human‑centered applications 🌍<a hidden class=anchor aria-hidden=true href=#humancentered-applications->#</a></h2><ul><li><strong>Language assessment (L2):</strong> Eye movements carry proficiency signals (e.g., <strong>EyeScore</strong>‑style similarity to L1 prototypes).</li><li><strong>Reading impairment screening/monitoring:</strong> Commercial tools (e.g., <strong>Lexplore</strong>) and research platforms point to scalable screening and longitudinal tracking. (<a href="https://eyetechusa.com/?utm_source=chatgpt.com" title="eye model, episcleral venomanometer, scleral depressors">eyetechusa.com</a>)</li><li><strong>Reading comprehension modeling:</strong> Predicting comprehension from gaze during QA is an emerging task on <strong>OneStop</strong>. (<a href="https://arxiv.org/html/2410.04484v1?utm_source=chatgpt.com" title="Fine-Grained Prediction of Reading Comprehension from Eye ...">arXiv</a>)</li></ul><hr><h2 id=how-to-get-started-my-checklist->How to get started (my checklist) ✅<a hidden class=anchor aria-hidden=true href=#how-to-get-started-my-checklist->#</a></h2><ol><li><strong>Pick a dataset</strong> that matches your question (MECO/OneStop/Provo/etc.). (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>Mind the structure</strong> (reader/text effects) and choose proper splits/stats.</li><li><strong>Use a pipeline</strong> (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Decide your integration</strong>: (a) <strong>features/embeddings</strong>, (b) <strong>auxiliary losses</strong> (multitask), or (c) <strong>synthetic gaze</strong> + LM fine‑tuning. (<a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>)</li><li><strong>Evaluate cognitively</strong>: add behavioral metrics (e.g., ARA with eye‑tracking) alongside standard accuracy. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li></ol><hr><h2 id=references--links->References & links 🔗<a hidden class=anchor aria-hidden=true href=#references--links->#</a></h2><ul><li><strong>Tutorial slides:</strong> <em>Eye Tracking and NLP</em> (ACL 2025) — many figures and examples here are adapted from the tutorial.</li><li><strong>Foundations:</strong> Rayner’s classic review on eye movements & cognition; eye‑mind assumption background. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Perceptual span & physiology:</strong> asymmetries and fovea/cone density. (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>, <a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>)</li><li><strong>Datasets/initiatives:</strong> MECO, MultiplEYE, OneStop, Provo; toolkit <strong>pymovements</strong>. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE – Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Gaze for modeling:</strong> NER with gaze; synthetic scanpaths + GLUE; multitask QA. (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)</li><li><strong>Behavioral eval:</strong> MT (eye‑tracking), summarization with eye‑gaze, readability via eye‑tracking. (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>, <a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li><li><strong>Psycholinguistic links:</strong> Smith & Levy; Demberg & Keller; Shain et al.; Wilcox et al.; Ryu & Lewis; Smith & Vasishth. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>, <a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource – Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li><li><strong>Alignment & recency bias:</strong> Oh & Schuler (TACL 2023); Kuribayashi et al. (2025); Haller et al. (2024); Bolliger et al. (2024); de Varda & Marelli (2024); Clark et al. (COLING 2025). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li><li><strong>Scanpath modeling:</strong> Eyettention; ScanDL 2.0; SP‑EyeGAN; SEAM; OB1‑Reader. (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li></ul><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://ldomenichelli.github.io/tags/nlp/>NLP</a></li><li><a href=https://ldomenichelli.github.io/tags/acl/>ACL</a></li><li><a href=https://ldomenichelli.github.io/tags/computational-linguistics/>Computational Linguistics</a></li></ul><nav class=paginav><a class=prev href=https://ldomenichelli.github.io/posts/post5/><span class=title>« Prev</span><br><span>❄️ HPLT × NLPL Winter School</span>
</a><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next »</span><br><span>Embeddings space 𖦹ׂ ₊˚⊹⋆</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>