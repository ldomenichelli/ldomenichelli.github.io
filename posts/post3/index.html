<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ACL Vienna 2025 🇦🇹 | lucia's notes</title>
<meta name=keywords content><meta name=description content="Here are the notes from some talks I attented at ACL 2025 in Vienna!

    Eye-tracking 

Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (PubMed)
Data is maturing: Multilingual, multi‑lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made high‑quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv)
Models & evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned. 
Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology)


Eye‑tracking 101 👁️
Some basic concepts:"><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post3/><link crossorigin=anonymous href=/assets/css/stylesheet.5ad9b1caa92e4cea83ebcd3088e97362f239b07d8144490aaf0bc1d6bd89cd17.css integrity="sha256-WtmxyqkuTOqD680wiOlzYvI5sH2BREkKrwvB1r2JzRc=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="ACL Vienna 2025 🇦🇹"><meta property="og:description" content="Here are the notes from some talks I attented at ACL 2025 in Vienna!

    Eye-tracking 

Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (PubMed)
Data is maturing: Multilingual, multi‑lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made high‑quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv)
Models & evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned. 
Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology)


Eye‑tracking 101 👁️
Some basic concepts:"><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post3/"><meta property="og:image" content="https://ldomenichelli.github.io/posts/post3/img/acl1.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="lucia's notes"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/posts/post3/img/acl1.png"><meta name=twitter:title content="ACL Vienna 2025 🇦🇹"><meta name=twitter:description content="Here are the notes from some talks I attented at ACL 2025 in Vienna!

    Eye-tracking 

Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (PubMed)
Data is maturing: Multilingual, multi‑lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made high‑quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv)
Models & evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned. 
Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology)


Eye‑tracking 101 👁️
Some basic concepts:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"ACL Vienna 2025 🇦🇹","item":"https://ldomenichelli.github.io/posts/post3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ACL Vienna 2025 🇦🇹","name":"ACL Vienna 2025 🇦🇹","description":"Here are the notes from some talks I attented at ACL 2025 in Vienna!\nEye-tracking Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (PubMed) Data is maturing: Multilingual, multi‑lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made high‑quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv) Models \u0026amp; evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned. Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology) Eye‑tracking 101 👁️ Some basic concepts:\n","keywords":[],"articleBody":"Here are the notes from some talks I attented at ACL 2025 in Vienna!\nEye-tracking Why gaze? Eye movements reflect online processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (PubMed) Data is maturing: Multilingual, multi‑lab efforts (e.g., MECO, MultiplEYE) + tooling (e.g., pymovements) have made high‑quality datasets and pipelines more accessible. (meco-read.com, multipleye.eu, arXiv) Models \u0026 evals: Gaze can improve certain NLP tasks and also evaluate systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned. Open debates: How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding recency biases can help fit human behavior. (ACL Anthology, dfki.de, ACL Anthology, ACL Anthology) Eye‑tracking 101 👁️ Some basic concepts:\nFixations \u0026 saccades. Reading is a hop‑and‑pause routine: brief saccades (tens of ms) between ~200–250 ms fixations; perception occurs mostly during fixations, not saccades. The classic eye‑mind assumption: minimal lag between what’s fixated and what’s processed. (andrewd.ces.clemson.edu, PubMed)\nPerceptual span. High‑acuity foveal vision is cone‑rich, while parafoveal vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (NCBI, PubMed Central, Frontiers)\nReading measures you’ll see in papers: skip rate, first‑fixation duration, gaze duration, regression rate, go‑past duration, total fixation time. These map fixations to Areas‑of‑Interest (AoIs) at token/region level. Hardware \u0026 sampling. For reading studies, stationary trackers with head stabilization and ≥200 Hz sampling are typical to get character‑level precision and reliable on/offsets. Pipelines \u0026 data structure. Raw samples → fixation detection → map to AoIs → compute measures per reader×word. Remember: data is not i.i.d. (nested readers/texts), which affects stats and ML splits. Low‑tech alternatives. When eye‑tracking isn’t feasible: Self‑Paced Reading (SPR), Maze, mouse‑tracking can capture useful online signals—with different trade‑offs. (PubMed Central, SpringerLink, SpringerLink)\nDatasets \u0026 tools 📚 MECO (Multilingual Eye‑movement Corpus): large, coordinated, cross‑linguistic reading data; Wave 2 keeps expanding. (meco-read.com, PubMed Central) MultiplEYE (COST Action): enabling multilingual eye‑tracking‑while‑reading at scale; infrastructure, protocols, and community. (multipleye.eu, Radboud Universiteit) OneStop Eye Movements: 360 native readers, 2.6M tokens; great for comprehension‑linked analyses. (lacclab.github.io) Provo, ZuCo, Dundee, CELER… Useful complements for different tasks and populations. (PubMed) pymovements: open‑source package to download datasets and preprocess gaze (event detection, angles/velocities, etc.). (arXiv, pymovements.readthedocs.io) ![[Pasted image 20250729095400.png]] Using gaze in NLP models 🔧 Word‑level alignment \u0026 embeddings. Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gaze‑motivated positional encodings and human attention signals. Synthetic scanpaths help scale. Since human gaze is scarce, Eyettention‑style scanpath generators and follow‑ups inject synthetic gaze to fine‑tune LMs, improving GLUE tasks (especially low‑resource). (arXiv, ACL Anthology, ACL Anthology)\nTask‑specific multitask learning. Training to predict reading measures jointly with downstream tasks (e.g., QA with question preview vs. ordinary reading) can induce more human‑like attention. (ACL Anthology) What to expect. Reported gains are real but often modest without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial. Examples \u0026 pointers: NER with gaze, compression/paraphrase, readability, parsing—plus general “gaze‑augmented PLMs.” (ACL Anthology, ACL Anthology, arXiv)\nUsing gaze to evaluate NLP 📏 Behavioral evaluation uses online human signals—complementing labels or preferences. We saw applications to MT (reading effort), summarization (human vs. model saliency), and readability (reading‑ease metrics). (SpringerLink, ACL Anthology)\nCase study — Automatic Readability Assessment (ARA). A new eye‑tracking‑based benchmark correlates model scores with reading speed, skip rate, regressions, and total fixation time, revealing weak spots of classic readability formulas. Promising direction for cognitive evaluation. (hundred.org)\nPsycholinguistics NLP Surprisal \u0026 RTs. Foundational results show a strong relation between LM surprisal and reading times; this holds across languages and for many modern LMs—with nuances. (lexplore.com, lexplore.com) *Classics to know: Surprisal theory, Dependency Locality Theory, Uniform Information Density, Cue‑based retrieval/ACT‑R—usually operationalized via parsers/LMs. (eyetechds.com)\nControlled tests. Agreement phenomena with GPT‑2 surprisal; embeddings as cognitive features for memory retrieval. (Appsource – Business Apps, eyetechds.com) Are LLMs aligned with human reading? 🤖🧍‍♀️ It’s complicated (and active in 2023–2025):\nBigger isn’t always better: Larger Transformers can fit worse to human RTs than smaller ones (surprisal‑RT link weakens with size). (ACL Anthology) …but layer matters: Intermediate layers may reverse that trend. (dfki.de) Individual differences: Surprisal better predicts first‑pass RTs for lower verbal IQ readers; entropy better fits those with higher working memory. (PubMed) Text \u0026 decoding matter: PP varies across generation strategies and reading measures; evaluating produced texts against human reading is informative. (ACL Anthology, ACL Anthology) Add cognitive bias: Injecting recency biases (e.g., ALiBi) improves LM fit to reading times. (ACL Anthology, ACL Anthology) Modeling eye movements themselves 🛠️ Cognitive models (fewer, interpretable parameters): E‑Z Reader, SWIFT, SEAM, OB1‑Reader. ML/NLP models (data‑hungry, high‑capacity): Eyettention, ScanDL 2.0, SP‑EyeGAN. The recent trend is to combine strengths (e.g., self‑supervised frameworks grounded in cognitive constraints). (PubMed, ScienceDirect, arXiv, ACM Digital Library, Zora, ACM Digital Library)\nHuman‑centered applications 🌍 Language assessment (L2): Eye movements carry proficiency signals (e.g., EyeScore‑style similarity to L1 prototypes). Reading impairment screening/monitoring: Commercial tools (e.g., Lexplore) and research platforms point to scalable screening and longitudinal tracking. (eyetechusa.com) Reading comprehension modeling: Predicting comprehension from gaze during QA is an emerging task on OneStop. (arXiv) How to get started Pick a dataset that matches your question (MECO/OneStop/Provo/etc.). (meco-read.com, lacclab.github.io, PubMed) Mind the structure (reader/text effects) and choose proper splits/stats. Use a pipeline (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (arXiv) Decide your integration: (a) features/embeddings, (b) auxiliary losses (multitask), or (c) synthetic gaze + LM fine‑tuning. (ACL Anthology, ACL Anthology) Evaluate cognitively: add behavioral metrics (e.g., ARA with eye‑tracking) alongside standard accuracy. (hundred.org) References \u0026 links 🔗 Tutorial slides: Eye Tracking and NLP (ACL 2025) — many figures and examples here are adapted from the tutorial. Foundations: Rayner’s classic review on eye movements \u0026 cognition; eye‑mind assumption background. (andrewd.ces.clemson.edu, PubMed) Perceptual span \u0026 physiology: asymmetries and fovea/cone density. (Frontiers, NCBI) Datasets/initiatives: MECO, MultiplEYE, OneStop, Provo; toolkit pymovements. (meco-read.com, multipleye.eu, lacclab.github.io, PubMed, arXiv) Gaze for modeling: NER with gaze; synthetic scanpaths + GLUE; multitask QA. (ACL Anthology, ACL Anthology, ACL Anthology, ACL Anthology) Behavioral eval: MT (eye‑tracking), summarization with eye‑gaze, readability via eye‑tracking. (SpringerLink, ACL Anthology, hundred.org) Psycholinguistic links: Smith \u0026 Levy; Demberg \u0026 Keller; Shain et al.; Wilcox et al.; Ryu \u0026 Lewis; Smith \u0026 Vasishth. (lexplore.com, eyetechds.com, lexplore.com, Appsource – Business Apps, eyetechds.com) Alignment \u0026 recency bias: Oh \u0026 Schuler (TACL 2023); Kuribayashi et al. (2025); Haller et al. (2024); Bolliger et al. (2024); de Varda \u0026 Marelli (2024); Clark et al. (COLING 2025). (ACL Anthology, dfki.de, PubMed, ACL Anthology, ACL Anthology, ACL Anthology) Scanpath modeling: Eyettention; ScanDL 2.0; SP‑EyeGAN; SEAM; OB1‑Reader. (arXiv, Zora, ACM Digital Library, arXiv, PubMed) Synthetic data for NLP At a glance We can’t label everything: synthetic data fills gaps when scraping/manual labels/privacy limits hit. Good synthetic data is task-tailored, sized right, and clean — but beware distribution shift. Evaluate two ways: extrinsic (downstream task) vs intrinsic (what the data itself looks like). Diversity sometimes beats raw correctness; noisy but varied sets can still improve models. Best results come from Human ↔︎ AI collaboration + strong filtering before use. Where do we get data? Scraping the web (scale, but licensing/noise). Manual labeling (accurate, expensive). Product/system data (useful but privacy-sensitive). Creative curation (high quality, limited volume).\nSynthetic data tries to extend/augment all of the above. What makes “good” synthetic data? Flexible \u0026 task-specific: format, difficulty, and style match your target task. Appropriate size: enough to move the needle, not so much it drowns real data. Clean: minimal contradictions/formatting errors. Aligned distributions: cover the same kinds of inputs/labels you expect in production. Why the warning? Because the real joint distribution often differs from the synthetic one:\n( P_{\\text{true}}(x,y) \\neq P_{\\text{synth}}(x,y) ).\nMismatch shows up as off-manifold inputs, wrong labels, or flawed reasoning traces.\nHow do we evaluate synthetic data? Extrinsic: train/evaluate models on tasks with/without the synthetic set.\n✅ Directly answers “does it help?”. ❌ Costly, indirect diagnostic signal. Intrinsic: inspect the data/generation process itself.\nCorrectness: e.g., spot-checking/self-instruct style manual audits. Diversity/coverage: does it span plausible inputs? (e.g., DataTune’s bigram diversity as a quick signal). Privacy, fairness, distributional similarity: toolkits like SynthTextEval help stress-test. Model choice as a proxy: pick the generator by how well its synthetic data matches human-written (e.g., AgoraBench). How is synthetic data created? Knowledge distillation (Hinton ’15 → sentence-level): train a student to mimic a teacher; cheap SFT, keeps style on-target. In-context generation: prompt LMs to produce new labeled examples for arbitrary tasks (Schick \u0026 Schütze ’21). Self-training/bootstrapping: fine-tune on the model’s own outputs (Wang et al. ’22). Observed effects: generated text is often more diverse (e.g., BERTScore) but only ~½ examples fully correct—and models can still improve thanks to coverage. Common patterns\nSampling-based generation: temperature/top-p, curriculum over difficulty. Instruction back-translation: given an answer (y), generate an instruction (x) that (y) would satisfy. Transform existing data: retrieve/convert to the target format (QA from StackExchange; ground to KGs; rephrase documents for pretraining, Maini ’24). Human–AI collaboration: LLM drafts, humans edit/verify (creativity ↔︎ correctness). Symbolic/programmable data: pretrain on formal languages/grammars to improve generalization (Hu ’24). #Filter before you use it\nDiversity filters: keep sets that are far apart (ROUGE-L, embedding cosine, semantic tags). Gradient diversity: prefer examples that produce different loss gradients → more robust models. Quality filters: pick highest-reward responses (e.g., RAFT-style ranking). Correctness filters: keep chains of thought that reach the right answer; drop inconsistent traces. Where synthetic data fits in the pipeline Pretraining\nWhen real data plateaus: rephrase corpora, verbalize knowledge bases, or mix in formal languages for structure. Domain adaptation to reduce hallucinations on niche topics. Supervised fine-tuning (SFT)\nDistillation is cheap but can be “style-locked”. Self-Instruct / Evol-Instruct to grow instruction variance. MAmmoTH-style transform/extract tasks (e.g., convert docs to QA). RL \u0026 feedback\nMinimal supervision, negative examples, adaptation to own token distribution. Synthetic rewards (e.g., Prometheus evaluators; checklist-style rubrics). Choosing a “judge”: agreement with human prefs (RewardBench), agreement with benchmarks (re-ranking), effectiveness inside RL loops. “Teacher” qualities: good accuracy and low variance; even non-RL textual feedback can help—mainly when the base model is already strong. Reasoning\nScale up inference to get longer CoT/PoT traces; pipelines like OpenThoughts curate reasoning data. Even noisy reasoning data can help; more (curated) data → better reasoning. Code\nCodeAlpaca, WaveCoder, WizardCoder, Magicoder, etc. Train with execution feedback (tests, runtimes). Useful data types: single-turn, simulated multi-turn, “fix-the-bug”, and near-duplicate (LeetCode-style) variants. Tools \u0026 agents\nGorilla (API-calling), ToolLLM, ToRa (tool-integrated math), AgentTuning, CodeActInstruct, AgentE, “GPT-4-tools” style setups. Multimodal / Multilingual\nLLaVA-style visual instruction tuning; multilingual instruction pipelines. Limitations \u0026 open questions Synthetic sets still trail real data in size, diversity, and distribution; production usage (e.g., Anthropic’s Clio) shows gaps. In controlled tests, synthetic often underperforms; artifacts creep in. Synthetic eval data can overestimate model performance. Model collapse risk under recursive self-training (Shumailov ’23). We should measure instance-level quality, not just dataset averages. Governance: “distillation-friendly” models, usage restrictions, provenance tracking. Practical checklist (what I’d do) Define the target: task format, difficulty mix, and metrics. Generate broad, then filter hard: diversity → quality → correctness. Mix with real data: keep an anchor set for sanity checks. Evaluate both ways: quick intrinsic dashboards + periodic extrinsic runs. Close the loop: human spot-edits, error mining, and focused regeneration. Watch drift: compare (P_{\\text{true}}(x,y)) vs (P_{\\text{synth}}(x,y)) over time. ","wordCount":"1876","inLanguage":"en","image":"https://ldomenichelli.github.io/posts/post3/img/acl1.png","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post3/"},"publisher":{"@type":"Organization","name":"lucia's notes","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script><script src="/assets/js/analytics.js" async></script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="lucia's notes (Alt + H)">lucia's notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=about><span>about</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=notes><span>notes</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=hobbies><span>hobbies</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>ACL Vienna 2025 🇦🇹</h1><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#eyetracking-101->Eye‑tracking 101 👁️</a></li></ul></nav></div></details></div><div class=post-content><p>Here are the notes from some talks I attented at ACL 2025 in Vienna!</p><details id=talk-eye><summary>Eye-tracking</summary><ul><li><strong>Why gaze?</strong> Eye movements reflect <em>online</em> processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That’s gold for modeling and evaluation. (<a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Data is maturing:</strong> Multilingual, multi‑lab efforts (e.g., <strong>MECO</strong>, <strong>MultiplEYE</strong>) + tooling (e.g., <strong>pymovements</strong>) have made high‑quality datasets and pipelines more accessible. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE – Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Models & evals:</strong> Gaze can <strong>improve</strong> certain NLP tasks and also <strong>evaluate</strong> systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‑aligned.</li><li><strong>Open debates:</strong> How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding <strong>recency biases</strong> can help fit human behavior. (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h2 id=eyetracking-101->Eye‑tracking 101 👁️<a hidden class=anchor aria-hidden=true href=#eyetracking-101->#</a></h2><p>Some basic concepts:</p><p><strong>Fixations & saccades.</strong> Reading is a hop‑and‑pause routine: brief saccades (tens of ms) between ~200–250 ms fixations; perception occurs mostly during fixations, not saccades. The classic <strong>eye‑mind assumption</strong>: minimal lag between what’s fixated and what’s processed. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</p><p><input type=checkbox id=zoomCheck-ee0e7 hidden>
<label for=zoomCheck-ee0e7><img class=zoomCheck loading=lazy decoding=async src=./img/fix.png alt=1>
</label><strong>Perceptual span.</strong> High‑acuity <strong>foveal</strong> vision is cone‑rich, while <strong>parafoveal</strong> vision still supports useful preview. Span size and asymmetry depend on script and reading direction. (<a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC3075059/?utm_source=chatgpt.com" title="Eye movements, the perceptual span, and reading speed - PMC">PubMed Central</a>, <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>)</p><p><input type=checkbox id=zoomCheck-d495c hidden>
<label for=zoomCheck-d495c><img class=zoomCheck loading=lazy decoding=async src=./img/fovea.png alt></label></p><p><strong>Reading measures you’ll see in papers:</strong>
<em>skip rate, first‑fixation duration, gaze duration, regression rate, go‑past duration, total fixation time</em>. These map fixations to Areas‑of‑Interest (AoIs) at token/region level.</p><p><input type=checkbox id=zoomCheck-9015d hidden>
<label for=zoomCheck-9015d><img class=zoomCheck loading=lazy decoding=async src=./img/gaze1.png alt></label></p><p><strong>Hardware & sampling.</strong> For reading studies, stationary trackers with head stabilization and <strong>≥200 Hz</strong> sampling are typical to get character‑level precision and reliable on/offsets.</p><p><strong>Pipelines & data structure.</strong> Raw samples → fixation detection → map to AoIs → compute measures per reader×word. Remember: <strong>data is not i.i.d.</strong> (nested readers/texts), which affects stats and ML splits.</p><p><strong>Low‑tech alternatives.</strong> When eye‑tracking isn’t feasible: <strong>Self‑Paced Reading (SPR)</strong>, <strong>Maze</strong>, <strong>mouse‑tracking</strong> can capture useful online signals—with different trade‑offs. (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12058810/?utm_source=chatgpt.com" title="A multiverse analysis of cleaning and analyzing procedures of eye ...">PubMed Central</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02536-8?utm_source=chatgpt.com" title="PoTeC: A German naturalistic eye-tracking-while-reading corpus">SpringerLink</a>, <a href="https://link.springer.com/article/10.3758/s13428-024-02517-x?utm_source=chatgpt.com" title="A Persian sentence reading corpus of eye movements">SpringerLink</a>)</p><p><input type=checkbox id=zoomCheck-3f0e6 hidden>
<label for=zoomCheck-3f0e6><img class=zoomCheck loading=lazy decoding=async src=./img/alter.png alt></label></p><hr><h1 id=datasets--tools-->Datasets & tools 📚<a hidden class=anchor aria-hidden=true href=#datasets--tools-->#</a></h1><ul><li><strong>MECO</strong> (Multilingual Eye‑movement Corpus): large, coordinated, cross‑linguistic reading data; Wave 2 keeps expanding. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12246088/?utm_source=chatgpt.com" title="Wave 2 of the Multilingual Eye-Movement Corpus (MECO): New text ...">PubMed Central</a>)</li><li><strong>MultiplEYE (COST Action)</strong>: enabling multilingual eye‑tracking‑while‑reading at scale; infrastructure, protocols, and community. (<a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE – Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://www.ru.nl/en/research/research-projects/multipleye?utm_source=chatgpt.com" title="MultiplEYE | Radboud University">Radboud Universiteit</a>)</li><li><strong>OneStop Eye Movements</strong>: 360 native readers, 2.6M tokens; great for comprehension‑linked analyses. (<a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>)</li><li><strong>Provo, ZuCo, Dundee, CELER…</strong> Useful complements for different tasks and populations. (<a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>pymovements</strong>: open‑source package to <strong>download</strong> datasets and <strong>preprocess</strong> gaze (event detection, angles/velocities, etc.). (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>, <a href="https://pymovements.readthedocs.io/en/v0.17.0/tutorials/preprocessing-raw-data.html?utm_source=chatgpt.com" title="Preprocessing Raw Gaze Data - the pymovements documentation!">pymovements.readthedocs.io</a>) ![[Pasted image 20250729095400.png]]</li></ul><hr><h1 id=using-gaze-in-nlp-models->Using gaze <strong>in</strong> NLP models 🔧<a hidden class=anchor aria-hidden=true href=#using-gaze-in-nlp-models->#</a></h1><p><strong>Word‑level alignment & embeddings.</strong> Align gaze measures to tokens and use them as positional/attention signals or embeddings; recent work explores gaze‑motivated positional encodings and human attention signals.</p><p><input type=checkbox id=zoomCheck-2e7a3 hidden>
<label for=zoomCheck-2e7a3><img class=zoomCheck loading=lazy decoding=async src=./img/1.png alt>
</label><input type=checkbox id=zoomCheck-579e1 hidden>
<label for=zoomCheck-579e1><img class=zoomCheck loading=lazy decoding=async src=./img/2.png alt>
</label><input type=checkbox id=zoomCheck-d6034 hidden>
<label for=zoomCheck-d6034><img class=zoomCheck loading=lazy decoding=async src=./img/3.png alt></label></p><p><strong>Synthetic scanpaths help scale.</strong> Since human gaze is scarce, <strong>Eyettention</strong>‑style scanpath generators and follow‑ups inject <em>synthetic</em> gaze to fine‑tune LMs, improving <strong>GLUE</strong> tasks (especially low‑resource). (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>)</p><p><strong>Task‑specific multitask learning.</strong> Training to predict reading measures jointly with downstream tasks (e.g., <strong>QA</strong> with question preview vs. ordinary reading) can induce more human‑like attention. (<a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)
<input type=checkbox id=zoomCheck-1c412 hidden>
<label for=zoomCheck-1c412><img class=zoomCheck loading=lazy decoding=async src=./img/4.png alt>
</label><strong>What to expect.</strong> Reported gains are real but often <strong>modest</strong> without careful modeling, good alignment, or synthetic data of sufficient quality. That came up repeatedly in the tutorial.</p><p><strong>Examples & pointers:</strong> NER with gaze, compression/paraphrase, readability, parsing—plus general “gaze‑augmented PLMs.” (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.conll-1.8.pdf?utm_source=chatgpt.com" title="[PDF] A Comparative Study on Textual Saliency of Styles from Eye ...">ACL Anthology</a>, <a href="https://arxiv.org/pdf/2310.14676?utm_source=chatgpt.com" title="[PDF] arXiv:2310.14676v1 [cs.CL] 23 Oct 2023">arXiv</a>)</p><hr><h1 id=using-gaze-to-evaluate-nlp->Using gaze to <strong>evaluate</strong> NLP 📏<a hidden class=anchor aria-hidden=true href=#using-gaze-to-evaluate-nlp->#</a></h1><p><strong>Behavioral evaluation</strong> uses <em>online</em> human signals—complementing labels or preferences. We saw applications to <strong>MT</strong> (reading effort), <strong>summarization</strong> (human vs. model saliency), and <strong>readability</strong> (reading‑ease metrics). (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>)</p><p><strong>Case study — Automatic Readability Assessment (ARA).</strong> A new eye‑tracking‑based benchmark correlates model scores with <strong>reading speed</strong>, <strong>skip rate</strong>, <strong>regressions</strong>, and <strong>total fixation time</strong>, revealing weak spots of classic readability formulas. Promising direction for <strong>cognitive evaluation</strong>. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</p><hr><h1 id=psycholinguistics-nlp>Psycholinguistics NLP<a hidden class=anchor aria-hidden=true href=#psycholinguistics-nlp>#</a></h1><ul><li><strong>Surprisal & RTs.</strong> Foundational results show a strong relation between <strong>LM surprisal</strong> and <strong>reading times</strong>; this holds across languages and for many modern LMs—with nuances. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>)</li></ul><p><input type=checkbox id=zoomCheck-b130c hidden>
<label for=zoomCheck-b130c><img class=zoomCheck loading=lazy decoding=async src=./img/5.png alt>
</label>*<strong>Classics to know:</strong> <em>Surprisal theory</em>, <em>Dependency Locality Theory</em>, <em>Uniform Information Density</em>, <em>Cue‑based retrieval/ACT‑R</em>—usually operationalized via parsers/LMs. (<a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>)</p><ul><li><strong>Controlled tests.</strong> Agreement phenomena with GPT‑2 surprisal; embeddings as cognitive features for memory retrieval. (<a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource – Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li></ul><hr><h1 id=are-llms-aligned-with-human-reading->Are LLMs aligned with human reading? 🤖🧍‍♀️<a hidden class=anchor aria-hidden=true href=#are-llms-aligned-with-human-reading->#</a></h1><p>It’s <strong>complicated</strong> (and active in 2023–2025):</p><p><input type=checkbox id=zoomCheck-b47b4 hidden>
<label for=zoomCheck-b47b4><img class=zoomCheck loading=lazy decoding=async src=./img/6.png alt></label></p><ul><li><strong>Bigger isn’t always better:</strong> Larger Transformers can fit <em>worse</em> to human RTs than smaller ones (surprisal‑RT link weakens with size). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>)</li><li><strong>…but layer matters:</strong> <strong>Intermediate layers</strong> may reverse that trend. (<a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>)</li><li><strong>Individual differences:</strong> <strong>Surprisal</strong> better predicts first‑pass RTs for <strong>lower verbal IQ</strong> readers; <strong>entropy</strong> better fits those with <strong>higher working memory</strong>. (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li><li><strong>Text & decoding matter:</strong> PP varies across <strong>generation strategies</strong> and <strong>reading measures</strong>; evaluating <em>produced</em> texts against human reading is informative. (<a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14/?utm_source=chatgpt.com" title="On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>)</li><li><strong>Add cognitive bias:</strong> Injecting <strong>recency biases</strong> (e.g., <strong>ALiBi</strong>) improves LM fit to reading times. (<a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517.pdf?utm_source=chatgpt.com" title="[PDF] Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li></ul><hr><h1 id=modeling-eye-movements-themselves->Modeling eye movements themselves 🛠️<a hidden class=anchor aria-hidden=true href=#modeling-eye-movements-themselves->#</a></h1><p><strong>Cognitive models</strong> (fewer, interpretable parameters): <strong>E‑Z Reader</strong>, <strong>SWIFT</strong>, <strong>SEAM</strong>, <strong>OB1‑Reader</strong>. <strong>ML/NLP models</strong> (data‑hungry, high‑capacity): <strong>Eyettention</strong>, <strong>ScanDL 2.0</strong>, <strong>SP‑EyeGAN</strong>. The recent trend is to combine strengths (e.g., self‑supervised frameworks grounded in cognitive constraints). (<a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://www.sciencedirect.com/science/article/pii/S0749596X23000955?utm_source=chatgpt.com" title="SEAM: An integrated activation-coupled model of sentence ...">ScienceDirect</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://dl.acm.org/doi/10.1145/3591131?utm_source=chatgpt.com" title="An Attention-based Dual-Sequence Model for Predicting Human ...">ACM Digital Library</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>)</p><hr><h1 id=humancentered-applications->Human‑centered applications 🌍<a hidden class=anchor aria-hidden=true href=#humancentered-applications->#</a></h1><ul><li><strong>Language assessment (L2):</strong> Eye movements carry proficiency signals (e.g., <strong>EyeScore</strong>‑style similarity to L1 prototypes).</li><li><strong>Reading impairment screening/monitoring:</strong> Commercial tools (e.g., <strong>Lexplore</strong>) and research platforms point to scalable screening and longitudinal tracking. (<a href="https://eyetechusa.com/?utm_source=chatgpt.com" title="eye model, episcleral venomanometer, scleral depressors">eyetechusa.com</a>)</li><li><strong>Reading comprehension modeling:</strong> Predicting comprehension from gaze during QA is an emerging task on <strong>OneStop</strong>. (<a href="https://arxiv.org/html/2410.04484v1?utm_source=chatgpt.com" title="Fine-Grained Prediction of Reading Comprehension from Eye ...">arXiv</a>)</li></ul><hr><h1 id=how-to-get-started>How to get started<a hidden class=anchor aria-hidden=true href=#how-to-get-started>#</a></h1><ol><li><strong>Pick a dataset</strong> that matches your question (MECO/OneStop/Provo/etc.). (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>)</li><li><strong>Mind the structure</strong> (reader/text effects) and choose proper splits/stats.</li><li><strong>Use a pipeline</strong> (e.g., pymovements) for reproducible preprocessing, AoI mapping, and event detection. (<a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Decide your integration</strong>: (a) <strong>features/embeddings</strong>, (b) <strong>auxiliary losses</strong> (multitask), or (c) <strong>synthetic gaze</strong> + LM fine‑tuning. (<a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>)</li><li><strong>Evaluate cognitively</strong>: add behavioral metrics (e.g., ARA with eye‑tracking) alongside standard accuracy. (<a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li></ol><hr><h1 id=references--links->References & links 🔗<a hidden class=anchor aria-hidden=true href=#references--links->#</a></h1><ul><li><strong>Tutorial slides:</strong> <em>Eye Tracking and NLP</em> (ACL 2025) — many figures and examples here are adapted from the tutorial.</li><li><strong>Foundations:</strong> Rayner’s classic review on eye movements & cognition; eye‑mind assumption background. (<a href="https://andrewd.ces.clemson.edu/courses/cpsc881/papers/reading/Ray98_readingSurvey.pdf?utm_source=chatgpt.com" title="[PDF] Eye Movements in Reading and Information Processing: 20 Years of ...">andrewd.ces.clemson.edu</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com" title="Eye movements in reading and information processing - PubMed">PubMed</a>)</li><li><strong>Perceptual span & physiology:</strong> asymmetries and fovea/cone density. (<a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.00807/full?utm_source=chatgpt.com" title="Investigating the Effectiveness of Spatial Frequencies to the Left and ...">Frontiers</a>, <a href="https://www.ncbi.nlm.nih.gov/books/NBK10848/?utm_source=chatgpt.com" title="Anatomical Distribution of Rods and Cones - Neuroscience - NCBI">NCBI</a>)</li><li><strong>Datasets/initiatives:</strong> MECO, MultiplEYE, OneStop, Provo; toolkit <strong>pymovements</strong>. (<a href="https://meco-read.com/?utm_source=chatgpt.com" title="MECO Multilingual Eye-movement Corpus">meco-read.com</a>, <a href="https://multipleye.eu/?utm_source=chatgpt.com" title="MultiplEYE – Enabling multilingual eye-tracking data collection for ...">multipleye.eu</a>, <a href="https://lacclab.github.io/OneStop-Eye-Movements/?utm_source=chatgpt.com" title="OneStop: A 360-Participant English Eye Tracking Dataset with ...">lacclab.github.io</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/28523601/?utm_source=chatgpt.com" title="A large eye-tracking corpus with predictability norms - PubMed">PubMed</a>, <a href="https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com" title="pymovements: A Python Package for Eye Movement Data Processing">arXiv</a>)</li><li><strong>Gaze for modeling:</strong> NER with gaze; synthetic scanpaths + GLUE; multitask QA. (<a href="https://aclanthology.org/N19-1001/?utm_source=chatgpt.com" title="Entity Recognition at First Sight: Improving NER with Eye Movement ...">ACL Anthology</a>, <a href="https://aclanthology.org/2023.emnlp-main.400.pdf?utm_source=chatgpt.com" title="[PDF] Pre-Trained Language Models Augmented with Synthetic ...">ACL Anthology</a>, <a href="https://aclanthology.org/2024.acl-short.21.pdf?utm_source=chatgpt.com" title="[PDF] Fine-Tuning Pre-Trained Language Models with Gaze Supervision">ACL Anthology</a>, <a href="https://aclanthology.org/2020.conll-1.11.pdf?utm_source=chatgpt.com" title="[PDF] Bridging Information-Seeking Human Gaze and Machine Reading ...">ACL Anthology</a>)</li><li><strong>Behavioral eval:</strong> MT (eye‑tracking), summarization with eye‑gaze, readability via eye‑tracking. (<a href="https://link.springer.com/article/10.1007/s10590-010-9070-9?utm_source=chatgpt.com" title="Eye tracking as an MT evaluation technique | Machine Translation">SpringerLink</a>, <a href="https://aclanthology.org/2024.lrec-main.84/?utm_source=chatgpt.com" title="Analyzing Interpretability of Summarization Model with Eye-gaze ...">ACL Anthology</a>, <a href="https://hundred.org/en/innovations/lexplore?utm_source=chatgpt.com" title="Lexplore - HundrED.org">hundred.org</a>)</li><li><strong>Psycholinguistic links:</strong> Smith & Levy; Demberg & Keller; Shain et al.; Wilcox et al.; Ryu & Lewis; Smith & Vasishth. (<a href="https://lexplore.com/lexplore-assessment?utm_source=chatgpt.com" title="Lexplore AI & Eyetracking Reading Assessment">lexplore.com</a>, <a href="https://eyetechds.com/?utm_source=chatgpt.com" title="EyeTech Digital Systems">eyetechds.com</a>, <a href="https://lexplore.com/?utm_source=chatgpt.com" title="Home | Lexplore | Systematic reading development">lexplore.com</a>, <a href="https://appsource.microsoft.com/en-nl/product/web-apps/spheregen.veyezergraph?tab=Overview&amp;utm_source=chatgpt.com" title="Reading XR - Microsoft AppSource">Appsource – Business Apps</a>, <a href="https://eyetechds.com/eyeon-elite/?utm_source=chatgpt.com" title="EyeOn Elite - EyeTech Digital Systems">eyetechds.com</a>)</li><li><strong>Alignment & recency bias:</strong> Oh & Schuler (TACL 2023); Kuribayashi et al. (2025); Haller et al. (2024); Bolliger et al. (2024); de Varda & Marelli (2024); Clark et al. (COLING 2025). (<a href="https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com" title="Why Does Surprisal From Larger Transformer-Based Language ...">ACL Anthology</a>, <a href="https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com" title="[PDF] A review of machine learning in scanpath analysis for passive gaze ...">dfki.de</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>, <a href="https://aclanthology.org/2024.blackboxnlp-1.14.pdf?utm_source=chatgpt.com" title="[PDF] On the alignment of LM language generation and ... - ACL Anthology">ACL Anthology</a>, <a href="https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com" title="Locally Biased Transformers Better Align with Human Reading Times">ACL Anthology</a>, <a href="https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com" title="Linear Recency Bias During Training Improves Transformers' Fit to ...">ACL Anthology</a>)</li><li><strong>Scanpath modeling:</strong> Eyettention; ScanDL 2.0; SP‑EyeGAN; SEAM; OB1‑Reader. (<a href="https://arxiv.org/abs/2304.10784?utm_source=chatgpt.com" title="Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading">arXiv</a>, <a href="https://www.zora.uzh.ch/id/eprint/278293/1/3725830.pdf?utm_source=chatgpt.com" title="[PDF] ScanDL 2.0: A Generative Model of Eye Movements in Reading ...">Zora</a>, <a href="https://dl.acm.org/doi/10.1145/3588015.3588410?utm_source=chatgpt.com" title="SP-EyeGAN: Generating Synthetic Eye Movement Data with ...">ACM Digital Library</a>, <a href="https://arxiv.org/abs/2303.05221?utm_source=chatgpt.com" title="SEAM: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading">arXiv</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/30080066/?utm_source=chatgpt.com" title="OB1-reader: A model of word recognition and eye movements in text ...">PubMed</a>)</li></ul><hr></details><details id=talk-synthetic-data><summary>Synthetic data for NLP</summary><h1 id=at-a-glance>At a glance<a hidden class=anchor aria-hidden=true href=#at-a-glance>#</a></h1><ul><li>We can’t label everything: synthetic data fills gaps when scraping/manual labels/privacy limits hit.</li><li>Good synthetic data is <strong>task-tailored</strong>, sized right, and <strong>clean</strong> — but beware distribution shift.</li><li>Evaluate two ways: <strong>extrinsic</strong> (downstream task) vs <strong>intrinsic</strong> (what the data itself looks like).</li><li>Diversity sometimes beats raw correctness; noisy but varied sets can still improve models.</li><li>Best results come from <strong>Human ↔︎ AI</strong> collaboration + strong filtering before use.</li></ul><hr><h1 id=where-do-we-get-data>Where do we get data?<a hidden class=anchor aria-hidden=true href=#where-do-we-get-data>#</a></h1><ul><li><strong>Scraping</strong> the web (scale, but licensing/noise).</li><li><strong>Manual labeling</strong> (accurate, expensive).</li><li><strong>Product/system data</strong> (useful but <strong>privacy</strong>-sensitive).</li><li><strong>Creative curation</strong> (high quality, limited volume).<br>Synthetic data tries to extend/augment all of the above.</li></ul><hr><h1 id=what-makes-good-synthetic-data>What makes “good” synthetic data?<a hidden class=anchor aria-hidden=true href=#what-makes-good-synthetic-data>#</a></h1><ul><li><strong>Flexible & task-specific:</strong> format, difficulty, and style match your target task.</li><li><strong>Appropriate size:</strong> enough to move the needle, not so much it drowns real data.</li><li><strong>Clean:</strong> minimal contradictions/formatting errors.</li><li><strong>Aligned distributions:</strong> cover the same <em>kinds</em> of inputs/labels you expect in production.</li></ul><blockquote><p>Why the warning? Because the real joint distribution often differs from the synthetic one:<br>( P_{\text{true}}(x,y) \neq P_{\text{synth}}(x,y) ).<br>Mismatch shows up as off-manifold inputs, <strong>wrong labels</strong>, or flawed reasoning traces.</p></blockquote><hr><h1 id=how-do-we-evaluate-synthetic-data>How do we evaluate synthetic data?<a hidden class=anchor aria-hidden=true href=#how-do-we-evaluate-synthetic-data>#</a></h1><p><strong>Extrinsic:</strong> train/evaluate models on tasks with/without the synthetic set.</p><ul><li>✅ Directly answers “does it help?”.</li><li>❌ Costly, indirect diagnostic signal.</li></ul><p><strong>Intrinsic:</strong> inspect the data/generation process itself.</p><ul><li><strong>Correctness:</strong> e.g., spot-checking/self-instruct style manual audits.</li><li><strong>Diversity/coverage:</strong> does it span plausible inputs? <em>(e.g., DataTune’s bigram diversity as a quick signal).</em></li><li><strong>Privacy, fairness, distributional similarity:</strong> toolkits like <em>SynthTextEval</em> help stress-test.</li><li><strong>Model choice as a proxy:</strong> pick the generator by how well its synthetic data matches human-written (e.g., <em>AgoraBench</em>).</li></ul><hr><h1 id=how-is-synthetic-data-created>How is synthetic data created?<a hidden class=anchor aria-hidden=true href=#how-is-synthetic-data-created>#</a></h1><ul><li><strong>Knowledge distillation (Hinton ’15 → sentence-level):</strong> train a student to mimic a teacher; cheap SFT, keeps style on-target.</li><li><strong>In-context generation:</strong> prompt LMs to produce new labeled examples for arbitrary tasks (Schick & Schütze ’21).</li><li><strong>Self-training/bootstrapping:</strong> fine-tune on the model’s own outputs (Wang et al. ’22).</li><li><strong>Observed effects:</strong> generated text is often <strong>more diverse</strong> (e.g., BERTScore) but only ~½ examples fully correct—<strong>and models can still improve</strong> thanks to coverage.</li></ul><p><strong>Common patterns</strong></p><ul><li><strong>Sampling-based generation:</strong> temperature/top-p, curriculum over difficulty.</li><li><strong>Instruction back-translation:</strong> given an answer (y), generate an instruction (x) that (y) would satisfy.</li><li><strong>Transform existing data:</strong> retrieve/convert to the target format (QA from StackExchange; ground to KGs; rephrase documents for pretraining, Maini ’24).</li><li><strong>Human–AI collaboration:</strong> LLM drafts, humans edit/verify (creativity ↔︎ correctness).</li><li><strong>Symbolic/programmable data:</strong> pretrain on formal languages/grammars to improve generalization (Hu ’24).</li></ul><hr><p>#Filter before you use it</p><ul><li><strong>Diversity filters:</strong> keep sets that are far apart (ROUGE-L, embedding cosine, semantic tags).</li><li><strong>Gradient diversity:</strong> prefer examples that produce different loss gradients → more robust models.</li><li><strong>Quality filters:</strong> pick highest-reward responses (e.g., RAFT-style ranking).</li><li><strong>Correctness filters:</strong> keep chains of thought that reach the right answer; drop inconsistent traces.</li></ul><hr><h1 id=where-synthetic-data-fits-in-the-pipeline>Where synthetic data fits in the pipeline<a hidden class=anchor aria-hidden=true href=#where-synthetic-data-fits-in-the-pipeline>#</a></h1><p><strong>Pretraining</strong></p><ul><li>When real data plateaus: rephrase corpora, verbalize knowledge bases, or mix in <strong>formal languages</strong> for structure.</li><li>Domain adaptation to reduce hallucinations on niche topics.</li></ul><p><strong>Supervised fine-tuning (SFT)</strong></p><ul><li><strong>Distillation</strong> is cheap but can be “style-locked”.</li><li><strong>Self-Instruct / Evol-Instruct</strong> to grow instruction variance.</li><li><strong>MAmmoTH-style</strong> transform/extract tasks (e.g., convert docs to QA).</li></ul><p><strong>RL & feedback</strong></p><ul><li>Minimal supervision, <strong>negative examples</strong>, adaptation to own token distribution.</li><li>Synthetic rewards (e.g., <strong>Prometheus</strong> evaluators; checklist-style rubrics).</li><li>Choosing a “judge”:<ol><li>agreement with <strong>human</strong> prefs (RewardBench),</li><li>agreement with <strong>benchmarks</strong> (re-ranking),</li><li>effectiveness <strong>inside RL</strong> loops.</li></ol></li><li>“Teacher” qualities: good accuracy <strong>and</strong> low variance; even non-RL textual feedback can help—mainly when the base model is already strong.</li></ul><p><strong>Reasoning</strong></p><ul><li>Scale up inference to get longer CoT/PoT traces; pipelines like <strong>OpenThoughts</strong> curate reasoning data.</li><li>Even noisy reasoning data can help; more (curated) data → better reasoning.</li></ul><p><strong>Code</strong></p><ul><li>CodeAlpaca, WaveCoder, WizardCoder, Magicoder, etc.</li><li>Train with <strong>execution feedback</strong> (tests, runtimes).</li><li>Useful data types: single-turn, simulated multi-turn, “fix-the-bug”, and near-duplicate (LeetCode-style) variants.</li></ul><p><strong>Tools & agents</strong></p><ul><li>Gorilla (API-calling), ToolLLM, ToRa (tool-integrated math), AgentTuning, CodeActInstruct, AgentE, “GPT-4-tools” style setups.</li></ul><p><strong>Multimodal / Multilingual</strong></p><ul><li>LLaVA-style visual instruction tuning; multilingual instruction pipelines.</li></ul><hr><h1 id=limitations--open-questions>Limitations & open questions<a hidden class=anchor aria-hidden=true href=#limitations--open-questions>#</a></h1><ul><li>Synthetic sets still trail real data in <strong>size, diversity, and distribution</strong>; production usage (e.g., Anthropic’s Clio) shows gaps.</li><li>In controlled tests, synthetic often <strong>underperforms</strong>; artifacts creep in.</li><li><strong>Synthetic eval data</strong> can <strong>overestimate</strong> model performance.</li><li><strong>Model collapse</strong> risk under recursive self-training (Shumailov ’23).</li><li>We should measure <strong>instance-level quality</strong>, not just dataset averages.</li><li>Governance: “distillation-friendly” models, usage restrictions, provenance tracking.</li></ul><hr><h1 id=practical-checklist-what-id-do>Practical checklist (what I’d do)<a hidden class=anchor aria-hidden=true href=#practical-checklist-what-id-do>#</a></h1><ol><li><strong>Define the target</strong>: task format, difficulty mix, and metrics.</li><li><strong>Generate broad, then filter hard</strong>: diversity → quality → correctness.</li><li><strong>Mix with real data</strong>: keep an anchor set for sanity checks.</li><li><strong>Evaluate both ways</strong>: quick intrinsic dashboards + periodic extrinsic runs.</li><li><strong>Close the loop</strong>: human spot-edits, error mining, and focused regeneration.</li><li><strong>Watch drift</strong>: compare (P_{\text{true}}(x,y)) vs (P_{\text{synth}}(x,y)) over time.</li></ol></details></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://ldomenichelli.github.io/posts/post5/><span class=title>« Prev</span><br><span>❄️ HPLT × NLPL Winter School</span>
</a><a class=next href=https://ldomenichelli.github.io/posts/post1/><span class=title>Next »</span><br><span>Embeddings space 𖦹ׂ ₊˚⊹⋆</span></a></nav></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2025 <a href=https://ldomenichelli.github.io/>lucia's notes</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>