<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Space uniformity | Lucia</title>
<meta name=keywords content><meta name=description content="Definition
A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space  is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post1/><link crossorigin=anonymous href=/assets/css/stylesheet.545ff313fe3387bb6faa83d75fcde7d20949cbe2fb53935708ada17d12aff612.css integrity="sha256-VF/zE/4zh7tvqoPXX83n0glJy+L7U5NXCK2hfRKv9hI=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post1/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="Space uniformity"><meta property="og:description" content="Definition
A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space  is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post1/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="Lucia"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="Space uniformity"><meta name=twitter:description content="Definition
A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space  is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Space uniformity","item":"https://ldomenichelli.github.io/posts/post1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Space uniformity","name":"Space uniformity","description":"Definition A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\n","keywords":[],"articleBody":"Definition A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\nOther measures Average cosine similarity : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution. Partition Isotropy score : Proposed by Arora et al. $$Z(c):= \\sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$ Intrinsic Dimensionality : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy. Linear dimensionality estimate: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions. Non linear dimensionality estimate: - [MLE] –\u003e Levina2005\n- [Two_NN] Variance Explained ratio: measures how much total variance is explained by the first $k$ principal components of data. It requieres an a priori number of PC to examine. Estimating Intrinsic Dimension of a Dataset by MLE from skdim.id import MLE Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 … X_n$ in $\\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\\mathbb{R}^m$ , with unknown $m \\leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\\mathbb{R}^m$ are mapped to close neighbors in the embedding.\nThe basic idea is to fix a point $x$ , assume $f(x) \\sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:\n$$ N(t, x) = \\sum_{i=1}^n \\mathbf{1} {X_i \\in S_x(t)} $$\nwhich counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\\lambda(t)$ of the process $N(t)$ as:\n$$ \\lambda(t) = f(x) V(m) m t^{m-1} $$\nThis follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \\frac{d}{dt} \\left[ V(m) t^m \\right]$ is the surface area of the sphere $S_x(t)$. Letting $\\theta = \\log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:\n$$ L(m, \\theta) = \\int_0^R \\log \\lambda(t) , d N(t) - \\int_0^R \\lambda(t) , dt $$\nThis is an exponential family for which MLEs exist with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$ and are unique. The MLEs must satisfy the likelihood equations\n$$ \\frac{\\partial L}{\\partial \\theta} = \\int_0^R d N(t) - \\int_0^R \\lambda(t) , dt = N(R) - e^\\theta V(m) R^m = 0, $$\n$$ \\frac{\\partial L}{\\partial m} = \\left( \\frac{1}{m} + \\frac{V^{\\prime}(m)}{V(m)} \\right) N(R) + \\int_0^R \\log t \\ d N(t) - e^\\theta V(m) R^m \\left( \\log R + \\frac{V^{\\prime}(m)}{V(m)} \\right) = 0. $$\nSubstituting we get:\n$$ m_R(x) = \\left[ \\frac{1}{N(R, x)} \\sum_{j=1}^{N(R, x)} \\log \\frac{R}{T_j(x)} \\right]^{-1}. $$\nIn practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:\n$$ m_k(x) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\log \\frac{T_k(x)}{T_j(x)} \\right]^{-1} $$\nNote that we omit the last (zero) term in the sum. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),\nFor some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. We will assume that all the data points come from the same “manifold,” and therefore average over all observations.\nThe choice of $k$ clearly affects the estimate. It can be the case that a dataset has different intrinsic dimensions at different scales, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.\n$$ m_k = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}_k(X_i) $$\n$$ m = \\frac{1}{k_2 - k_1 + 1} \\sum_{k=k_1}^{k_2} \\hat{m}_k. $$\nThe only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.\nEstimating Intrinsic Dimension of a Dataset by NN Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \\ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.\nThe volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by $$ \\Delta v_l=\\omega_d\\left(r_l^d-r_{l-1}^d\\right), $$ where $d$ is the dimensionality of the space in which the points are embedded and $\\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\\rho$ : $$ P\\left(\\Delta v_l \\in[v, v+d v]\\right)=\\rho e^{-\\rho v} d v $$\nConsider two shells $\\Delta v_1$ and $\\Delta v_2$, and let $R$ be the quantity $\\frac{\\Delta v_i}{\\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :\n$P(R \\in[\\bar{R}, \\bar{R}+d \\bar{R}]) = \\int_0^{\\infty} d v_i \\int_0^{\\infty} d v_j \\rho^2 e^{-\\rho(v_i+v_j)} {\\frac{v_j}{v_i} \\in[\\bar{R}, \\bar{R}+d \\bar{R}]} = d \\bar{R} \\frac{1}{(1+\\bar{R})^2}$\nwhere 1 represents the indicator function. Dividing by $d \\bar{R}$ we obtain the pdf for $R$ : $$ g(R)=\\frac{1}{(1+R)^2} $$\nThe pdf does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\\mu \\doteq \\frac{r_2}{r_1} \\in[1,+\\infty) . R$ and $\\mu$ are related by equality: $$ R=\\mu^d-1 $$\nThis equation allows to find an explicit formula for the distribution of $\\mu$ : $$ f(\\mu)=d \\mu^{-d-1} 1_{[1,+\\infty]}(\\mu), $$ while the cumulative distribution (cdf) is obtained by integration: $$ F(\\mu)=\\left(1-\\mu^{-d}\\right) 1_{[1,+\\infty]}(\\mu) . $$\nFunctions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.\nThe derivation presented above leads to a simple observation. The value of the intrinsic dimension $d$ can be estimated through the following equation: $$ \\frac{\\log (1-F(\\mu))}{\\log (\\mu)}=d $$\nRemarkably the density $\\rho$ does not appear in this equation, since the $\\operatorname{cdf} F$ is independent of $\\rho$. If we consider the set $S \\subset \\mathbb{R}^2, S \\doteq{(\\log (\\mu),-\\log (1-F(\\mu)))}$, $S$ is contained in a straight line $l \\doteq{(x, y) \\mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:\nCompute the pairwise distances for each point in the dataset $i=1, \\ldots, N$. For each point i find the two shortest distances $r_1$ and $r_2$. For each point i compute $\\mu_i=\\frac{r_2}{r_1}$. Compute the empirical cumulate $F^{e m p}(\\mu)$ by sorting the values of $\\mu$ in an ascending order through a permutation $\\sigma$, then define $F^{e m p}\\left(\\mu_{\\sigma(i)}\\right) \\doteq \\frac{i}{N}$. Fit the points of the plane given by coordinates ${\\ (\\log(\\mu_i), -\\log(1 - F^{\\text{emp}}(\\mu_i))) \\ | \\ i = 1, \\ldots, N\\ }$ with a straight line passing through the origin. Even if the results above are derived in the case of a uniform distribution of points in equations (5) and (7) there is no dependence on the density $\\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.\nProperties of Isotropy Mean Agnosticism:\nIsotropy is a property solely of the covariance matrix of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.\nInvariance to Scalar Multiples of the Covariance Matrix:\nIsotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\\Sigma=Cov(I)$ where $I$ is the identity matrix and $\\lambda \u003e0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the shape of the distribution rather than the overall size of the data spread. Thus, for an isotropic distribution, $Cov(\\lambda)=Cov(\\lambda I)$.\nVariance Distribution Across Dimensions:\nFor a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.\nRotation Invariance:\nAn ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.\nUtilization of Dimensions:\nThere is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.\nGlobal Stability:\nAn isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.\n","wordCount":"1997","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post1/"},"publisher":{"@type":"Organization","name":"Lucia","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="Lucia's Notes (Alt + H)"><img src=https://ldomenichelli.github.io/logo.svg alt="Site icon in header" aria-label=logo height=35>Lucia's Notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=Random><span>Random</span></a></li><li><a href=https://ldomenichelli.github.io/games/ title=Games><span>Games</span></a></li><li><a href=#ZgotmplZ title="Suggested pages"><span>Suggested pages</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>Space uniformity</h1><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#definition>Definition</a></li><li><a href=#other-measures>Other measures</a></li><li><a href=#estimating-intrinsic-dimension-of-a-dataset-by-mle><em>Estimating Intrinsic Dimension of a Dataset by MLE</em></a></li><li><a href=#estimating-intrinsic-dimension-of-a-dataset-by-nn><em>Estimating Intrinsic Dimension of a Dataset by NN</em></a></li><li><a href=#properties-of-isotropy>Properties of Isotropy</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h3 id=definition>Definition<a hidden class=anchor aria-hidden=true href=#definition>#</a></h3><p><em>A distribution is isotropic if its variance is uniformly distributed across all dimensions.</em> Namely, the covariance matrix of an isotropic distribution is proportional to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in <em>n-dimensional vector space</em> is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.</p><h3 id=other-measures>Other measures<a hidden class=anchor aria-hidden=true href=#other-measures>#</a></h3><ol><li><strong>Average cosine similarity</strong> : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution.</li><li><strong>Partition Isotropy score</strong> : Proposed by <a href>Arora et al.</a> $$Z(c):= \sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$</li><li><strong>Intrinsic Dimensionality</strong> : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy.</li></ol><ul><li><em>Linear dimensionality estimate</em>: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions.</li><li><em>Non linear dimensionality estimate</em>:
- [MLE] &ndash;> <a href=https://scikit-dimension.readthedocs.io/en/stable/skdim.id.MLE.html>Levina2005</a><br>- [Two_NN]</li></ul><ol start=5><li><strong>Variance Explained ratio</strong>: measures how much total variance is explained by the first $k$ principal components of data. It requieres an <em>a priori</em> number of <em>PC</em> to examine.</li></ol><h3 id=estimating-intrinsic-dimension-of-a-dataset-by-mle><em>Estimating Intrinsic Dimension of a Dataset by MLE</em><a hidden class=anchor aria-hidden=true href=#estimating-intrinsic-dimension-of-a-dataset-by-mle>#</a></h3><pre><code>from skdim.id import MLE 
</code></pre><p>Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 &mldr; X_n$ in $\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\mathbb{R}^m$ , with unknown $m \leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\mathbb{R}^m$ are mapped to close neighbors in the embedding.</p><p>The basic idea is to fix a point $x$ , assume $f(x) \sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:</p><p>$$
N(t, x) = \sum_{i=1}^n \mathbf{1} {X_i \in S_x(t)}
$$</p><p>which counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\lambda(t)$ of the process $N(t)$ as:</p><p>$$
\lambda(t) = f(x) V(m) m t^{m-1}
$$</p><p>This follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \frac{d}{dt} \left[ V(m) t^m \right]$ is the surface area of the sphere $S_x(t)$. Letting $\theta = \log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:</p><p>$$
L(m, \theta) = \int_0^R \log \lambda(t) , d N(t) - \int_0^R \lambda(t) , dt
$$</p><p>This is an exponential family for which MLEs exist with probability $\rightarrow 1$ as $n \rightarrow \infty$ and are unique. The MLEs must satisfy the likelihood equations</p><p>$$
\frac{\partial L}{\partial \theta} = \int_0^R d N(t) - \int_0^R \lambda(t) , dt = N(R) - e^\theta V(m) R^m = 0,
$$</p><p>$$
\frac{\partial L}{\partial m} = \left( \frac{1}{m} + \frac{V^{\prime}(m)}{V(m)} \right) N(R) + \int_0^R \log t \ d N(t) - e^\theta V(m) R^m \left( \log R + \frac{V^{\prime}(m)}{V(m)} \right) = 0.
$$</p><p>Substituting we get:</p><p>$$
m_R(x) = \left[ \frac{1}{N(R, x)} \sum_{j=1}^{N(R, x)} \log \frac{R}{T_j(x)} \right]^{-1}.
$$</p><p>In practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:</p><p>$$
m_k(x) = \left[ \frac{1}{k-1} \sum_{j=1}^{k-1} \log \frac{T_k(x)}{T_j(x)} \right]^{-1}
$$</p><p><em>Note that we omit the last (zero) term in the sum</em>. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),</p><p>For some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. <em>We will assume that all the data points come from the same &ldquo;manifold,&rdquo; and therefore average over all observations.</em></p><p>The choice of $k$ clearly affects the estimate. <em>It can be the case that a dataset has different intrinsic dimensions at different scales</em>, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.</p><p>$$
m_k = \frac{1}{n} \sum_{i=1}^n \hat{m}_k(X_i)
$$</p><p>$$
m = \frac{1}{k_2 - k_1 + 1} \sum_{k=k_1}^{k_2} \hat{m}_k.
$$</p><p>The only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.</p><h3 id=estimating-intrinsic-dimension-of-a-dataset-by-nn><em>Estimating Intrinsic Dimension of a Dataset by NN</em><a hidden class=anchor aria-hidden=true href=#estimating-intrinsic-dimension-of-a-dataset-by-nn>#</a></h3><p>Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.</p><p>The volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by
$$
\Delta v_l=\omega_d\left(r_l^d-r_{l-1}^d\right),
$$
where $d$ is the dimensionality of the space in which the points are embedded and $\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\rho$ :
$$
P\left(\Delta v_l \in[v, v+d v]\right)=\rho e^{-\rho v} d v
$$</p><p>Consider two shells $\Delta v_1$ and $\Delta v_2$, and let $R$ be the quantity $\frac{\Delta v_i}{\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :</p><p>$P(R \in[\bar{R}, \bar{R}+d \bar{R}]) = \int_0^{\infty} d v_i \int_0^{\infty} d v_j \rho^2 e^{-\rho(v_i+v_j)} {\frac{v_j}{v_i} \in[\bar{R}, \bar{R}+d \bar{R}]} = d \bar{R} \frac{1}{(1+\bar{R})^2}$</p><p>where 1 represents the indicator function. Dividing by $d \bar{R}$ we obtain the pdf for $R$ :
$$
g(R)=\frac{1}{(1+R)^2}
$$</p><p>The <em>pdf</em> does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\mu \doteq \frac{r_2}{r_1} \in[1,+\infty) . R$ and $\mu$ are related by equality:
$$
R=\mu^d-1
$$</p><p>This equation allows to find an explicit formula for the distribution of $\mu$ :
$$
f(\mu)=d \mu^{-d-1} 1_{[1,+\infty]}(\mu),
$$
while the cumulative distribution (<em>cdf</em>) is obtained by integration:
$$
F(\mu)=\left(1-\mu^{-d}\right) 1_{[1,+\infty]}(\mu) .
$$</p><p>Functions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.</p><p>The derivation presented above leads to a simple observation. <em>The value of the intrinsic dimension $d$ can be estimated through the following equation</em>:
$$
\frac{\log (1-F(\mu))}{\log (\mu)}=d
$$</p><p>Remarkably the density $\rho$ does not appear in this equation, since the $\operatorname{cdf} F$ is independent of $\rho$. If we consider the set $S \subset \mathbb{R}^2, S \doteq{(\log (\mu),-\log (1-F(\mu)))}$, $S$ is contained in a straight line $l \doteq{(x, y) \mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:</p><ol><li>Compute the pairwise distances for each point in the dataset $i=1, \ldots, N$.</li><li>For each point i find the two shortest distances $r_1$ and $r_2$.</li><li>For each point i compute $\mu_i=\frac{r_2}{r_1}$.</li><li>Compute the empirical cumulate $F^{e m p}(\mu)$ by sorting the values of $\mu$ in an ascending order through a permutation $\sigma$, then define $F^{e m p}\left(\mu_{\sigma(i)}\right) \doteq \frac{i}{N}$.</li><li>Fit the points of the plane given by coordinates ${\ (\log(\mu_i), -\log(1 - F^{\text{emp}}(\mu_i))) \ | \ i = 1, \ldots, N\ }$ with a straight line passing through the origin.</li></ol><p>Even if the results above are derived in the case of a uniform distribution of points in equations (5) and (7) there is no dependence on the density $\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.</p><h3 id=properties-of-isotropy>Properties of Isotropy<a hidden class=anchor aria-hidden=true href=#properties-of-isotropy>#</a></h3><ol><li><p><strong>Mean Agnosticism</strong>:<br>Isotropy is a property solely of the <em>covariance matrix</em> of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.</p></li><li><p><strong>Invariance to Scalar Multiples of the Covariance Matrix</strong>:<br>Isotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\Sigma=Cov(I)$ where $I$ is the identity matrix and $\lambda >0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the <em>shape</em> of the distribution rather than the overall <em>size</em> of the data spread. Thus, for an isotropic distribution, $Cov(\lambda)=Cov(\lambda I)$.</p></li><li><p><strong>Variance Distribution Across Dimensions</strong>:<br>For a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.</p></li><li><p><strong>Rotation Invariance</strong>:<br>An ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.</p></li><li><p><strong>Utilization of Dimensions</strong>:<br>There is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.</p></li><li><p><strong>Global Stability</strong>:<br>An isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.</p></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2024 <a href=https://ldomenichelli.github.io/>Lucia</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>