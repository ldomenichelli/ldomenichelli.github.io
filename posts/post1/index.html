<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Space uniformity | Lucia</title>
<meta name=keywords content><meta name=description content="Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods."><meta name=author content="Lucia"><link rel=canonical href=https://ldomenichelli.github.io/posts/post1/><link crossorigin=anonymous href=/assets/css/stylesheet.545ff313fe3387bb6faa83d75fcde7d20949cbe2fb53935708ada17d12aff612.css integrity="sha256-VF/zE/4zh7tvqoPXX83n0glJy+L7U5NXCK2hfRKv9hI=" rel="preload stylesheet" as=style><link rel=icon href=https://ldomenichelli.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ldomenichelli.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ldomenichelli.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://ldomenichelli.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ldomenichelli.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ldomenichelli.github.io/posts/post1/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style><meta property="og:title" content="Space uniformity"><meta property="og:description" content="Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods."><meta property="og:type" content="article"><meta property="og:url" content="https://ldomenichelli.github.io/posts/post1/"><meta property="og:image" content="https://ldomenichelli.github.io/logo.png"><meta property="article:section" content="posts"><meta property="og:site_name" content="Lucia"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ldomenichelli.github.io/logo.png"><meta name=twitter:title content="Space uniformity"><meta name=twitter:description content="Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"https://ldomenichelli.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Space uniformity","item":"https://ldomenichelli.github.io/posts/post1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Space uniformity","name":"Space uniformity","description":"Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\n","keywords":[],"articleBody":"Understanding the curse of dimensionality requires more than just examining the representational aspect of data. A key insight lies in the concept of intrinsic dimensionality (ID)—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.\nThe Role of Dimensionality Reduction Dimensionality reduction has gained prominence in machine learning and data science for its ability to simplify complex datasets while preserving essential structure. Methods in this domain are broadly categorized into:\nLinear Techniques:\nPrincipal Component Analysis (PCA) and its variants (e.g., Bouveyron et al. 2011) focus on orthogonal transformations to project data onto a lower-dimensional space. Non-linear Techniques (Manifold Learning):\nThese methods include Isometric Mapping (Tenenbaum et al. 2000), Locally Linear Embedding (Roweis and Saul 2000), and Hessian Eigenmapping (Donoho and Grimes 2003). They aim to capture the underlying structure of data that lies on a non-linear manifold. While most methods require users to specify a target dimension, some techniques adaptively infer it based on the dataset’s intrinsic dimensionality. This adaptability underscores the significance of ID estimation. Models and Methods for Estimating Intrinsic Dimensionality Over the years, researchers have developed diverse models to estimate intrinsic dimensionality. These fall into several categories:\nTopological Approaches: Analyze the tangent space of the manifold using local samples (e.g., Fukunaga and Olsen 1971; Verveer and Duin 1995).\nFractal Measures: Use metrics like the Correlation Dimension (Faloutsos and Kamel 1994) to estimate ID based on space-filling properties of data.\nGraph-based Methods: Employ $k$-nearest neighbors and density metrics to infer ID (Costa and Hero 2004).\nParametric Models: Leverage statistical models to estimate ID, such as those by Levina and Bickel (2004).\nGlobal vs. Local Intrinsic Dimensionality Intrinsic dimensionality measures can be broadly classified into global and local approaches:\nGlobal Measures: Analyze the dataset as a whole, treating all objects uniformly. These measures are well-suited for homogeneous datasets with a single dominant manifold.\nLocal Measures: Focus on the $k$-nearest neighbors of a specific point. These methods are essential for heterogeneous datasets comprising multiple, overlapping manifolds. Notable local ID models include:\nExpansion Dimension (ED) (Karger and Ruhl 2002). Generalized Expansion Dimension (GED) (Houle et al. 2012). Local Intrinsic Dimensionality (LID) (Houle 2013). Local ID measures are particularly relevant in applications such as similarity search, where they can estimate query complexity or optimize search termination. They are also applied in outlier detection and density estimation.\nBalancing Local and Global Insights Machine learning techniques often face challenges like overfitting when relying heavily on local information. To mitigate this, methods such as Local Tangent Space Alignment (LTSA) (Zhang and Zha 2004) combine local and global perspectives by aligning neighborhoods of points while penalizing overfitting during optimization. This balance enables a more comprehensive understanding of the data structure.\nContinuous Intrinsic Dimension In this section, we explore Local Intrinsic Dimensionality (LID), a model that extends intrinsic dimensionality to continuous distributions of distances, as proposed by Houle (2013). LID quantifies the local intrinsic dimensionality (ID) of a feature space by focusing exclusively on the distribution of inter-point distances.\nDefining the Distribution of Distances Let $(\\mathbb{R}^m, \\text{dist})$ represent a domain equipped with a non-negative distance function dist. Consider the distribution of distances with respect to a fixed reference point. This distribution can be modeled as a random variable $\\mathbf{X}$, with support $[0, \\infty)$. The probability density function (PDF) of $\\mathbf{X}$ is denoted by $f_{\\mathbf{X}}$, where $f_{\\mathbf{X}}$ is a non-negative, Lebesgue-integrable function. For any $a, b \\in [0, \\infty)$ such that $a \\leq b$, the probability is given by:\n$$ \\operatorname{Pr}[a \\leq \\mathbf{X} \\leq b] = \\int_a^b f_{\\mathbf{X}}(x) , \\mathrm{d}x. $$\nThe corresponding cumulative density function (CDF) $F_{\\mathbf{X}}$ is defined as:\n$$ F_{\\mathbf{X}}(x) = \\operatorname{Pr}[\\mathbf{X} \\leq x] = \\int_0^x f_{\\mathbf{X}}(u) , \\mathrm{d}u. $$\nFor values where $\\mathbf{X}$ is absolutely continuous at $x$, the CDF $F_{\\mathbf{X}}$ is differentiable at $x$, and its first-order derivative is $f_{\\mathbf{X}}(x)$.\nThe Local Continuous Intrinsic Dimension The local intrinsic dimension at distance $x$ is defined as follows:\nDefinition 1 (Houle, 2013):\nGiven an absolutely continuous random distance variable $\\mathbf{X}$, for any distance threshold $x$ such that $F_{\\mathbf{X}}(x) \u003e 0$, the local continuous intrinsic dimension $\\mathrm{ID}_{\\mathbf{X}}(x)$ of $\\mathbf{X}$ at distance $x$ is:\n$$ \\mathrm{ID}{\\mathbf{X}}(x) \\triangleq \\lim{\\epsilon \\to 0^+} \\frac{\\ln F_{\\mathbf{X}}((1+\\epsilon) x) - \\ln F_{\\mathbf{X}}(x)}{\\ln (1+\\epsilon)} $$\nwherever the limit exists.\nRelation to the Generalized Expansion Dimension The LID definition builds upon the generalized expansion dimension (GED) proposed by Houle et al. (2012a). GED measures dimensionality by comparing neighborhood radii $x$ and $(1+\\epsilon)x$, replacing neighborhood cardinalities with the expected number of neighbors.\nIn essence, the LID formulation quantifies the discriminative power of a distance measure. Both LID and GED share the same closed-form representation, reflecting their foundational equivalence in characterizing local dimensionality.\nTheorem 1 (Houle 2013) Let X be an absolutely continuous random distance variable. If $F_X$ is both positive and differentiable at x, then:\n$$ \\text{ID}_X(x) = \\frac{x f_X(x)}{F_X(x)}. $$\nLocal intrinsic dimensionality (Local ID) has potential for wide application thanks to its very general treatment of distances as a continuous random variable. Direct estimation of $ID_X(x)$, however, requires the knowledge of the distribution of $X$. Extreme value theory, which we survey in the following section, allows the estimation of the limit of $x \\to 0$ without any explicit assumptions of the data distribution other than continuity.\n3. Extreme Value Theory Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes.\nDefinition 2 Let $\\mu \\in R$ and $\\sigma \u003e 0$. The family of generalized extreme value distributions $F_{GEV}$ covers distributions whose cumulative distribution functions have the form:\n$$ F_{GEV} = \\exp \\left( - \\left[ 1 + \\xi \\left( \\frac{x - \\mu}{\\sigma} \\right) \\right]^{-\\frac{1}{\\xi}} \\right) \u0026 \\text{if } \\xi \\neq 0, \\ \\exp \\left( -\\exp \\left( -\\frac{x - \\mu}{\\sigma} \\right) \\right) \u0026 \\text{if } \\xi = 0. $$\nA distribution $G \\in F_{GEV}$ has support:\n$$ \\text{supp}(G) = \\begin{cases} [\\mu - \\frac{\\sigma}{\\xi}, \\infty) \u0026 \\text{when } \\xi \u003e 0, \\ (-\\infty, \\mu - \\frac{\\sigma}{\\xi}] \u0026 \\text{when } \\xi \u003c 0, \\ (-\\infty, \\infty) \u0026 \\text{if } \\xi = 0. \\end{cases} $$\nIts best-known theorem, attributed in parts to Fisher and Tippett (1928), and Gnedenko (1943), states that the maximum of n independent identically-distributed random variables (after proper renormalization) converges in distribution to a generalized extreme value distribution as $n \\to \\infty$ .\nTheorem 2 (Fisher-Tippett-Gnedenko)\nLet $(X_i)_{i \\in N}$ be a sequence of independent identically-distributed random variables and let $M_n=max X_i$ If there exist a sequence of positive constants $a_n$, $n \\in N$ and a sequence of constants $b_n$ , $n \\in N$ , such that:\n$$ \\lim_{n \\to \\infty} P\\left( \\frac{M_n - b_n}{a_n} \\leq x \\right) = F(x), $$\nthen F(x) belongs to the generalized extreme value family.\nIsotropy A distribution is isotropic if its variance is uniformly distributed across all dimensions. Namely, the covariance matrix of an isotropic distribution is proportion al to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in n-dimensional vector space is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.\nOther measures Avg cosine similarity : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution. Partition Isotropy score : Proposed by Arora et al. $$Z(c):= \\sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$ Intrinsic Dimensionality : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy. Linear dimensionality estimate: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions. *Non linear dimensionality estimate: - [MLE] –\u003e Levina2005\n- [Moment’s Method]–\u003e() - [Two_NN] Variance Explained ratio: measures how much total variance is explained by the first $k$ principal components of data. It requieres an a priori number of PC to examine. Estimating Intrinsic Dimension of a Dataset by MLE from skdim.id import MLE Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 … X_n$ in $\\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\\mathbb{R}^m$ , with unknown $m \\leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\\mathbb{R}^m$ are mapped to close neighbors in the embedding.\nThe basic idea is to fix a point $x$ , assume $f(x) \\sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:\n$$ N(t, x) = \\sum_{i=1}^n \\mathbf{1} {X_i \\in S_x(t)} $$\nwhich counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\\lambda(t)$ of the process $N(t)$ as:\n$$ \\lambda(t) = f(x) V(m) m t^{m-1} $$\nThis follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \\frac{d}{dt} \\left[ V(m) t^m \\right]$ is the surface area of the sphere $S_x(t)$. Letting $\\theta = \\log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:\n$$ L(m, \\theta) = \\int_0^R \\log \\lambda(t) , d N(t) - \\int_0^R \\lambda(t) , dt $$\nThis is an exponential family for which MLEs exist with probability $\\rightarrow 1$ as $n \\rightarrow \\infty$ and are unique. The MLEs must satisfy the likelihood equations\n$$ \\frac{\\partial L}{\\partial \\theta} = \\int_0^R d N(t) - \\int_0^R \\lambda(t) , dt = N(R) - e^\\theta V(m) R^m = 0, $$\n$$ \\frac{\\partial L}{\\partial m} = \\left( \\frac{1}{m} + \\frac{V^{\\prime}(m)}{V(m)} \\right) N(R) + \\int_0^R \\log t , d N(t)\ne^\\theta V(m) R^m \\left( \\log R + \\frac{V^{\\prime}(m)}{V(m)} \\right) = 0. $$ Substituting we get:\n$$ m_R(x) = \\left[ \\frac{1}{N(R, x)} \\sum_{j=1}^{N(R, x)} \\log \\frac{R}{T_j(x)} \\right]^{-1}. $$\nIn practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:\n$$ m_k(x) = \\left[ \\frac{1}{k-1} \\sum_{j=1}^{k-1} \\log \\frac{T_k(x)}{T_j(x)} \\right]^{-1} $$\nNote that we omit the last (zero) term in the sum. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),\nFor some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. We will assume that all the data points come from the same “manifold,” and therefore average over all observations.\nThe choice of $k$ clearly affects the estimate. It can be the case that a dataset has different intrinsic dimensions at different scales, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.\n$$ m_k = \\frac{1}{n} \\sum_{i=1}^n \\hat{m}_k(X_i) $$\n$$ m = \\frac{1}{k_2 - k_1 + 1} \\sum_{k=k_1}^{k_2} \\hat{m}_k. $$\nThe only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.\nEstimating Intrinsic Dimension of a Dataset by NN Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \\ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.\nThe volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by $$ \\Delta v_l=\\omega_d\\left(r_l^d-r_{l-1}^d\\right), $$ where $d$ is the dimensionality of the space in which the points are embedded and $\\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\\rho$ : $$ P\\left(\\Delta v_l \\in[v, v+d v]\\right)=\\rho e^{-\\rho v} d v $$\nConsider two shells $\\Delta v_1$ and $\\Delta v_2$, and let $R$ be the quantity $\\frac{\\Delta v_i}{\\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :\n$$ P(R \\in[\\bar{R}, \\bar{R}+d \\bar{R}]) = \\int_0^{\\infty} d v_i \\int_0^{\\infty} d v_j \\rho^2 e^{-\\rho\\left(v_i+v_j\\right)} \\left{\\frac{v_j}{v_i} \\in[\\bar{R}, \\bar{R}+d \\bar{R}]\\right} = d \\bar{R} \\frac{1}{(1+\\bar{R})^2}. $$\nwhere 1 represents the indicator function. Dividing by $d \\bar{R}$ we obtain the pdf for $R$ : $$ g(R)=\\frac{1}{(1+R)^2} $$\nThe pdf does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\\mu \\doteq \\frac{r_2}{r_1} \\in[1,+\\infty) . R$ and $\\mu$ are related by equality: $$ R=\\mu^d-1 $$\nThis equation allows to find an explicit formula for the distribution of $\\mu$ : $$ f(\\mu)=d \\mu^{-d-1} 1_{[1,+\\infty]}(\\mu), $$ while the cumulative distribution (cdf) is obtained by integration: $$ F(\\mu)=\\left(1-\\mu^{-d}\\right) 1_{[1,+\\infty]}(\\mu) . $$\nFunctions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.\nThe derivation presented above leads to a simple observation. The value of the intrinsic dimension $d$ can be estimated through the following equation: $$ \\frac{\\log (1-F(\\mu))}{\\log (\\mu)}=d $$\nRemarkably the density $\\rho$ does not appear in this equation, since the $\\operatorname{cdf} F$ is independent of $\\rho$. If we consider the set $S \\subset \\mathbb{R}^2, S \\doteq{(\\log (\\mu),-\\log (1-F(\\mu)))}$, $S$ is contained in a straight line $l \\doteq{(x, y) \\mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:\nCompute the pairwise distances for each point in the dataset $i=1, \\ldots, N$. For each point i find the two shortest distances $r_1$ and $r_2$. For each point i compute $\\mu_i=\\frac{r_2}{r_1}$. Compute the empirical cumulate $F^{e m p}(\\mu)$ by sorting the values of $\\mu$ in an ascending order through a permutation $\\sigma$, then define $F^{e m p}\\left(\\mu_{\\sigma(i)}\\right) \\doteq \\frac{i}{N}$. Fit the points of the plane given by coordinates ${\\ (\\log(\\mu_i), -\\log(1 - F^{\\text{emp}}(\\mu_i))) \\ | \\ i = 1, \\ldots, N\\ }$ with a straight line passing through the origin. Even if the results above are derived in the case of a uniform distribution of points there is no dependence on the density $\\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.\nEstimating Intrinsic Dimension of a Dataset by Moment’s Method from skdim.id import TwoNN Properties of Isotropy Mean Agnosticism:\nIsotropy is a property solely of the covariance matrix of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.\nInvariance to Scalar Multiples of the Covariance Matrix:\nIsotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\\Sigma=Cov(I)$ where $I$ is the identity matrix and $\\lambda \u003e0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the shape of the distribution rather than the overall size of the data spread. Thus, for an isotropic distribution, $Cov(\\lambda)=Cov(\\lambda I)$.\nVariance Distribution Across Dimensions:\nFor a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.\nRotation Invariance:\nAn ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.\nUtilization of Dimensions:\nThere is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.\nGlobal Stability:\nAn isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.\n","wordCount":"3159","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Lucia"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ldomenichelli.github.io/posts/post1/"},"publisher":{"@type":"Organization","name":"Lucia","logo":{"@type":"ImageObject","url":"https://ldomenichelli.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ldomenichelli.github.io/ accesskey=h title="Lucia's Notes (Alt + H)"><img src=https://ldomenichelli.github.io/logo.svg alt="Site icon in header" aria-label=logo height=35>Lucia's Notes</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><button id=menu-trigger aria-haspopup=menu aria-label="Menu Button"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><ul class="menu hidden"><li><a href=https://ldomenichelli.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ldomenichelli.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://ldomenichelli.github.io/random/ title=Random><span>Random</span></a></li><li><a href=https://ldomenichelli.github.io/games/ title=Games><span>Games</span></a></li><li><a href=#ZgotmplZ title="Suggested pages"><span>Suggested pages</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ldomenichelli.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ldomenichelli.github.io/posts/></a></div><h1 class=post-title>Space uniformity</h1><div class=post-meta>Lucia</div></header><div class="toc side left"><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-role-of-dimensionality-reduction>The Role of Dimensionality Reduction</a></li><li><a href=#models-and-methods-for-estimating-intrinsic-dimensionality>Models and Methods for Estimating Intrinsic Dimensionality</a></li><li><a href=#global-vs-local-intrinsic-dimensionality>Global vs. Local Intrinsic Dimensionality</a></li><li><a href=#balancing-local-and-global-insights>Balancing Local and Global Insights</a></li></ul><ul><li><a href=#defining-the-distribution-of-distances>Defining the Distribution of Distances</a></li><li><a href=#the-local-continuous-intrinsic-dimension>The Local Continuous Intrinsic Dimension</a><ul><li><a href=#relation-to-the-generalized-expansion-dimension>Relation to the Generalized Expansion Dimension</a></li></ul></li><li><a href=#theorem-1>Theorem 1</a></li><li><a href=#3-extreme-value-theory>3. Extreme Value Theory</a><ul><li><a href=#definition-2>Definition 2</a></li><li><a href=#theorem-2>Theorem 2</a></li><li><a href=#isotropy>Isotropy</a></li><li><a href=#other-measures>Other measures</a></li><li><a href=#estimating-intrinsic-dimension-of-a-dataset-by-mle><em>Estimating Intrinsic Dimension of a Dataset by MLE</em></a></li><li><a href=#estimating-intrinsic-dimension-of-a-dataset-by-nn><em>Estimating Intrinsic Dimension of a Dataset by NN</em></a></li><li><a href=#estimating-intrinsic-dimension-of-a-dataset-by-moments-method><em>Estimating Intrinsic Dimension of a Dataset by Moment&rsquo;s Method</em></a></li><li><a href=#properties-of-isotropy>Properties of Isotropy</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Understanding the <strong>curse of dimensionality</strong> requires more than just examining the representational aspect of data. A key insight lies in the concept of <strong>intrinsic dimensionality (ID)</strong>—the number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.</p><h2 id=the-role-of-dimensionality-reduction>The Role of Dimensionality Reduction<a hidden class=anchor aria-hidden=true href=#the-role-of-dimensionality-reduction>#</a></h2><p>Dimensionality reduction has gained prominence in machine learning and data science for its ability to simplify complex datasets while preserving essential structure. Methods in this domain are broadly categorized into:</p><ol><li><p><strong>Linear Techniques:</strong></p><ul><li><strong>Principal Component Analysis (PCA)</strong> and its variants (e.g., Bouveyron et al. 2011) focus on orthogonal transformations to project data onto a lower-dimensional space.</li></ul></li><li><p><strong>Non-linear Techniques (Manifold Learning):</strong></p><ul><li>These methods include <strong>Isometric Mapping (Tenenbaum et al. 2000)</strong>, <strong>Locally Linear Embedding (Roweis and Saul 2000)</strong>, and <strong>Hessian Eigenmapping (Donoho and Grimes 2003)</strong>.</li><li>They aim to capture the underlying structure of data that lies on a non-linear manifold.</li></ul></li></ol><p>While most methods require users to specify a target dimension, some techniques adaptively infer it based on the dataset&rsquo;s intrinsic dimensionality. This adaptability underscores the significance of ID estimation.
<input type=checkbox id=zoomCheck-8acd3 hidden>
<label for=zoomCheck-8acd3><img class=zoomCheck loading=lazy decoding=async src=img/tax.png alt=taxonomy></label></p><h2 id=models-and-methods-for-estimating-intrinsic-dimensionality>Models and Methods for Estimating Intrinsic Dimensionality<a hidden class=anchor aria-hidden=true href=#models-and-methods-for-estimating-intrinsic-dimensionality>#</a></h2><p>Over the years, researchers have developed diverse models to estimate intrinsic dimensionality. These fall into several categories:</p><ul><li><p><strong>Topological Approaches:</strong>
Analyze the tangent space of the manifold using local samples (e.g., Fukunaga and Olsen 1971; Verveer and Duin 1995).</p></li><li><p><strong>Fractal Measures:</strong>
Use metrics like the <strong>Correlation Dimension</strong> (Faloutsos and Kamel 1994) to estimate ID based on space-filling properties of data.</p></li><li><p><strong>Graph-based Methods:</strong>
Employ $k$-nearest neighbors and density metrics to infer ID (Costa and Hero 2004).</p></li><li><p><strong>Parametric Models:</strong>
Leverage statistical models to estimate ID, such as those by Levina and Bickel (2004).</p></li></ul><h2 id=global-vs-local-intrinsic-dimensionality>Global vs. Local Intrinsic Dimensionality<a hidden class=anchor aria-hidden=true href=#global-vs-local-intrinsic-dimensionality>#</a></h2><p>Intrinsic dimensionality measures can be broadly classified into <strong>global</strong> and <strong>local</strong> approaches:</p><ul><li><p><strong>Global Measures:</strong>
Analyze the dataset as a whole, treating all objects uniformly. These measures are well-suited for homogeneous datasets with a single dominant manifold.</p></li><li><p><strong>Local Measures:</strong>
Focus on the $k$-nearest neighbors of a specific point. These methods are essential for heterogeneous datasets comprising multiple, overlapping manifolds. Notable local ID models include:</p><ul><li><strong>Expansion Dimension (ED)</strong> (Karger and Ruhl 2002).</li><li><strong>Generalized Expansion Dimension (GED)</strong> (Houle et al. 2012).</li><li><strong>Local Intrinsic Dimensionality (LID)</strong> (Houle 2013).</li></ul></li></ul><p>Local ID measures are particularly relevant in applications such as similarity search, where they can estimate query complexity or optimize search termination. They are also applied in outlier detection and density estimation.</p><h2 id=balancing-local-and-global-insights>Balancing Local and Global Insights<a hidden class=anchor aria-hidden=true href=#balancing-local-and-global-insights>#</a></h2><p>Machine learning techniques often face challenges like overfitting when relying heavily on local information. To mitigate this, methods such as <strong>Local Tangent Space Alignment (LTSA)</strong> (Zhang and Zha 2004) combine local and global perspectives by aligning neighborhoods of points while penalizing overfitting during optimization. This balance enables a more comprehensive understanding of the data structure.</p><h1 id=continuous-intrinsic-dimension>Continuous Intrinsic Dimension<a hidden class=anchor aria-hidden=true href=#continuous-intrinsic-dimension>#</a></h1><p>In this section, we explore <strong>Local Intrinsic Dimensionality (LID)</strong>, a model that extends intrinsic dimensionality to continuous distributions of distances, as proposed by Houle (2013). LID quantifies the <strong>local intrinsic dimensionality (ID)</strong> of a feature space by focusing exclusively on the distribution of inter-point distances.</p><h2 id=defining-the-distribution-of-distances>Defining the Distribution of Distances<a hidden class=anchor aria-hidden=true href=#defining-the-distribution-of-distances>#</a></h2><p>Let $(\mathbb{R}^m, \text{dist})$ represent a domain equipped with a non-negative distance function <code>dist</code>. Consider the distribution of distances with respect to a fixed reference point. This distribution can be modeled as a random variable $\mathbf{X}$, with support $[0, \infty)$. The probability density function (PDF) of $\mathbf{X}$ is denoted by $f_{\mathbf{X}}$, where $f_{\mathbf{X}}$ is a non-negative, Lebesgue-integrable function. For any $a, b \in [0, \infty)$ such that $a \leq b$, the probability is given by:</p><p>$$
\operatorname{Pr}[a \leq \mathbf{X} \leq b] = \int_a^b f_{\mathbf{X}}(x) , \mathrm{d}x.
$$</p><p>The corresponding cumulative density function (CDF) $F_{\mathbf{X}}$ is defined as:</p><p>$$
F_{\mathbf{X}}(x) = \operatorname{Pr}[\mathbf{X} \leq x] = \int_0^x f_{\mathbf{X}}(u) , \mathrm{d}u.
$$</p><p>For values where $\mathbf{X}$ is absolutely continuous at $x$, the CDF $F_{\mathbf{X}}$ is differentiable at $x$, and its first-order derivative is $f_{\mathbf{X}}(x)$.</p><h2 id=the-local-continuous-intrinsic-dimension>The Local Continuous Intrinsic Dimension<a hidden class=anchor aria-hidden=true href=#the-local-continuous-intrinsic-dimension>#</a></h2><p>The local intrinsic dimension at distance $x$ is defined as follows:</p><p><strong>Definition 1 (Houle, 2013):</strong><br>Given an absolutely continuous random distance variable $\mathbf{X}$, for any distance threshold $x$ such that $F_{\mathbf{X}}(x) > 0$, the <strong>local continuous intrinsic dimension</strong> $\mathrm{ID}_{\mathbf{X}}(x)$ of $\mathbf{X}$ at distance $x$ is:</p><p>$$
\mathrm{ID}<em>{\mathbf{X}}(x) \triangleq \lim</em>{\epsilon \to 0^+} \frac{\ln F_{\mathbf{X}}((1+\epsilon) x) - \ln F_{\mathbf{X}}(x)}{\ln (1+\epsilon)}
$$</p><p>wherever the limit exists.</p><h3 id=relation-to-the-generalized-expansion-dimension>Relation to the Generalized Expansion Dimension<a hidden class=anchor aria-hidden=true href=#relation-to-the-generalized-expansion-dimension>#</a></h3><p>The LID definition builds upon the <strong>generalized expansion dimension</strong> (GED) proposed by Houle et al. (2012a). GED measures dimensionality by comparing neighborhood radii $x$ and $(1+\epsilon)x$, replacing neighborhood cardinalities with the <strong>expected number of neighbors</strong>.</p><p>In essence, the LID formulation quantifies the discriminative power of a distance measure. Both LID and GED share the same closed-form representation, reflecting their foundational equivalence in characterizing local dimensionality.</p><h2 id=theorem-1>Theorem 1<a hidden class=anchor aria-hidden=true href=#theorem-1>#</a></h2><p><em>(Houle 2013)</em>
Let <em>X</em> be an absolutely continuous random distance variable. If $F_X$ is both positive and differentiable at <em>x</em>, then:</p><p>$$
\text{ID}_X(x) = \frac{x f_X(x)}{F_X(x)}.
$$</p><p>Local intrinsic dimensionality (Local ID) has potential for wide application thanks to its very general treatment of distances as a continuous random variable. Direct estimation of $ID_X(x)$, however, requires the knowledge of the distribution of $X$. Extreme value theory, which we survey in the following section, allows the estimation of the limit of $x \to 0$ without any explicit assumptions of the data distribution other than continuity.</p><hr><h2 id=3-extreme-value-theory>3. Extreme Value Theory<a hidden class=anchor aria-hidden=true href=#3-extreme-value-theory>#</a></h2><p>Extreme value theory is concerned with the modeling of what can be regarded as the extreme behavior of stochastic processes.</p><h3 id=definition-2>Definition 2<a hidden class=anchor aria-hidden=true href=#definition-2>#</a></h3><p>Let $\mu \in R$ and $\sigma > 0$. The family of <strong>generalized extreme value distributions</strong> $F_{GEV}$ covers distributions whose cumulative distribution functions have the form:</p><p>$$
F_{GEV} =
\exp \left( - \left[ 1 + \xi \left( \frac{x - \mu}{\sigma} \right) \right]^{-\frac{1}{\xi}} \right) & \text{if } \xi \neq 0, \
\exp \left( -\exp \left( -\frac{x - \mu}{\sigma} \right) \right) & \text{if } \xi = 0.
$$</p><p>A distribution $G \in F_{GEV}$ has support:</p><p>$$
\text{supp}(G) =
\begin{cases}
[\mu - \frac{\sigma}{\xi}, \infty) & \text{when } \xi > 0, \
(-\infty, \mu - \frac{\sigma}{\xi}] & \text{when } \xi &lt; 0, \
(-\infty, \infty) & \text{if } \xi = 0.
\end{cases}
$$</p><p>Its best-known theorem, attributed in parts to Fisher and Tippett <em>(1928)</em>, and Gnedenko <em>(1943)</em>, states that the maximum of <em>n</em> independent identically-distributed random variables (after proper renormalization) converges in distribution to a generalized extreme value distribution as $n \to \infty$ .</p><h3 id=theorem-2>Theorem 2<a hidden class=anchor aria-hidden=true href=#theorem-2>#</a></h3><p><em>(Fisher-Tippett-Gnedenko)</em><br>Let $(X_i)_{i \in N}$ be a sequence of independent identically-distributed random variables and let $M_n=max X_i$ If there exist a sequence of positive constants $a_n$, $n \in N$ and a sequence of constants $b_n$ , $n \in N$ , such that:</p><p>$$
\lim_{n \to \infty} P\left( \frac{M_n - b_n}{a_n} \leq x \right) = F(x),
$$</p><p>then <em>F(x)</em> belongs to the generalized extreme value family.</p><h3 id=isotropy>Isotropy<a hidden class=anchor aria-hidden=true href=#isotropy>#</a></h3><p><em>A distribution is isotropic if its variance is uniformly distributed across all dimensions.</em> Namely, the covariance matrix of an isotropic distribution is proportion
al to the identity matrix. Conversely, an anisotropic distribution of data is one where the variance is dominated by a single dimension. For example, a line in <em>n-dimensional vector space</em> is maximally anisotropic. Robust isotropy metrics should return maximally isotropic scores for balls and minimally isotropic (i.e. anisotropic) scores for lines.</p><h3 id=other-measures>Other measures<a hidden class=anchor aria-hidden=true href=#other-measures>#</a></h3><ol><li><strong>Avg cosine similarity</strong> : Calculated as one minus the average cosine similarity of $N$ reandomly sampled pairs of points from $X$, data distribution.</li><li><strong>Partition Isotropy score</strong> : Proposed by <a href>Arora et al.</a> $$Z(c):= \sum_x exp(c^T x) $$ where $c$ is chosen from the eigenspectrum of $XX^T$</li><li><strong>Intrinsic Dimensionality</strong> : To compute the true dimension of a given manifold from which we assume a point cloud has been sampled. Dividing by $n$, the number of samples, gives us a normalized score of isotropy.</li></ol><ul><li><em>Linear dimensionality estimate</em>: *Principal Component Analysis to measure the number of significant components. The cumulative explained variance can provide a threshold for identifying intrinsic dimensions.</li><li>*Non linear dimensionality estimate:
- [MLE] &ndash;> <a href=https://scikit-dimension.readthedocs.io/en/stable/skdim.id.MLE.html>Levina2005</a><br>- [Moment&rsquo;s Method]&ndash;>()
- [Two_NN]</li></ul><ol start=5><li><strong>Variance Explained ratio</strong>: measures how much total variance is explained by the first $k$ principal components of data. It requieres an <em>a priori</em> number of <em>PC</em> to examine.</li></ol><h3 id=estimating-intrinsic-dimension-of-a-dataset-by-mle><em>Estimating Intrinsic Dimension of a Dataset by MLE</em><a hidden class=anchor aria-hidden=true href=#estimating-intrinsic-dimension-of-a-dataset-by-mle>#</a></h3><pre><code>from skdim.id import MLE 
</code></pre><p>Aim is to derive the maximum likelihood estimator (MLE) of the dimension $m$ from i.i.d. observations $X_1 &mldr; X_n$ in $\mathbb{R}^p$ . The observations represent an embedding of a lower-dimensional sample, $X_i = g(Y_i)$ , where $Y_i$ are sampled from an unknown smooth density $f$ on $\mathbb{R}^m$ , with unknown $m \leq p$, and $g$ is a continuous and sufficiently smooth (but not necessarily globally isometric) mapping. This assumption ensures that close neighbors in $\mathbb{R}^m$ are mapped to close neighbors in the embedding.</p><p>The basic idea is to fix a point $x$ , assume $f(x) \sim const$ in a small sphere $S_x(R)$ of radius $R$ around $x$, and treat the observations as a homogeneous Poisson process in $S_x(R)$ . Consider the inhomogeneous process:</p><p>$$
N(t, x) = \sum_{i=1}^n \mathbf{1} {X_i \in S_x(t)}
$$</p><p>which counts observations within distance $t$ from $x$ . Approximating this binomial fixed $n$ process by a Poisson process and suppressing the dependence on $x$ for now, we can write the rate $\lambda(t)$ of the process $N(t)$ as:</p><p>$$
\lambda(t) = f(x) V(m) m t^{m-1}
$$</p><p>This follows immediately from the Poisson process properties since $( V(m) m t^{m-1} = \frac{d}{dt} \left[ V(m) t^m \right]$ is the surface area of the sphere $S_x(t)$. Letting $\theta = \log f(x)$ , we can write the log-likelihood of the observed process $N(t)$ as:</p><p>$$
L(m, \theta) = \int_0^R \log \lambda(t) , d N(t) - \int_0^R \lambda(t) , dt
$$</p><p>This is an exponential family for which MLEs exist with probability $\rightarrow 1$ as $n \rightarrow \infty$ and are unique. The MLEs must satisfy the likelihood equations</p><p>$$
\frac{\partial L}{\partial \theta} = \int_0^R d N(t) - \int_0^R \lambda(t) , dt = N(R) - e^\theta V(m) R^m = 0,
$$</p><p>$$
\frac{\partial L}{\partial m} = \left( \frac{1}{m} + \frac{V^{\prime}(m)}{V(m)} \right) N(R) + \int_0^R \log t , d N(t)</p><ul><li>e^\theta V(m) R^m \left( \log R + \frac{V^{\prime}(m)}{V(m)} \right) = 0.
$$</li></ul><p>Substituting we get:</p><p>$$
m_R(x) = \left[ \frac{1}{N(R, x)} \sum_{j=1}^{N(R, x)} \log \frac{R}{T_j(x)} \right]^{-1}.
$$</p><p>In practice, it may be more convenient to fix the number of neighbors $k$ rather than the radius of the sphere $R$ . Then the estimate becomes:</p><p>$$
m_k(x) = \left[ \frac{1}{k-1} \sum_{j=1}^{k-1} \log \frac{T_k(x)}{T_j(x)} \right]^{-1}
$$</p><p><em>Note that we omit the last (zero) term in the sum</em>. One could divide by $k-2$ rather than $k-1$ to make the estimator asymptotically unbiased (not shown here),</p><p>For some applications, one may want to evaluate local dimension estimates at every data point or average estimated dimensions within data clusters. <em>We will assume that all the data points come from the same &ldquo;manifold,&rdquo; and therefore average over all observations.</em></p><p>The choice of $k$ clearly affects the estimate. <em>It can be the case that a dataset has different intrinsic dimensions at different scales</em>, e.g. a line with noise added to it can be viewed as either $1D$ or $2D$. In such a case, it is informative to have different estimates at different scales. In general, for our estimator to work well, the sphere should be small and contain sufficiently many points, and we have work in progress on choosing such a $k$ automatically.</p><p>$$
m_k = \frac{1}{n} \sum_{i=1}^n \hat{m}_k(X_i)
$$</p><p>$$
m = \frac{1}{k_2 - k_1 + 1} \sum_{k=k_1}^{k_2} \hat{m}_k.
$$</p><p>The only parameters to set for this method are $k_1$ and $k_2$ , and the computational cost is essentially the cost of finding $k_2$ nearest neighbors for every point, which has to be done for most manifold projection methods anyway.</p><h3 id=estimating-intrinsic-dimension-of-a-dataset-by-nn><em>Estimating Intrinsic Dimension of a Dataset by NN</em><a hidden class=anchor aria-hidden=true href=#estimating-intrinsic-dimension-of-a-dataset-by-nn>#</a></h3><p>Let $i$ be a point in the dataset, and consider the list of its first $k$ nearest neighbors; let $r_1, r_2, \ldots, r_k$ be a sorted list of their distances from $i$. Thus, $r_1$ is the distance between $i$ and its nearest neighbor, $r_2$ is the distance with its second nearest neighbor and so on; in this definition we conventionally set $r_0=0$.</p><p>The volume of the hypersferical shell enclosed between two successive neighbors $l-1$ and $l$ is given by
$$
\Delta v_l=\omega_d\left(r_l^d-r_{l-1}^d\right),
$$
where $d$ is the dimensionality of the space in which the points are embedded and $\omega_d$ is the volume of the $d$-sphere with unitary radius. It can be proved (see SI for a derivation) that, if the density is constant around point $i$, all the $\Delta v_l$ are independently drawn from an exponential distribution with rate equal to the density $\rho$ :
$$
P\left(\Delta v_l \in[v, v+d v]\right)=\rho e^{-\rho v} d v
$$</p><p>Consider two shells $\Delta v_1$ and $\Delta v_2$, and let $R$ be the quantity $\frac{\Delta v_i}{\Delta v_j}$; the previous considerations allow us, in the case of constant density, to compute exactly the probability distribution (pdf) of $R$ :</p><p>$$
P(R \in[\bar{R}, \bar{R}+d \bar{R}]) =
\int_0^{\infty} d v_i \int_0^{\infty} d v_j \rho^2 e^{-\rho\left(v_i+v_j\right)}
\left{\frac{v_j}{v_i} \in[\bar{R}, \bar{R}+d \bar{R}]\right}
= d \bar{R} \frac{1}{(1+\bar{R})^2}.
$$</p><p>where 1 represents the indicator function. Dividing by $d \bar{R}$ we obtain the pdf for $R$ :
$$
g(R)=\frac{1}{(1+R)^2}
$$</p><p>The <em>pdf</em> does not depend explicitly on the dimensionality $d$, which appears only in the definition of $R$. In order to work with a cdf depending explicitly on $d$ we define quantity $\mu \doteq \frac{r_2}{r_1} \in[1,+\infty) . R$ and $\mu$ are related by equality:
$$
R=\mu^d-1
$$</p><p>This equation allows to find an explicit formula for the distribution of $\mu$ :
$$
f(\mu)=d \mu^{-d-1} 1_{[1,+\infty]}(\mu),
$$
while the cumulative distribution (<em>cdf</em>) is obtained by integration:
$$
F(\mu)=\left(1-\mu^{-d}\right) 1_{[1,+\infty]}(\mu) .
$$</p><p>Functions $f$ and $F$ are independent of the local density, but depend explicitly on the intrinsic dimension $d$.</p><p>The derivation presented above leads to a simple observation. <em>The value of the intrinsic dimension $d$ can be estimated through the following equation</em>:
$$
\frac{\log (1-F(\mu))}{\log (\mu)}=d
$$</p><p>Remarkably the density $\rho$ does not appear in this equation, since the $\operatorname{cdf} F$ is independent of $\rho$. If we consider the set $S \subset \mathbb{R}^2, S \doteq{(\log (\mu),-\log (1-F(\mu)))}$, $S$ is contained in a straight line $l \doteq{(x, y) \mid y=d * x}$ passing through the origin and having slope equal to $d$. In practice $F(\mu)$ is estimated empirically from a finite number of points; as a consequence, the left term in equation will be different for different data points, and the set $S$ will only lie around $l$. This line of reasoning naturally suggests an algorithm to estimate the intrinsic dimension of a dataset:</p><ol><li>Compute the pairwise distances for each point in the dataset $i=1, \ldots, N$.</li><li>For each point i find the two shortest distances $r_1$ and $r_2$.</li><li>For each point i compute $\mu_i=\frac{r_2}{r_1}$.</li><li>Compute the empirical cumulate $F^{e m p}(\mu)$ by sorting the values of $\mu$ in an ascending order through a permutation $\sigma$, then define $F^{e m p}\left(\mu_{\sigma(i)}\right) \doteq \frac{i}{N}$.</li><li>Fit the points of the plane given by coordinates ${\ (\log(\mu_i), -\log(1 - F^{\text{emp}}(\mu_i))) \ | \ i = 1, \ldots, N\ }$ with a straight line passing through the origin.</li></ol><p>Even if the results above are derived in the case of a uniform distribution of points there is no dependence on the density $\rho$; as a consequence from the point of view of the algorithm we can a posteriori relax our hypothesis: we require the dataset to be only locally uniform in density, where locally means in the range of the second neighbor. From a theoretical point of view, this condition is satisfied in the limit of $N$ going to infinity. By performing numerical experiments on datasets in which the density is non-uniform we show empirically that even for a finite number of points the estimation is reasonably insensitive to density variations. The requirement of local uniformity only in the range of the second neighbor is an advantage with respect to competing approaches where local uniformity is required at larger distances.</p><h3 id=estimating-intrinsic-dimension-of-a-dataset-by-moments-method><em>Estimating Intrinsic Dimension of a Dataset by Moment&rsquo;s Method</em><a hidden class=anchor aria-hidden=true href=#estimating-intrinsic-dimension-of-a-dataset-by-moments-method>#</a></h3><pre><code>from skdim.id import TwoNN
</code></pre><h3 id=properties-of-isotropy>Properties of Isotropy<a hidden class=anchor aria-hidden=true href=#properties-of-isotropy>#</a></h3><ol><li><p><strong>Mean Agnosticism</strong>:<br>Isotropy is a property solely of the <em>covariance matrix</em> of a distribution, meaning it is unaffected by the mean or central location of the data. Therefore, an isotropy measure should depend only on how the data varies in different directions, not on where it is centered. If a score or measure changes with shifts in the mean, it does not truly measure isotropy. This is because isotropy is concerned only with the dispersion of data points and the relative balance of variance across dimensions.</p></li><li><p><strong>Invariance to Scalar Multiples of the Covariance Matrix</strong>:<br>Isotropy remains unchanged if we scale the covariance matrix of the underlying distribution by a positive scalar. Mathematically, if we consider a covariance matrix $\Sigma=Cov(I)$ where $I$ is the identity matrix and $\lambda >0$ is a scalar, isotropy remains the same since scaling affects all dimensions equally. This property ensures that isotropy reflects the <em>shape</em> of the distribution rather than the overall <em>size</em> of the data spread. Thus, for an isotropic distribution, $Cov(\lambda)=Cov(\lambda I)$.</p></li><li><p><strong>Variance Distribution Across Dimensions</strong>:<br>For a distribution to be more isotropic, the variance should be approximately equal across all dimensions. Specifically, as isotropy increases, the difference between the maximum variance value in any single dimension and the average variance across all other dimensions should decrease. This means that in an isotropic distribution, no single dimension should dominate the spread of the data. Instead, the variance should be evenly distributed, indicating that the data is spread uniformly in all directions.</p></li><li><p><strong>Rotation Invariance</strong>:<br>An ideal measure of isotropy should be invariant to rotations of the data. Since isotropy is about the uniformity of variance in different directions, rotating the data should not change this property. In other words, the distribution of principal components (PCs) should remain constant under any rotation of the data. This property ensures that isotropy reflects the internal spread of data points rather than any specific orientation in space.</p></li><li><p><strong>Utilization of Dimensions</strong>:<br>There is a direct relationship between isotropy and the number of dimensions effectively used by the data. A distribution that uniformly utilizes a higher number of dimensions requires more principal components to capture all of the variance in the data. Therefore, as the dimensionality utilized by the data increases, the isotropy measure should ideally reflect this by increasing as well. A high isotropy score suggests that the data is distributed more evenly across available dimensions, not concentrated in just a few directions.</p></li><li><p><strong>Global Stability</strong>:<br>An isotropy measure should capture the global structure of the distribution, providing a holistic view of how evenly the data is spread across all dimensions. Rather than reflecting local variations or structure, isotropy should be a global measure, stable to small perturbations or local clusters in the data. This stability is essential for a reliable measure of the overall spatial utilization of a distribution.</p></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer style=padding-top:18px;padding-bottom:18px><div class=social-icons style=padding-bottom:0><a style=border-bottom:none href=https://x.com/workerplacemint rel=me title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a style=border-bottom:none href=https://github.com/ldomenichelli rel=me title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a style=border-bottom:none href=mailto:ldomenichelli2@gmail.com rel=me title=Email><svg viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div><span>&copy; 2024 <a href=https://ldomenichelli.github.io/>Lucia</a></span>
<span>•
Powered by
<a href=https://gohugo.io/>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/>PaperMod</a>
</span><span>•
<a href=/privacy>Privacy Policy</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let b=document.querySelector("#menu-trigger"),m=document.querySelector(".menu");b.addEventListener("click",function(){m.classList.toggle("hidden")}),document.body.addEventListener("click",function(e){b.contains(e.target)||m.classList.add("hidden")})</script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>