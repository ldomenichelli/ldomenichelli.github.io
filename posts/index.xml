<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>lucia&#39;s notes</title>
    <link>https://ldomenichelli.github.io/posts/</link>
    <description>Recent content on lucia&#39;s notes</description>
    <image>
      <title>lucia&#39;s notes</title>
      <url>https://ldomenichelli.github.io/logo.png</url>
      <link>https://ldomenichelli.github.io/logo.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://ldomenichelli.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>‚ùÑÔ∏è HPLT √ó NLPL Winter School</title>
      <link>https://ldomenichelli.github.io/posts/post5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post5/</guid>
      <description>&lt;h1 id=&#34;-day1--monday-10feb2025&#34;&gt;üóì Day¬†1 ‚Äî Monday, 10‚ÄØFeb‚ÄØ2025&lt;/h1&gt;
&lt;h3 id=&#34;-what-is-commoncrawl&#34;&gt;üåç What Is &lt;strong&gt;Common‚ÄØCrawl&lt;/strong&gt;?&lt;/h3&gt;
&lt;p&gt;Common‚ÄØCrawl is a &lt;strong&gt;huge, free snapshot of the public web&lt;/strong&gt;.&lt;br&gt;
A non‚Äëprofit updates it every month, storing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Billions of HTML pages&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Their cleaned‚Äëup &lt;strong&gt;text content&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Extra &lt;strong&gt;metadata&lt;/strong&gt; (links, timestamps, MIME types, ‚Ä¶)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;why-it-matters&#34;&gt;Why It Matters&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Track language change&lt;/strong&gt; ‚Äì see how words, memes, and topics shift over time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Map the web‚Äôs link network&lt;/strong&gt; ‚Äì study which sites connect and why.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Train big ML models&lt;/strong&gt; ‚Äì use real‚Äëworld data instead of tiny toy datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because each release includes both the &lt;strong&gt;raw HTML&lt;/strong&gt; and a parsed text layer, you can analyze:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ACL Vienna 2025 üá¶üáπ</title>
      <link>https://ldomenichelli.github.io/posts/post3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post3/</guid>
      <description>&lt;p&gt;Here are the notes from some talks I attented at ACL 2025 in Vienna!&lt;/p&gt;
&lt;details id=&#34;talk-eye&#34;&gt;
  &lt;summary&gt;  Eye-tracking &lt;/summary&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why gaze?&lt;/strong&gt; Eye movements reflect &lt;em&gt;online&lt;/em&gt; processing (not just end products), letting us probe difficulty, attention, and strategies during reading. That‚Äôs gold for modeling and evaluation. (&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/9849112/?utm_source=chatgpt.com&#34; title=&#34;Eye movements in reading and information processing - PubMed&#34;&gt;PubMed&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data is maturing:&lt;/strong&gt; Multilingual, multi‚Äëlab efforts (e.g., &lt;strong&gt;MECO&lt;/strong&gt;, &lt;strong&gt;MultiplEYE&lt;/strong&gt;) + tooling (e.g., &lt;strong&gt;pymovements&lt;/strong&gt;) have made high‚Äëquality datasets and pipelines more accessible. (&lt;a href=&#34;https://meco-read.com/?utm_source=chatgpt.com&#34; title=&#34;MECO Multilingual Eye-movement Corpus&#34;&gt;meco-read.com&lt;/a&gt;, &lt;a href=&#34;https://multipleye.eu/?utm_source=chatgpt.com&#34; title=&#34;MultiplEYE ‚Äì Enabling multilingual eye-tracking data collection for ...&#34;&gt;multipleye.eu&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2304.09859?utm_source=chatgpt.com&#34; title=&#34;pymovements: A Python Package for Eye Movement Data Processing&#34;&gt;arXiv&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Models &amp;amp; evals:&lt;/strong&gt; Gaze can &lt;strong&gt;improve&lt;/strong&gt; certain NLP tasks and also &lt;strong&gt;evaluate&lt;/strong&gt; systems with behavioral signals (e.g., readability, MT, summarization). But gains are often modest unless modeling is careful or data is task‚Äëaligned. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Open debates:&lt;/strong&gt; How well LLM surprisal predicts human reading times varies with model size, layers, and populations; adding &lt;strong&gt;recency biases&lt;/strong&gt; can help fit human behavior. (&lt;a href=&#34;https://aclanthology.org/2023.tacl-1.20/?utm_source=chatgpt.com&#34; title=&#34;Why Does Surprisal From Larger Transformer-Based Language ...&#34;&gt;ACL Anthology&lt;/a&gt;, &lt;a href=&#34;https://www.dfki.de/fileadmin/user_upload/import/14976_frai-07-1391745.pdf?utm_source=chatgpt.com&#34; title=&#34;[PDF] A review of machine learning in scanpath analysis for passive gaze ...&#34;&gt;dfki.de&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2024.cmcl-1.3/?utm_source=chatgpt.com&#34; title=&#34;Locally Biased Transformers Better Align with Human Reading Times&#34;&gt;ACL Anthology&lt;/a&gt;, &lt;a href=&#34;https://aclanthology.org/2025.coling-main.517/?utm_source=chatgpt.com&#34; title=&#34;Linear Recency Bias During Training Improves Transformers&#39; Fit to ...&#34;&gt;ACL Anthology&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;eyetracking-101-&#34;&gt;Eye‚Äëtracking 101 üëÅÔ∏è&lt;/h2&gt;
&lt;p&gt;Some basic concepts:&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Embeddings space ñ¶π◊Ç ‚ÇäÀö‚äπ‚ãÜ</title>
      <link>https://ldomenichelli.github.io/posts/post1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post1/</guid>
      <description>&lt;p&gt;Understanding the &lt;strong&gt;curse of dimensionality&lt;/strong&gt; requires more than just examining the representational aspect of data. A key insight lies in the concept of &lt;strong&gt;intrinsic dimensionality (ID)&lt;/strong&gt;‚Äîthe number of degrees of freedom within a data manifold or subspace. This ID is often independent of the dimensionality of the space in which the data is embedded. As a result, ID serves as a foundation for dimensionality reduction, which improves similarity measures and enhances the scalability of machine learning and data mining methods.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Geometric Deep Learning</title>
      <link>https://ldomenichelli.github.io/posts/post7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post7/</guid>
      <description>Some notes on geometric deep learning</description>
    </item>
    
    <item>
      <title>Optimal Transport üï∑Ô∏è and Wasserstein distance</title>
      <link>https://ldomenichelli.github.io/posts/post2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post2/</guid>
      <description>Intro to Optimal Transport with spider Cedric (Villani) :) üï∑Ô∏è</description>
    </item>
    
    <item>
      <title>Statistical Learning and Large Data üìä</title>
      <link>https://ldomenichelli.github.io/posts/post8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post8/</guid>
      <description>&lt;p&gt;Here are the slides and notes on the course hold by Prof. Chiaromonte at Sant&amp;rsquo;Anna University, Pisa.
&lt;embed src=&#34;https://ldomenichelli.github.io/SLLD_new.pdf&#34; width=&#34;100%&#34; height=&#34;800px&#34; type=&#34;application/pdf&#34;&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Time Series üìà</title>
      <link>https://ldomenichelli.github.io/posts/post9/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post9/</guid>
      <description>&lt;h1 id=&#34;predictive-models-for-time-series-analysis&#34;&gt;Predictive Models for Time Series Analysis&lt;/h1&gt;
&lt;p&gt;Time series analysis involves understanding data points collected over time, where the order of observations matters. By modeling the temporal dependencies, we can make forecasts, detect anomalies, cluster similar patterns, and perform classification or regression tasks on sequence data. Below is an extended overview with added details.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;1-introduction-to-time-series-analysis&#34;&gt;1. Introduction to Time Series Analysis&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;time series&lt;/strong&gt; is typically defined as:
$$
T = { x_1, x_2, \dots, x_m }
$$
where each observation ( x_i ) is recorded at a specific time ( t_i ), usually at uniform intervals.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Topological Data Analysis üåê</title>
      <link>https://ldomenichelli.github.io/posts/post4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ldomenichelli.github.io/posts/post4/</guid>
      <description>On the HPLT &amp;amp; NLPL Winter School</description>
    </item>
    
  </channel>
</rss>
